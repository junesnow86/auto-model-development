
Helsinki-NLP/opus-mt-bcl-sv failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

SEBIS/code_trans_t5_small_code_documentation_generation_javascript_transfer_learning_finetune failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


songhee/i-manual-mbert failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert_fast.py", line 221, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 111, in __init__
    fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)
Exception: No such file or directory (os error 2)

meongracun/nmt-mpst-id-en-lr_0.0001-ep_10-seq_128_bs-32 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

cemdenizsel/10k-finetuned-bert-model failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'cemdenizsel/10k-finetuned-bert-model'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'cemdenizsel/10k-finetuned-bert-model' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

neal49/roberta-nrc-fear failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 111, in <module>
    assert check_align(before_trace, after_trace), "Traced model does not match the original model"
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 49, in check_align
    for key in after_trace.keys():
AttributeError: 'tuple' object has no attribute 'keys'

Hichnick/ex_bot failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Hichnick/ex_bot'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Hichnick/ex_bot' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

danielvasic/distilbert-wordnet-uncased failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 96, in <module>
    concrete_args = tokenizer(text, return_tensors="pt")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2798, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2904, in _call_one
    return self.encode_plus(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2977, in encode_plus
    return self._encode_plus(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 576, in _encode_plus
    batched_output = self._batch_encode_plus(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 504, in _batch_encode_plus
    encodings = self._tokenizer.encode_batch(
Exception: WordPiece error: Missing [UNK] token from the vocabulary

phpaiola/ptt5-base-summ-cstnews failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


yhavinga/longt5-local-eff-large-nl8-voc8k-ddwn-neddx2-nl-en failed
Architectures: ['LongT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/longt5/modeling_longt5.py", line 1877, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/longt5/modeling_longt5.py", line 1435, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-is-fr failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

rdxsun/mbart_large_50_fmt failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mbart50/tokenization_mbart50_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
MBart50Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


m3/m3-experiment-roberta-base-tweet-eval-hate-eda-2 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-roberta-base-tweet-eval-hate-eda-2/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f0ccc-5126a55c46804c5f6d088fcb;d9262b48-58c3-481d-a265-ba0689f940e0)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-roberta-base-tweet-eval-hate-eda-2/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-roberta-base-tweet-eval-hate-eda-2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Helsinki-NLP/opus-mt-fi-ceb failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-es-hil failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-zh-bg failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

joyebright/Top1-with-without-mixing failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/joyebright/Top1-with-without-mixing/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f0ce1-521eb9245ac46ac53fbf1c04;488ae806-8373-4ba9-9e16-66de4eac664a)

Entry Not Found for url: https://huggingface.co/joyebright/Top1-with-without-mixing/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: joyebright/Top1-with-without-mixing does not appear to have a file named config.json. Checkout 'https://huggingface.co/joyebright/Top1-with-without-mixing/main' for available files.

jorge-henao/gpt2-small-spanish-disco-poetry failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'jorge-henao/gpt2-small-spanish-disco-poetry'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'jorge-henao/gpt2-small-spanish-disco-poetry' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

larryboy825/distilbert-base-uncased-finetuned-imdb failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'larryboy825/distilbert-base-uncased-finetuned-imdb'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'larryboy825/distilbert-base-uncased-finetuned-imdb' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

simecek/DNADebertaSmall failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'simecek/DNADebertaSmall'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'simecek/DNADebertaSmall' is the correct path to a directory containing all relevant files for a DebertaTokenizerFast tokenizer.

oftshsl/t5_ua_gec failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Abdelmageed95/distilgpt2-finetuned-wikitext2 failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Abdelmageed95/distilgpt2-finetuned-wikitext2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Abdelmageed95/distilgpt2-finetuned-wikitext2' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

tner/xlm-roberta-base-panx-dataset-ja failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Helsinki-NLP/opus-mt-ts-sv failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

samroni/gpt-2 failed
Architectures: ['XLMRobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'samroni/gpt-2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'samroni/gpt-2' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

bencodehard/mt5-small-finetuned-thaisum failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

totoro4007/cryptoroberta-base-all-finetuned failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 765, in from_pretrained
    raise ValueError(
ValueError: Tokenizer class FairSeqRobertaSentencePieceTokenizer does not exist or is not currently imported.

tscholak/t5.1.1.lm100k.base failed
Architectures: ['ElectraForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


knok/japanese-distilgpt2 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'knok/japanese-distilgpt2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'knok/japanese-distilgpt2' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

mpapucci/it5-topic-classification-tag-it failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

MickyMike/77-GPT2SP-mule-mulestudio failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'MickyMike/77-GPT2SP-mule-mulestudio'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'MickyMike/77-GPT2SP-mule-mulestudio' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

Helsinki-NLP/opus-mt-tc-big-itc-he failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

google/t5-efficient-base-dl8 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ghadeermobasher/BioRED-Chem-WLT-256-BlueBERT-20 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BioRED-Chem-WLT-256-BlueBERT-20'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BioRED-Chem-WLT-256-BlueBERT-20' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

shtoshni/gpt2-chess-uci failed
Architectures: ['ElectraForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'shtoshni/gpt2-chess-uci'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'shtoshni/gpt2-chess-uci' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

ankitsharma/YTFineTuneBert failed
Architectures: ['ElectraForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ankitsharma/YTFineTuneBert'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ankitsharma/YTFineTuneBert' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

yhavinga/long-t5-local-base-dutch-english failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/longt5/modeling_longt5.py", line 1877, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/longt5/modeling_longt5.py", line 1435, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-fr-to failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

unicamp-dl/mt5-base-en-pt-msmarco-v2 failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


annadmitrieva/rut5-base-par-simp failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


mercelisw/xlm-roberta-base-extended-language-detection failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'mercelisw/xlm-roberta-base-extended-language-detection'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'mercelisw/xlm-roberta-base-extended-language-detection' is the correct path to a directory containing all relevant files for a XLMRobertaTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-es-nl failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

versae/byt5-base-finetuned-modernisa failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

m3/m3-experiment-albert-base-v2-tweet-eval-irony-add-v3-greedy failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-tweet-eval-irony-add-v3-greedy/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f0dbd-643d11e132bf08ae223444c3;abc2f8c1-c113-49b0-8a02-c017d9e53b55)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-tweet-eval-irony-add-v3-greedy/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-albert-base-v2-tweet-eval-irony-add-v3-greedy is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

UBC-NLP/eyad-xl failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/UBC-NLP/eyad-xl/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f0dbd-4482d9194417ab163bb39b04;4e3db175-a2d5-4cc4-879a-dfc1e206bdcb)

Repository Not Found for url: https://huggingface.co/UBC-NLP/eyad-xl/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: UBC-NLP/eyad-xl is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Mcy/distilbert-base-uncased-finetuned-cola failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: DistilBertModel.forward() got an unexpected keyword argument 'token_type_ids'

Splend1dchan/t5small4-squad1024 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Splend1dchan/t5small4-squad1024'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Splend1dchan/t5small4-squad1024' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

google/t5-efficient-tiny-nl24 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Mustang/BERT_responsible_AI failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Mustang/BERT_responsible_AI'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Mustang/BERT_responsible_AI' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Lvxue/distilled-mt5-small-1-1 failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

VMware/t5-small-question-generator failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


EddieChen372/longT5-js2jest failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-tl-es failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

GuiGel/beto-uncased-flert-context-we-lstm-crf-meddocan failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/GuiGel/beto-uncased-flert-context-we-lstm-crf-meddocan/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f0e40-13db4c7d4ed4b63c7a89b790;05d70c72-7a63-4174-a818-9355a5f055f5)

Entry Not Found for url: https://huggingface.co/GuiGel/beto-uncased-flert-context-we-lstm-crf-meddocan/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: GuiGel/beto-uncased-flert-context-we-lstm-crf-meddocan does not appear to have a file named config.json. Checkout 'https://huggingface.co/GuiGel/beto-uncased-flert-context-we-lstm-crf-meddocan/main' for available files.

mrm8488/t5-base-finetuned-Reddit-TIFU-TLDR failed
Architectures: ['BloomForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


pritoms/distilgpt2-finetuned-irll2 failed
Architectures: ['XLMRobertaModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'pritoms/distilgpt2-finetuned-irll2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'pritoms/distilgpt2-finetuned-irll2' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

osanseviero/ca_core_news_sm failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/osanseviero/ca_core_news_sm/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f0e69-2d951d1279f343253d93ea0f;f1d4a372-ee70-4211-b807-daaf6762671a)

Entry Not Found for url: https://huggingface.co/osanseviero/ca_core_news_sm/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: osanseviero/ca_core_news_sm does not appear to have a file named config.json. Checkout 'https://huggingface.co/osanseviero/ca_core_news_sm/main' for available files.

mamlong34/t5_base_race_cosmos_qa failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

brwillia/distilgpt2-finetuned-wikitext2 failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'brwillia/distilgpt2-finetuned-wikitext2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'brwillia/distilgpt2-finetuned-wikitext2' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

k4black/roberta-reviews-lm failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/k4black/roberta-reviews-lm/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f0e77-7d2857e35dcc8f4a4fa79961;6c61b9ff-35a8-4ad0-bc2d-3f14a36d3dd6)

Repository Not Found for url: https://huggingface.co/k4black/roberta-reviews-lm/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: k4black/roberta-reviews-lm is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

tareknaous/t5-empathetic-dialogues failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Hate-speech-CNERG/deoffxlmr-mono-tamil failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


raruidol/ArgumentMining-EN-ARI-US2016 failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py", line 828, in forward
    embedding_output = self.embeddings(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py", line 102, in forward
    position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py", line 1560, in create_position_ids_from_input_ids
    mask = input_ids.ne(padding_idx).int()
TypeError: ne() received an invalid combination of arguments - got (NoneType), but expected one of:
 * (Tensor other)
      didn't match because some of the arguments have invalid types: ([31;1mNoneType[0m)
 * (Number other)
      didn't match because some of the arguments have invalid types: ([31;1mNoneType[0m)


ganchengguang/RoBERTa-base-japanese-sentencepiece failed
Architectures: ['XLMRobertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ganchengguang/RoBERTa-base-japanese-sentencepiece'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ganchengguang/RoBERTa-base-japanese-sentencepiece' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

alina1997/MarianMT failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

minemile/dummy-model failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'minemile/dummy-model'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'minemile/dummy-model' is the correct path to a directory containing all relevant files for a CamembertTokenizerFast tokenizer.

mathew/layoutlmv2-finetuned-funsd-1024 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'mathew/layoutlmv2-finetuned-funsd-1024'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'mathew/layoutlmv2-finetuned-funsd-1024' is the correct path to a directory containing all relevant files for a LayoutLMv2TokenizerFast tokenizer.

vidhur2k/mBERT-RomanceLang failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'vidhur2k/mBERT-RomanceLang'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'vidhur2k/mBERT-RomanceLang' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

sohamtiwari3120/testing_optimal_20 failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'sohamtiwari3120/testing_optimal_20'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sohamtiwari3120/testing_optimal_20' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

CEBaB/roberta-base.CEBaB.sa.2-class.exclusive.seed_88 failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'CEBaB/roberta-base.CEBaB.sa.2-class.exclusive.seed_88'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'CEBaB/roberta-base.CEBaB.sa.2-class.exclusive.seed_88' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

zhuqing/bert-base-uncased-reddit-lib-v2 failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'zhuqing/bert-base-uncased-reddit-lib-v2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'zhuqing/bert-base-uncased-reddit-lib-v2' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

debarghabhattofficial/t5-small-squad-finetuned-a2c-avg_batch_gleu-joint_training-best failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'debarghabhattofficial/t5-small-squad-finetuned-a2c-avg_batch_gleu-joint_training-best'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'debarghabhattofficial/t5-small-squad-finetuned-a2c-avg_batch_gleu-joint_training-best' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

xzhang/distilgpt2-finetuned-spam failed
Architectures: ['CamembertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'xzhang/distilgpt2-finetuned-spam'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'xzhang/distilgpt2-finetuned-spam' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

masakhane/m2m100_418M_ewe_fr_news failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1181, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 966, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Bhumika-kumaran/dummy-model failed
Architectures: ['XLMRobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Bhumika-kumaran/dummy-model'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Bhumika-kumaran/dummy-model' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

gilf/french-camembert-postag-model failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/camembert/tokenization_camembert_fast.py", line 127, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
CamembertConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


diegozs97/finetuned-chemprot-seed-1-100k failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/finetuned-chemprot-seed-1-100k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/finetuned-chemprot-seed-1-100k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-en-nyk failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

matheusvolpon/WE4LKD_AML_distilbert_1921_1984 failed
Architectures: ['DistilBertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: DistilBertModel.forward() got an unexpected keyword argument 'token_type_ids'

RTM/Lucky failed
Architectures: ['DistilBertForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/RTM/Lucky/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f0f4e-03092d8d6dec5925213565bb;b71bc655-762f-4aca-9aaa-bea3c97e2c20)

Entry Not Found for url: https://huggingface.co/RTM/Lucky/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: RTM/Lucky does not appear to have a file named config.json. Checkout 'https://huggingface.co/RTM/Lucky/main' for available files.

ChrisRhw/DialoGPT-medium-Chizuru failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/ChrisRhw/DialoGPT-medium-Chizuru/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f0f53-4aac9a3163d9c6361c2d20c5;46c05be7-9f44-43f2-9a38-6fc565dc5c6f)

Repository Not Found for url: https://huggingface.co/ChrisRhw/DialoGPT-medium-Chizuru/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: ChrisRhw/DialoGPT-medium-Chizuru is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

frtna/jwt300_mt-Italian-to-Spanish_transformers failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

subhasisj/en-TAPT-MLM-MiniLM failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'subhasisj/en-TAPT-MLM-MiniLM'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'subhasisj/en-TAPT-MLM-MiniLM' is the correct path to a directory containing all relevant files for a XLMRobertaTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-afa-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

bishalbaaniya/marian-finetuned-kde4-en-to-fr failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

insop/xlm-roberta-base-finetuned-panx-fr failed
Architectures: ['ElectraForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/insop/xlm-roberta-base-finetuned-panx-fr/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f0f88-1e129e89184cd60223c7b199;b4a53807-f38a-41bc-b179-1adcffc708b4)

Repository Not Found for url: https://huggingface.co/insop/xlm-roberta-base-finetuned-panx-fr/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: insop/xlm-roberta-base-finetuned-panx-fr is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

SEBIS/legal_t5_small_cls_cs failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


stevemobs/deberta-base-finetuned-aqa-squad1 failed
Architectures: ['DebertaForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 31, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 974, in new_func
    encoder_outputs = self.encoder(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 470, in new_func
    hidden_states = layer_module(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 383, in new_func
    attention_output = self.attention(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 316, in new_func
    self_output = self.self(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 661, in new_func
    rel_att = self.disentangled_att_bias(query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 710, in new_func
    c2p_att = torch.gather(c2p_att, dim=-1, index=c2p_dynamic_expand(c2p_pos, query_layer, relative_pos))
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
RuntimeError: c2p_dynamic_expand() Expected a value of type 'Tensor (inferred)' for argument 'c2p_pos' but instead found type 'ConcreteProxy'.
Inferred 'c2p_pos' to be of type 'Tensor' because it was not annotated with an explicit type.
Position: 0
Value: ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]]))
Declaration: c2p_dynamic_expand(Tensor c2p_pos, Tensor query_layer, Tensor relative_pos) -> Tensor
Cast error details: Unable to cast ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]])) to Tensor

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 33, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

thu-coai/CDial-GPT2_LCCC-base failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1073, in from_pretrained
    raise ValueError(
ValueError: Unrecognized model in thu-coai/CDial-GPT2_LCCC-base. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chinese_clip, clap, clip, clipseg, code_llama, codegen, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, data2vec-audio, data2vec-text, data2vec-vision, deberta, deberta-v2, decision_transformer, deformable_detr, deit, deta, detr, dinat, dinov2, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, git, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, graphormer, groupvit, hubert, ibert, idefics, imagegpt, informer, instructblip, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, longformer, longt5, luke, lxmert, m2m_100, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mistral, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, mpnet, mpt, mra, mt5, musicgen, mvp, nat, nezha, nllb-moe, nougat, nystromformer, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, pegasus, pegasus_x, perceiver, persimmon, pix2struct, plbart, poolformer, pop2piano, prophetnet, pvt, qdqbert, rag, realm, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rwkv, sam, seamless_m4t, segformer, sew, sew-d, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, umt5, unispeech, unispeech-sat, upernet, van, videomae, vilt, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vits, vivit, wav2vec2, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso

soorya12/t5-small-finetuned-on-cloudsek_data failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

MaryaAI/opus-mt-en-ar-finetunedSTEM-v5-en-to-ar failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

SEBIS/code_trans_t5_large_commit_generation_multitask_finetune failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Helsinki-NLP/opus-mt-fr-ber failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

tahmpa/bert-based-german-cased-scene-classification3 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/tahmpa/bert-based-german-cased-scene-classification3/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f0fcc-21fa07ab3cd2e80d003ef92e;ca24a3fd-7670-4dec-b66d-3e0c1bdf0763)

Repository Not Found for url: https://huggingface.co/tahmpa/bert-based-german-cased-scene-classification3/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: tahmpa/bert-based-german-cased-scene-classification3 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

mrm8488/t5-small-finetuned-quora-for-paraphrasing failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


google/t5-efficient-tiny-el2 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

vicl/canine-c-finetuned-mrpc failed
Architectures: ['CanineForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 31, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/canine/modeling_canine.py", line 1212, in new_func
    sequence_output = self.projection(concat)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/canine/modeling_canine.py", line 383, in new_func
    result = self.conv(pad(inputs))
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 841, in module_call_wrapper
    module_qualified_name = self.path_of_module(mod)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 580, in path_of_module
    sub_path = _orig_type(mod).__name__ + mod.extra_repr()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/padding.py", line 28, in extra_repr
    return 'padding={}, value={}'.format(self.padding, self.value)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 820, in module_getattribute_wrapper
    if self.the_path_of_middle_class[id(mod)] == '':
KeyError: 139683282597392

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 33, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

ghadeermobasher/BC5CDR-chem-WLT-384-BlueBERT-latest-100 failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC5CDR-chem-WLT-384-BlueBERT-latest-100'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC5CDR-chem-WLT-384-BlueBERT-latest-100' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

ghadeermobasher/NCBI-disease-WLT-384-SciBERT-10 failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/NCBI-disease-WLT-384-SciBERT-10'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/NCBI-disease-WLT-384-SciBERT-10' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

dpetrini/t5-small-finetuned-ro-to-en failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Jinchen/t5-small-finetuned-xsum failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Jinchen/t5-small-finetuned-xsum'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Jinchen/t5-small-finetuned-xsum' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

Monisha/opus-mt-en-de-finetuned-en-to-de failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

junnyu/roformer_chinese_base failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/roformer/tokenization_roformer.py", line 398, in __init__
    import rjieba
ModuleNotFoundError: No module named 'rjieba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2056, in _from_pretrained
    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/roformer/tokenization_roformer.py", line 400, in __init__
    raise ImportError(
ImportError: You need to install rjieba to use RoFormerTokenizer. See https://pypi.org/project/rjieba/ for installation.

jcai1/distilbert-base-uncased-finetuned-imdb failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'jcai1/distilbert-base-uncased-finetuned-imdb'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'jcai1/distilbert-base-uncased-finetuned-imdb' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

ai4bharat/IndicBARTSS failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/albert/tokenization_albert_fast.py", line 148, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
AlbertConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


research-backup/t5-small-subjqa-vanilla-electronics-qg failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Jyotiyadav/matricolas-3 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/Jyotiyadav/matricolas-3/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f10c9-541b6b8266ca960a38133f9e;edbc0ae5-b2eb-4315-a764-45b9f951d864)

Repository Not Found for url: https://huggingface.co/Jyotiyadav/matricolas-3/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: Jyotiyadav/matricolas-3 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Soonhwan-Kwon/xlm-roberta-xlarge failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Finnish-NLP/ul2-base-nl36-finnish failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


eslamxm/mt5-base-finetuned-en-cnn failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

StonyBrookNLP/teabreac-preasm-large-drop failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


MickyMike/2-GPT2SP-bamboo failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'MickyMike/2-GPT2SP-bamboo'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'MickyMike/2-GPT2SP-bamboo' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

driwnet/stsb-m-mt-ca-distilbert-base-uncased failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/driwnet/stsb-m-mt-ca-distilbert-base-uncased/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f10ff-3f4d9f187f9c763b78e3baf2;2b0c4848-8012-441f-91c0-2e8aa16ae7f6)

Entry Not Found for url: https://huggingface.co/driwnet/stsb-m-mt-ca-distilbert-base-uncased/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: driwnet/stsb-m-mt-ca-distilbert-base-uncased does not appear to have a file named config.json. Checkout 'https://huggingface.co/driwnet/stsb-m-mt-ca-distilbert-base-uncased/main' for available files.

ghadeermobasher/BC5CDR-disease-WLT-256-BlueBERT-latest-33 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC5CDR-disease-WLT-256-BlueBERT-latest-33'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC5CDR-disease-WLT-256-BlueBERT-latest-33' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

StonyBrookNLP/preasm-large-iirc-gold failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Edomonndo/opus-mt-ja-en-finetuned-ja-to-en_xml failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

prajdabre/morisien_english failed
Architectures: ['ElectraForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/albert/tokenization_albert_fast.py", line 148, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
AlbertConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


vencortex/DeepFeatEcosystemAnsweringEngineXLM failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/vencortex/DeepFeatEcosystemAnsweringEngineXLM/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f111d-59ff637062c67fbe236e92d2;4a709dbc-a9da-4115-be15-35749917d610)

Repository Not Found for url: https://huggingface.co/vencortex/DeepFeatEcosystemAnsweringEngineXLM/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: vencortex/DeepFeatEcosystemAnsweringEngineXLM is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

masakhane/m2m100_418M_wol_fr_rel failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1181, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 966, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

huak95/TNANA_V2-attacut-th-to-en-pt2 failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py", line 147, in __init__
    assert Path(source_spm).exists(), f"cannot find spm source {source_spm}"
  File "/anaconda/envs/amc/lib/python3.10/pathlib.py", line 960, in __new__
    self = cls._from_parts(args)
  File "/anaconda/envs/amc/lib/python3.10/pathlib.py", line 594, in _from_parts
    drv, root, parts = self._parse_args(args)
  File "/anaconda/envs/amc/lib/python3.10/pathlib.py", line 578, in _parse_args
    a = os.fspath(a)
TypeError: expected str, bytes or os.PathLike object, not NoneType

Jeevesh8/t5-small_re-cogs_22 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

nllg/poetry-bygpt5-small-de failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 765, in from_pretrained
    raise ValueError(
ValueError: Tokenizer class ByGPT5Tokenizer does not exist or is not currently imported.

nbalepur/cs_history_general_with_wiki_bert failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/nbalepur/cs_history_general_with_wiki_bert/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f1139-0d949a6275bfeca30fa513be;7159d5c3-dc5b-4185-94c8-7d4bd05e15f3)

Repository Not Found for url: https://huggingface.co/nbalepur/cs_history_general_with_wiki_bert/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: nbalepur/cs_history_general_with_wiki_bert is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

dllllb/poetnet-mt5-stihiru-libru failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-sv-lu failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

seduerr/paiformalityclassifier failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'seduerr/paiformalityclassifier'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'seduerr/paiformalityclassifier' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

ahmeddbahaa/mt5-small-finetuned-mt5-en failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

rickySaka/en-md failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

rajistics/distilbert-imdb-mlflow failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'rajistics/distilbert-imdb-mlflow'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'rajistics/distilbert-imdb-mlflow' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

afbudiman/distilled-indobert-classification failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: DistilBertModel.forward() got an unexpected keyword argument 'token_type_ids'

rbesaleli/t5-regex-summarization failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


rolosaCBTech/autotrain-mt5_xlsum_msamsum-2231571360 failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

misawann/bert-base-jaquad-ffn2150-head-10 failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py", line 457, in __init__
    import fugashi
ModuleNotFoundError: No module named 'fugashi'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py", line 192, in __init__
    self.word_tokenizer = MecabTokenizer(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py", line 459, in __init__
    raise error.__class__(
ModuleNotFoundError: You need to install fugashi to use MecabTokenizer. See https://pypi.org/project/fugashi/ for installation.

m3/m3-experiment-roberta-base-tweet-eval-hate-add-v2 failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-roberta-base-tweet-eval-hate-add-v2/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f11a2-30d0d0ee4480721065ca83b4;7e0cf670-2b86-44ea-91ee-9ff74b99a0d6)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-roberta-base-tweet-eval-hate-add-v2/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-roberta-base-tweet-eval-hate-add-v2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

laihuiyuan/lmm-multilingual-parsing failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/laihuiyuan/lmm-multilingual-parsing/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f11a2-5dcb9a1406d3810078ce2410;9e37dc76-b90b-4cbe-8e1f-56727b711976)

Repository Not Found for url: https://huggingface.co/laihuiyuan/lmm-multilingual-parsing/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: laihuiyuan/lmm-multilingual-parsing is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

ucabqfe/roberta_PER_io failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ucabqfe/roberta_PER_io'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ucabqfe/roberta_PER_io' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

amphora/ket5-tgt-masking failed
Architectures: ['DistilBertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/amphora/ket5-tgt-masking/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f11a4-4654623348a6eb590f548122;d8e35a95-8d0b-40b8-90aa-2ba6c6ab3481)

Repository Not Found for url: https://huggingface.co/amphora/ket5-tgt-masking/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: amphora/ket5-tgt-masking is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Helsinki-NLP/opus-mt-war-sv failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ghadeermobasher/BC4CHEMD-WLT-256-PubMedBERT-Trial-latest-60 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC4CHEMD-WLT-256-PubMedBERT-Trial-latest-60'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC4CHEMD-WLT-256-PubMedBERT-Trial-latest-60' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

fawrama/suicidal_text_classification failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/fawrama/suicidal_text_classification/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f11b4-297f33ce2758df770218a000;f75e9968-e08a-4d7a-b91c-505a04e43f93)

Repository Not Found for url: https://huggingface.co/fawrama/suicidal_text_classification/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: fawrama/suicidal_text_classification is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Amalq/distilbert-base-uncased-finetuned-cola failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/Amalq/distilbert-base-uncased-finetuned-cola/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f11b4-0c5043974d00588d6381b92b;64fe855c-a2cd-4a6b-9c9e-b533bd26c5ee)

Repository Not Found for url: https://huggingface.co/Amalq/distilbert-base-uncased-finetuned-cola/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: Amalq/distilbert-base-uncased-finetuned-cola is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Helsinki-NLP/opus-mt-en-kwn failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

leokai/distilroberta-base-finetuned-marktextepoch_n200 failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'leokai/distilroberta-base-finetuned-marktextepoch_n200'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'leokai/distilroberta-base-finetuned-marktextepoch_n200' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

berkergurcay/1k-pretrained-bert-model failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'berkergurcay/1k-pretrained-bert-model'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'berkergurcay/1k-pretrained-bert-model' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

m3/m3-experiment-albert-base-v2-citation-intent-eda-0 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-citation-intent-eda-0/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f11d2-1ad363a6610eaf231cd1df63;13b70f27-36f2-4c64-be16-a396eea94d40)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-citation-intent-eda-0/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-albert-base-v2-citation-intent-eda-0 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

masakhane/afribyt5_yor_en_news failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ytlin/19rdmhqc failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mbart/tokenization_mbart_fast.py", line 123, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
MBartConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


ghadeermobasher/BioRED-Chem-WLT-384 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BioRED-Chem-WLT-384'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BioRED-Chem-WLT-384' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

projectaligned/gpt2-xl-reddit-writingprompts-behavior-cloning failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/projectaligned/gpt2-xl-reddit-writingprompts-behavior-cloning/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f1228-69d82569130de2f01e4fedb6;ae372fe9-97f8-4450-97b9-3295df104a58)

Repository Not Found for url: https://huggingface.co/projectaligned/gpt2-xl-reddit-writingprompts-behavior-cloning/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: projectaligned/gpt2-xl-reddit-writingprompts-behavior-cloning is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

kiddothe2b/adhoc-hierarchical-transformer-base-4096 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/.cache/huggingface/modules/transformers_modules/kiddothe2b/adhoc-hierarchical-transformer-base-4096/f118131fa67033dce5847e33aac3ce6487791824/tokenization_hat.py", line 19, in <module>
    from nltk import sent_tokenize
ModuleNotFoundError: No module named 'nltk'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 751, in from_pretrained
    tokenizer_class = get_class_from_dynamic_module(class_ref, pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/dynamic_module_utils.py", line 499, in get_class_from_dynamic_module
    return get_class_in_module(class_name, final_module.replace(".py", ""))
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/dynamic_module_utils.py", line 199, in get_class_in_module
    module = importlib.import_module(module_path)
  File "/anaconda/envs/amc/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/yileiyang/.cache/huggingface/modules/transformers_modules/kiddothe2b/adhoc-hierarchical-transformer-base-4096/f118131fa67033dce5847e33aac3ce6487791824/tokenization_hat.py", line 21, in <module>
    raise Exception('NLTK is not installed! Install it with `pip install nltk`...')
Exception: NLTK is not installed! Install it with `pip install nltk`...

Helsinki-NLP/opus-mt-efi-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

flax-community/t5-recipe-generation failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

jaimin/formal_to_informal failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

SEBIS/legal_t5_small_cls_sv failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Helsinki-NLP/opus-mt-de-da failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-sm-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Cheatham/xlm-roberta-large-finetuned3 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


eliasws/openApiT5-to-description-v2 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

aditi2222/t5-paraphrase failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


vvn/en-to-it-marianmt failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

anas-awadalla/t5-base-few-shot-k-64-finetuned-squad-seed-4 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

BlinkDL/rwkv-4-pile-1b5 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/BlinkDL/rwkv-4-pile-1b5/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f129e-49188a3367f31371457205f0;cd13bf9a-1806-445b-9e6f-0797b683a99b)

Entry Not Found for url: https://huggingface.co/BlinkDL/rwkv-4-pile-1b5/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: BlinkDL/rwkv-4-pile-1b5 does not appear to have a file named config.json. Checkout 'https://huggingface.co/BlinkDL/rwkv-4-pile-1b5/main' for available files.

Onlydrinkwater/t5-small-de-en-mt failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Onlydrinkwater/t5-small-de-en-mt'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Onlydrinkwater/t5-small-de-en-mt' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

anikethjr/PromoGen_K562_2080Ti_restart failed
Architectures: ['ProphetNetForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/prophetnet/modeling_prophetnet.py", line 1828, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/prophetnet/modeling_prophetnet.py", line 1465, in forward
    raise ValueError("Either `decoder_input_ids` or `decoder_inputs_embeds` has to be passed.")
ValueError: Either `decoder_input_ids` or `decoder_inputs_embeds` has to be passed.

rkp74/t5_automated_mcq failed
Architectures: ['ProphetNetForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


masakhane/afribyt5_pcm_en_news failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

rahulchakwate/albert-xlarge-finetuned-squad failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/rahulchakwate/albert-xlarge-finetuned-squad/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f12bf-3e13cbfd31ce194f7c63b88c;291881d6-c2ba-4084-b194-9252948e8352)

Repository Not Found for url: https://huggingface.co/rahulchakwate/albert-xlarge-finetuned-squad/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: rahulchakwate/albert-xlarge-finetuned-squad is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

ryo0634/bert-base-log_linear-encoder-en-0 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/ryo0634/bert-base-log_linear-encoder-en-0/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f12c3-11adeab10fb03f5e4fa3cf4a;aff31dea-e664-48c6-9393-cc50e2ca2caa)

Repository Not Found for url: https://huggingface.co/ryo0634/bert-base-log_linear-encoder-en-0/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: ryo0634/bert-base-log_linear-encoder-en-0 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

AtulSingh31/t5-small-finetuned-xsum failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Finnish-NLP/ul2-mini-nl8-finnish failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Kamel/t5-darija-summarization failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

annahaz/xlm-roberta-base-misogyny-sexism-indomain-mix-trans failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/annahaz/xlm-roberta-base-misogyny-sexism-indomain-mix-trans/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f12f8-2a3d9af352894f8a5e061c32;f19a1361-6784-4198-838f-d77f43d96492)

Repository Not Found for url: https://huggingface.co/annahaz/xlm-roberta-base-misogyny-sexism-indomain-mix-trans/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: annahaz/xlm-roberta-base-misogyny-sexism-indomain-mix-trans is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

ml6team/mbart-large-cc25-cnn-dailymail-nl failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mbart/tokenization_mbart_fast.py", line 123, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
MBartConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


eunjin/kobart_gyeongsang_to_standard_translator failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: BartModel.forward() got an unexpected keyword argument 'token_type_ids'

FritzOS/TEdetection_distilBERT_mLM_V4 failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'FritzOS/TEdetection_distilBERT_mLM_V4'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'FritzOS/TEdetection_distilBERT_mLM_V4' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

paola-md/recipe-lr8e06-wd0.01-bs16 failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'paola-md/recipe-lr8e06-wd0.01-bs16'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'paola-md/recipe-lr8e06-wd0.01-bs16' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-en-ar failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

flax-community/clip-vision-bert-cc12m-70k failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1064, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 761, in __getitem__
    raise KeyError(key)
KeyError: 'clip-vision-bert'

stanlochten/t5-KGQgen failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'stanlochten/t5-KGQgen'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'stanlochten/t5-KGQgen' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

tau/False_large_rouge_paraNone_sent0_spanNone_itFalse_sargmax_rrFalse_8_1024_0.15_1 failed
Architectures: ['LEDForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


kevincstowe/psq failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'kevincstowe/psq'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'kevincstowe/psq' is the correct path to a directory containing all relevant files for a BartTokenizerFast tokenizer.

alireza7/ARMAN-MSR-persian-base-voa-title failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/tokenization_pegasus_fast.py", line 147, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
PegasusConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


edumunozsala/mt5-small-summarization-mlsum-es failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

KM4STfulltext/CSS_BERT failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/KM4STfulltext/CSS_BERT/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f13be-22a9ee2a740e061d1eab79f4;7c1435b1-96b9-4f88-9eef-b072bddd3238)

Repository Not Found for url: https://huggingface.co/KM4STfulltext/CSS_BERT/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: KM4STfulltext/CSS_BERT is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

tugstugi/bert-base-mongolian-cased failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/albert/tokenization_albert_fast.py", line 148, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
AlbertConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


akhisreelibra/mt5-small-finetuned-amazon-en-es failed
Architectures: ['XLMRobertaForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/akhisreelibra/mt5-small-finetuned-amazon-en-es/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-656f13e7-118289153dd7eed003b9c51b;10483052-c2b2-4112-aa01-4a5f9b1c0476)

Cannot access gated repo for url https://huggingface.co/akhisreelibra/mt5-small-finetuned-amazon-en-es/resolve/main/tokenizer_config.json.
Repo model akhisreelibra/mt5-small-finetuned-amazon-en-es is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 445, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/akhisreelibra/mt5-small-finetuned-amazon-en-es and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.

MJS2022/t5-small-finetuned-giga-test failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

PSW/t5-base-tweetsumm-seed33 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ghomasHudson/gptj_long_contra_pro failed
Architectures: ['OPTForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghomasHudson/gptj_long_contra_pro'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghomasHudson/gptj_long_contra_pro' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

uer/t5-v1_1-small-chinese-cluecorpussmall failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: MT5Model.forward() got an unexpected keyword argument 'token_type_ids'

anjandash/finetuned-bert-java-cmpx failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: DistilBertModel.forward() got an unexpected keyword argument 'token_type_ids'

CEBaB/lstm.CEBaB.absa.exclusive.seed_42 failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'CEBaB/lstm.CEBaB.absa.exclusive.seed_42'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'CEBaB/lstm.CEBaB.absa.exclusive.seed_42' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-fi-ln failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ck46/t5-base-qg-prefix failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

hf-internal-testing/tiny-random-ReformerForSequenceClassification failed
Architectures: ['ReformerForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 31, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/reformer/modeling_reformer.py", line 2101, in new_func
    encoder_outputs = self.encoder(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/reformer/modeling_reformer.py", line 1727, in new_func
    hidden_states = _ReversibleFunction.apply(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 958, in agfunc_apply_wrapper
    return self.create_proxy('call_function', self.agfunc_dict[clz], args, kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 444, in create_proxy
    value_unwrapped = self.run_target(kind, target, args_unwrapped, kwargs_unwrapped)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 374, in run_target
    result = run(kind, target, args, kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 347, in run
    return OperatorPatcherContext.patch_run(fn, *args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
TypeError: save_for_backward can only save variables, but argument 0 is of type ConcreteProxy

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 33, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

masakhane/m2m100_418M_fr_ewe_rel_news failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1181, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 966, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

sports-ru/sports-detox failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ssharm87/t5-small-finetuned-xsum-ss failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

MarianaLC/mt5-finetuned-pt-radiology-reports-512tokens failed
Architectures: ['ElectraForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/MarianaLC/mt5-finetuned-pt-radiology-reports-512tokens/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f1473-51f0961f7f3ee7fb0699db0e;f4b8203b-775e-4874-9aa7-902f062ca83b)

Repository Not Found for url: https://huggingface.co/MarianaLC/mt5-finetuned-pt-radiology-reports-512tokens/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: MarianaLC/mt5-finetuned-pt-radiology-reports-512tokens is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

kravchenko/uk-mt5-gec failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


negfir/bert_uncased_L-8_H-256_A-4wiki103 failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'negfir/bert_uncased_L-8_H-256_A-4wiki103'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'negfir/bert_uncased_L-8_H-256_A-4wiki103' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

NovelAI/genji-jp failed
Architectures: ['RobertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'NovelAI/genji-jp'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'NovelAI/genji-jp' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

SWQ/gpt2-medium-combine failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'SWQ/gpt2-medium-combine'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'SWQ/gpt2-medium-combine' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

paulowoicho/t5-podcast-summarisation failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


adamlin/ml999_grinding_machine failed
Architectures: ['GPTNeoForCausalLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/adamlin/ml999_grinding_machine/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f14ab-05b48fe53ff524057afb99b4;4bdb77a0-eb76-4809-8a90-68274e9ce335)

Repository Not Found for url: https://huggingface.co/adamlin/ml999_grinding_machine/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: adamlin/ml999_grinding_machine is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

negfir/bert_uncased_L-6_H-768_A-12 failed
Architectures: ['GPTNeoForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'negfir/bert_uncased_L-6_H-768_A-12'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'negfir/bert_uncased_L-6_H-768_A-12' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-bzs-fi failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

stanfordnlp/stanza-kmr failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/stanfordnlp/stanza-kmr/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f14bd-42714e2012b9193d05129435;3406f648-f403-4f51-a90c-4d0033eecaaf)

Entry Not Found for url: https://huggingface.co/stanfordnlp/stanza-kmr/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: stanfordnlp/stanza-kmr does not appear to have a file named config.json. Checkout 'https://huggingface.co/stanfordnlp/stanza-kmr/main' for available files.

circulus/kobart-trans-formal-v1 failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: BartModel.forward() got an unexpected keyword argument 'token_type_ids'

fxmarty/20220712-h08m02s04_example failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/fxmarty/20220712-h08m02s04_example/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f14dd-3bc080f713908d8117466cce;768120a6-c69d-403b-b8ff-709205171740)

Entry Not Found for url: https://huggingface.co/fxmarty/20220712-h08m02s04_example/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: fxmarty/20220712-h08m02s04_example does not appear to have a file named config.json. Checkout 'https://huggingface.co/fxmarty/20220712-h08m02s04_example/main' for available files.

Helsinki-NLP/opus-mt-tc-big-et-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-gil-sv failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Jeevesh8/t5-small-cogs_1 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

neal49/distilbert-sst2-freeze failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 111, in <module>
    assert check_align(before_trace, after_trace), "Traced model does not match the original model"
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 49, in check_align
    for key in after_trace.keys():
AttributeError: 'tuple' object has no attribute 'keys'

MickyMike/111-GPT2SP-appceleratorstudio-mulestudio failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'MickyMike/111-GPT2SP-appceleratorstudio-mulestudio'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'MickyMike/111-GPT2SP-appceleratorstudio-mulestudio' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

fanzru/t5-small-finetuned-xlsum-concat-multi-news-withlm failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

adalbertojunior/test_en_aligned failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/adalbertojunior/test_en_aligned/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f152a-04cfd88524f9c2470038aabf;6d606080-2d4c-49a8-b4fd-45065d834d8a)

Repository Not Found for url: https://huggingface.co/adalbertojunior/test_en_aligned/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: adalbertojunior/test_en_aligned is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Matthewww/mt5_NytNews failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

masakhane/m2m100_418M_wol_fr_rel_news_ft failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1181, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 966, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

FritzOS/train_NER_M_V1 failed
Architectures: ['BloomForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'FritzOS/train_NER_M_V1'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'FritzOS/train_NER_M_V1' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

sangmini/ReviewGeneration failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'sangmini/ReviewGeneration'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sangmini/ReviewGeneration' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

DemangeJeremy/4-sentiments-with-flaubert failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 789, in from_pretrained
    return tokenizer_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'DemangeJeremy/4-sentiments-with-flaubert'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'DemangeJeremy/4-sentiments-with-flaubert' is the correct path to a directory containing all relevant files for a FlaubertTokenizer tokenizer.

osanseviero/flair_test3 failed
Architectures: ['DistilBertForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/osanseviero/flair_test3/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f157b-680b23f12da98aaf23391f8d;676c411e-a090-44dc-9c10-6ee83eae2ab1)

Entry Not Found for url: https://huggingface.co/osanseviero/flair_test3/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: osanseviero/flair_test3 does not appear to have a file named config.json. Checkout 'https://huggingface.co/osanseviero/flair_test3/main' for available files.

AllenGeng/OCamlBert failed
Architectures: ['DistilBertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'AllenGeng/OCamlBert'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'AllenGeng/OCamlBert' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

af1tang/personaGPT failed
Architectures: ['XLMRobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py", line 134, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 111, in __init__
    fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)
Exception: No such file or directory (os error 2)

arunreddy/pan-kyc failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'arunreddy/pan-kyc'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'arunreddy/pan-kyc' is the correct path to a directory containing all relevant files for a LayoutLMv3TokenizerFast tokenizer.

hiiamsid/est5-base failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


muchad/idt5-base failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


m3/m3-experiment-albert-base-v2-tweet-eval-irony-word-swapping-random-2 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-tweet-eval-irony-word-swapping-random-2/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f15a6-12c85de02d3a373f12eb61b7;bd1c069e-4bd3-4e14-ab3e-b865af5a4b1c)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-tweet-eval-irony-word-swapping-random-2/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-albert-base-v2-tweet-eval-irony-word-swapping-random-2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Helsinki-NLP/opus-mt-ig-fi failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

domenicrosati/opus-mt-en-es-scielo failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Moo/kogpt2-proofreader failed
Architectures: ['DistilBertModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Moo/kogpt2-proofreader'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Moo/kogpt2-proofreader' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

MickyMike/codebert-c failed
Architectures: ['DistilBertModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'MickyMike/codebert-c'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'MickyMike/codebert-c' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-en-tvl failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

SEBIS/legal_t5_small_trans_de_en failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


ai4bharat/MultiIndicQuestionGenerationUnified failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/albert/tokenization_albert_fast.py", line 148, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
AlbertConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


lct-rug-2022/edos-2023-baseline-albert-base-v2-label_sexist failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/lct-rug-2022/edos-2023-baseline-albert-base-v2-label_sexist/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f15ee-740a120144e8c92e0bfdeeeb;44ece96d-7db7-4d95-af26-7a4b74baf6df)

Repository Not Found for url: https://huggingface.co/lct-rug-2022/edos-2023-baseline-albert-base-v2-label_sexist/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: lct-rug-2022/edos-2023-baseline-albert-base-v2-label_sexist is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

yancong/distilbert-base-uncased-finetuned-mi failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'yancong/distilbert-base-uncased-finetuned-mi'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'yancong/distilbert-base-uncased-finetuned-mi' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

explosion/es_udv25_spanishancora_trf failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/explosion/es_udv25_spanishancora_trf/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f15f5-094b7cea13423d46108324a5;58860604-e011-4bfe-800c-d7bd1b0c8be2)

Entry Not Found for url: https://huggingface.co/explosion/es_udv25_spanishancora_trf/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: explosion/es_udv25_spanishancora_trf does not appear to have a file named config.json. Checkout 'https://huggingface.co/explosion/es_udv25_spanishancora_trf/main' for available files.

dbernsohn/t5_wikisql_en2SQL failed
Architectures: ['DistilBertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


masakhane/m2m100_418M_en_yor_rel failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1181, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 966, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

nbalepur/cs_history_history_no_wiki_bert failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/nbalepur/cs_history_history_no_wiki_bert/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f1618-753f120755a479d51106f877;bf194f76-9e2d-4b80-b772-ba309d1bd203)

Repository Not Found for url: https://huggingface.co/nbalepur/cs_history_history_no_wiki_bert/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: nbalepur/cs_history_history_no_wiki_bert is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Helsinki-NLP/opus-mt-de-lt failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-hil-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

rudacaya/bert-base-uncased-finetuned-rappi-data failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/rudacaya/bert-base-uncased-finetuned-rappi-data/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f1647-7cd54ac135a49bc83b956a67;a6b7784e-fe94-4ba2-bf05-a0aeb83260fe)

Repository Not Found for url: https://huggingface.co/rudacaya/bert-base-uncased-finetuned-rappi-data/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: rudacaya/bert-base-uncased-finetuned-rappi-data is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

meongracun/nmt-mpst-id-en-lr_0.0001-ep_10-seq_128_bs-16 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

nbtpj/tiny-bart failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/nbtpj/tiny-bart/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f165c-34c9350140bc3559344371e0;38a0ba14-095f-4c17-a0ea-a4542a21a64b)

Repository Not Found for url: https://huggingface.co/nbtpj/tiny-bart/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: nbtpj/tiny-bart is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

HalaIT/RobertaForSequenceClassification failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/HalaIT/RobertaForSequenceClassification/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-656f165c-3054c5c3020754fc2a2ef2ce;a1fd5c4a-a001-4576-81a6-c56b954d8948)

Cannot access gated repo for url https://huggingface.co/HalaIT/RobertaForSequenceClassification/resolve/main/tokenizer_config.json.
Repo model HalaIT/RobertaForSequenceClassification is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 445, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/HalaIT/RobertaForSequenceClassification and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.

chenqian/bert_cn_finetuning failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2056, in _from_pretrained
    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert.py", line 199, in __init__
    if not os.path.isfile(vocab_file):
  File "/anaconda/envs/amc/lib/python3.10/genericpath.py", line 30, in isfile
    st = os.stat(path)
TypeError: stat: path should be string, bytes, os.PathLike or integer, not NoneType

ghadeermobasher/BC2GM-WLT-128-SciBERT-latest-80 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC2GM-WLT-128-SciBERT-latest-80'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC2GM-WLT-128-SciBERT-latest-80' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

google/switch-base-16 failed
Architectures: ['SwitchTransformersForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 1437, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 966, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

anas-awadalla/t5-base-few-shot-k-256-finetuned-squad-seed-0 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

SEBIS/code_trans_t5_base_code_documentation_generation_java_multitask failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


juanseleon1/gpt-2-spanish-finetuned-tweetsv2100 failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/juanseleon1/gpt-2-spanish-finetuned-tweetsv2100/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f1694-480585542053980f062921cd;a8a39146-a73e-4602-8fb3-391997cb24da)

Repository Not Found for url: https://huggingface.co/juanseleon1/gpt-2-spanish-finetuned-tweetsv2100/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: juanseleon1/gpt-2-spanish-finetuned-tweetsv2100 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

annahaz/distilbert-base-multilingual-cased-misogyny-sexism-decay0.05-fr-outofdomain failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: DistilBertModel.forward() got an unexpected keyword argument 'token_type_ids'

cnut1648/BioLinkBERT-large-mnli failed
Architectures: ['AlbertForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/cnut1648/BioLinkBERT-large-mnli/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f169c-75836b3a08360614141cecaa;20897bfc-61d8-403f-a1dd-d715f3666e57)

Repository Not Found for url: https://huggingface.co/cnut1648/BioLinkBERT-large-mnli/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: cnut1648/BioLinkBERT-large-mnli is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Helsinki-NLP/opus-mt-ru-sl failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

SetFit/deberta-v3-large__sst2__train-32-1 failed
Architectures: ['XLMRobertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py", line 133, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
DebertaV2Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Helsinki-NLP/opus-mt-ms-ms failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-es-tll failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Samuel-Fipps/t5-efficient-large-nl36_fine_tune_sum_V2 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


DeskDown/MarianMix_en-ja-10 failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

assayw119/koelectra-wellnesee-text-classification.pth failed
Architectures: ['GPTNeoForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'assayw119/koelectra-wellnesee-text-classification.pth'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'assayw119/koelectra-wellnesee-text-classification.pth' is the correct path to a directory containing all relevant files for a ElectraTokenizerFast tokenizer.

AdapterHub/bert-base-uncased-pf-scitail failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/AdapterHub/bert-base-uncased-pf-scitail/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f170f-260f910918f58e327312556f;6bb840d1-0e82-433f-85ff-67a95f2bc7d8)

Entry Not Found for url: https://huggingface.co/AdapterHub/bert-base-uncased-pf-scitail/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: AdapterHub/bert-base-uncased-pf-scitail does not appear to have a file named config.json. Checkout 'https://huggingface.co/AdapterHub/bert-base-uncased-pf-scitail/main' for available files.

masakhane/m2m100_418M_fr_wol_rel_news_ft failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1181, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 966, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ghadeermobasher/linnaeus-WLT-128-SciBERT failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/linnaeus-WLT-128-SciBERT'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/linnaeus-WLT-128-SciBERT' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

SEBIS/legal_t5_small_cls_multitask_cs failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Helsinki-NLP/opus-mt-en-mk failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

m3/m3-experiment-albert-base-v2-citation-intent-word-swapping-synonym-2 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-citation-intent-word-swapping-synonym-2/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f172c-3f4d23db0651162a7651c0c1;cef2bd86-4c0f-4720-a0ef-15e92d86a891)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-citation-intent-word-swapping-synonym-2/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-albert-base-v2-citation-intent-word-swapping-synonym-2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Helsinki-NLP/opus-mt-es-hr failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Declan/HuffPost_model_v3 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Declan/HuffPost_model_v3'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Declan/HuffPost_model_v3' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

PSW/t5-base-tweetsummgen-xsum-conv-tweetsumm-seed55 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

tau/t5_lm_4_1024_0.3_epoch1 failed
Architectures: ['BertModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'tau/t5_lm_4_1024_0.3_epoch1'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'tau/t5_lm_4_1024_0.3_epoch1' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

tau/false_large_pmi_para0_sent1_span2_True_multi_masks_with_types_7_1024_0.3_epoch1 failed
Architectures: ['BertModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'tau/false_large_pmi_para0_sent1_span2_True_multi_masks_with_types_7_1024_0.3_epoch1'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'tau/false_large_pmi_para0_sent1_span2_True_multi_masks_with_types_7_1024_0.3_epoch1' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

neulab/reatt-large-nq-fiqa failed
Architectures: ['ReAttForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

sibckukgvaxsepbkyb/mT5IndoQG failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'sibckukgvaxsepbkyb/mT5IndoQG'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sibckukgvaxsepbkyb/mT5IndoQG' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

mboth/klassifizierungSichern failed
Architectures: ['GPTNeoForCausalLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/mboth/klassifizierungSichern/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f1769-69dad06414d1b38d06358487;ea9f363c-c756-4b0f-90c7-cefc329164cb)

Repository Not Found for url: https://huggingface.co/mboth/klassifizierungSichern/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: mboth/klassifizierungSichern is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Helsinki-NLP/opus-mt-loz-fi failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

FourthBrain/bert_model_reddit_tsla failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/FourthBrain/bert_model_reddit_tsla/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f176f-7bf505745852237b73530204;2ed16945-ddb2-49f7-a824-c3043367decf)

Repository Not Found for url: https://huggingface.co/FourthBrain/bert_model_reddit_tsla/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: FourthBrain/bert_model_reddit_tsla is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Hoax0930/kyoto_marian_mod_2 failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

hf-internal-testing/tiny-random-RoFormerForMaskedLM failed
Architectures: ['RoFormerForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 31, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/roformer/modeling_roformer.py", line 914, in new_func
    encoder_outputs = self.encoder(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/roformer/modeling_roformer.py", line 570, in new_func
    sinusoidal_pos = self.embed_positions(hidden_states.shape[:-1], past_key_values_length)[None, None, :, :]
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 114, in new_func
    with ctx_factory():
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 974, in torch_no_grad_enter_wrapper
    return self.create_proxy('call_function', _orig_torch_no_grad_enter, (no_grad,), {})
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 446, in create_proxy
    args_ = self.create_arg(args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 555, in create_arg
    return super().create_arg(a)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/proxy.py", line 239, in create_arg
    return type(a)(self.create_arg(elem) for elem in a)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/proxy.py", line 239, in <genexpr>
    return type(a)(self.create_arg(elem) for elem in a)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 555, in create_arg
    return super().create_arg(a)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/proxy.py", line 267, in create_arg
    raise NotImplementedError(f"argument of type: {type(a)}")
NotImplementedError: argument of type: <class 'torch.autograd.grad_mode.no_grad'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 33, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

jkgrad/longformer-base-stsb failed
Architectures: ['RobertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2056, in _from_pretrained
    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/longformer/tokenization_longformer.py", line 236, in __init__
    with open(merges_file, encoding="utf-8") as merges_handle:
TypeError: expected str, bytes or os.PathLike object, not NoneType

jstep750/mlflow-test failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'jstep750/mlflow-test'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'jstep750/mlflow-test' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

ShengdingHu/superglue-boolq-multig failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Declan/WallStreetJournal_model_v2 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Declan/WallStreetJournal_model_v2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Declan/WallStreetJournal_model_v2' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

m3/m3-experiment-albert-base-v2-tweet-eval-irony-eda-2 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-tweet-eval-irony-eda-2/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f178c-2bb5cee343dfd16241ae6ec1;aaef4a8e-6290-4510-8c90-bd41fc03935d)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-tweet-eval-irony-eda-2/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-albert-base-v2-tweet-eval-irony-eda-2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

amanneo/distilgpt2-finetuned-custom-mail failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'amanneo/distilgpt2-finetuned-custom-mail'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'amanneo/distilgpt2-finetuned-custom-mail' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

ssantanag/pasajes_de_la_biblia failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

doraemon1998/opus-mt-en-ro-finetuned-en-to-ro failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

nsi319/legal-pegasus failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/tokenization_pegasus_fast.py", line 147, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
PegasusConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


arjunth2001/priv_sum failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Sandipan1994/t5-small-finetuned-eli5 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-tc-big-en-ko failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

mesolitica/t5-small-finetuned-noisy-ms-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/mesolitica/t5-small-finetuned-noisy-ms-en/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f17ba-6ad73d9a79e1694d7e56b1b9;9473cb7c-4dbf-4b71-a8ac-289991fe08e5)

Repository Not Found for url: https://huggingface.co/mesolitica/t5-small-finetuned-noisy-ms-en/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: mesolitica/t5-small-finetuned-noisy-ms-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

AhmedSSoliman/MarianCG-DJANGO failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Ogayo/mt-en-adh failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-en-hu failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

TuhinColumbia/russianpoetrymany failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mbart/tokenization_mbart_fast.py", line 123, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
MBartConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


xlm-mlm-en-2048 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm/tokenization_xlm.py", line 617, in __init__
    import sacremoses
ModuleNotFoundError: No module named 'sacremoses'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 789, in from_pretrained
    return tokenizer_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm/tokenization_xlm.py", line 619, in __init__
    raise ImportError(
ImportError: You need to install sacremoses to use XLMTokenizer. See https://pypi.org/project/sacremoses/ for installation.

ahmeddbahaa/xlmroberta2xlmroberta-finetune-summarization-ur failed
Architectures: ['EncoderDecoderModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 101, in <module>
    model = AutoModel.from_config(config, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 443, in from_config
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig'> for this kind of AutoModel: AutoModel.
Model type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, LlamaConfig, CodeGenConfig, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MistralConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SamConfig, SeamlessM4TConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VitDetConfig, VitsConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig.

SEBIS/legal_t5_small_trans_fr_es_small_finetuned failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


lightonai/RITA_l failed
Architectures: ['RITAModelForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 31, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/home/yileiyang/.cache/huggingface/modules/transformers_modules/lightonai/RITA_l/b38b5daa35b416f710216305c7258e3138503b62/rita_modeling.py", line 251, in new_func
    x = layer(x, causal_mask=causal_mask, attention_mask=attention_mask)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/home/yileiyang/.cache/huggingface/modules/transformers_modules/lightonai/RITA_l/b38b5daa35b416f710216305c7258e3138503b62/rita_modeling.py", line 208, in new_func
    y = self.mlp(y)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 855, in module_call_wrapper
    ret_val = _orig_module_call(mod, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 855, in module_call_wrapper
    ret_val = _orig_module_call(mod, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yileiyang/.cache/huggingface/modules/transformers_modules/lightonai/RITA_l/b38b5daa35b416f710216305c7258e3138503b62/rita_modeling.py", line 33, in forward
    return RITA_gelu(hidden_states)
RuntimeError: RITA_gelu() Expected a value of type 'Tensor (inferred)' for argument 'hidden_states' but instead found type 'ConcreteProxy'.
Inferred 'hidden_states' to be of type 'Tensor' because it was not annotated with an explicit type.
Position: 0
Value: ConcreteProxy(linear_4, tensor([[[-0.3130,  0.3382,  0.1431,  ...,  1.1837,  0.7714, -0.5013],
         [-0.8369, -0.0860,  0.0904,  ...,  0.1827, -0.5153, -0.8861],
         [-1.1623, -0.0331,  0.8701,  ...,  0.4367,  0.0040, -0.9949]]]))
Declaration: RITA_gelu(Tensor hidden_states) -> Tensor
Cast error details: Unable to cast ConcreteProxy(linear_4, tensor([[[-0.3130,  0.3382,  0.1431,  ...,  1.1837,  0.7714, -0.5013],
         [-0.8369, -0.0860,  0.0904,  ...,  0.1827, -0.5153, -0.8861],
         [-1.1623, -0.0331,  0.8701,  ...,  0.4367,  0.0040, -0.9949]]])) to Tensor

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 33, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

anas-awadalla/t5-small-few-shot-k-32-finetuned-squad-seed-2 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

princeton-nlp/CoFi-MRPC-s60 failed
Architectures: ['CoFiBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 31, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py", line 1013, in new_func
    encoder_outputs = self.encoder(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py", line 607, in new_func
    layer_outputs = layer_module(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py", line 497, in new_func
    self_attention_outputs = self.attention(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py", line 436, in new_func
    attention_output = self.output(self_outputs[0], hidden_states)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py", line 386, in new_func
    hidden_states = self.dense(hidden_states)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 855, in module_call_wrapper
    ret_val = _orig_module_call(mod, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1383, in func_wrapper
    return tracer.create_proxy('call_function', to_func, args, kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 444, in create_proxy
    value_unwrapped = self.run_target(kind, target, args_unwrapped, kwargs_unwrapped)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 374, in run_target
    result = run(kind, target, args, kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 347, in run
    return OperatorPatcherContext.patch_run(fn, *args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 33, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

SandraB/mt5-small-mlsum_training_sample failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

dkleczek/Polish_BART_base_OPI failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: BartModel.forward() got an unexpected keyword argument 'token_type_ids'

dropout05/lfom_distilt5_6l_8h_512d_2048ff failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

MickyMike/1-GPT2SP-mulestudio failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'MickyMike/1-GPT2SP-mulestudio'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'MickyMike/1-GPT2SP-mulestudio' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

gayanin/t5-small-med-term-conditional-masking failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

cvcio/mediawatch-el-topics failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/cvcio/mediawatch-el-topics/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-656f1875-7be577c06bce7db6260d5837;3e9a3272-f68a-4816-9c24-fd44b9ac61c2)

Cannot access gated repo for url https://huggingface.co/cvcio/mediawatch-el-topics/resolve/main/tokenizer_config.json.
Repo model cvcio/mediawatch-el-topics is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 445, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/cvcio/mediawatch-el-topics and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.

paola-md/distilr2-lr2e05-wd0.08-bs16 failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'paola-md/distilr2-lr2e05-wd0.08-bs16'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'paola-md/distilr2-lr2e05-wd0.08-bs16' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

Chemsseddine/flaubert_base_cased-finetuned-DOP7 failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/Chemsseddine/flaubert_base_cased-finetuned-DOP7/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f1878-478b77901203f404492282a7;e8a66153-26e8-4551-b6c4-1ecce0a43318)

Repository Not Found for url: https://huggingface.co/Chemsseddine/flaubert_base_cased-finetuned-DOP7/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: Chemsseddine/flaubert_base_cased-finetuned-DOP7 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

SEBIS/code_trans_t5_base_program_synthese failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Helsinki-NLP/opus-mt-is-sv failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Team-PIXEL/pixel-base-finetuned-cola failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1064, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 761, in __getitem__
    raise KeyError(key)
KeyError: 'pixel'

Helsinki-NLP/opus-mt-sv-iso failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Khyeyoon/MRC_roberta failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Khyeyoon/MRC_roberta'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Khyeyoon/MRC_roberta' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

airesearch/wangchanberta-base-att-spm-uncased failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/camembert/tokenization_camembert_fast.py", line 127, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
CamembertConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


paola-md/distilr2-lr1e05-wd0.05-bs32 failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'paola-md/distilr2-lr1e05-wd0.05-bs32'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'paola-md/distilr2-lr1e05-wd0.05-bs32' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

edmundmills/utilitarian-fold-3 failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py", line 133, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
DebertaV2Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


lightbansal/autotrain-metadata_postprocess-1277848897 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-fi-bzs failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

valurank/headline_generator_baseline failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/tokenization_pegasus_fast.py", line 147, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
PegasusConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


radhikabansal/t5-base-finetuned-news-summary failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

anshr/t5-small_supervised_baseline_01 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'anshr/t5-small_supervised_baseline_01'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'anshr/t5-small_supervised_baseline_01' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

ArafatBHossain/distill_bert_fine_tuned_mind failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ArafatBHossain/distill_bert_fine_tuned_mind'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ArafatBHossain/distill_bert_fine_tuned_mind' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

SEBIS/legal_t5_small_cls_multitask_en failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


ghadeermobasher/BC4CHEMD-WLT-320-BlueBERT-Trial-latest-60 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC4CHEMD-WLT-320-BlueBERT-Trial-latest-60'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC4CHEMD-WLT-320-BlueBERT-Trial-latest-60' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

NinaXiao/distilroberta-base-finetuned-wikitext2 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'NinaXiao/distilroberta-base-finetuned-wikitext2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'NinaXiao/distilroberta-base-finetuned-wikitext2' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

thet-system/en_core_sci_md failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/thet-system/en_core_sci_md/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-656f1927-1675e1391e1d92b869bebc74;55a857cf-8bde-4ace-ac43-d0501c771cf1)

Cannot access gated repo for url https://huggingface.co/thet-system/en_core_sci_md/resolve/main/tokenizer_config.json.
Repo model thet-system/en_core_sci_md is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 445, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/thet-system/en_core_sci_md and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.

Helsinki-NLP/opus-mt-tc-big-lv-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

muks14/og-deberta-extra-o failed
Architectures: ['DebertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 31, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 974, in new_func
    encoder_outputs = self.encoder(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 470, in new_func
    hidden_states = layer_module(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 383, in new_func
    attention_output = self.attention(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 316, in new_func
    self_output = self.self(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 661, in new_func
    rel_att = self.disentangled_att_bias(query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 710, in new_func
    c2p_att = torch.gather(c2p_att, dim=-1, index=c2p_dynamic_expand(c2p_pos, query_layer, relative_pos))
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
RuntimeError: c2p_dynamic_expand() Expected a value of type 'Tensor (inferred)' for argument 'c2p_pos' but instead found type 'ConcreteProxy'.
Inferred 'c2p_pos' to be of type 'Tensor' because it was not annotated with an explicit type.
Position: 0
Value: ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]]))
Declaration: c2p_dynamic_expand(Tensor c2p_pos, Tensor query_layer, Tensor relative_pos) -> Tensor
Cast error details: Unable to cast ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]])) to Tensor

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 33, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

charsiu/g2p_multilingual_byT5_tiny_8_layers_100 failed
Architectures: ['DebertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'charsiu/g2p_multilingual_byT5_tiny_8_layers_100'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'charsiu/g2p_multilingual_byT5_tiny_8_layers_100' is the correct path to a directory containing all relevant files for a ByT5Tokenizer tokenizer.

annahaz/xlm-roberta-base-misogyny-sexism-decay0.05-fr-outdomain failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/annahaz/xlm-roberta-base-misogyny-sexism-decay0.05-fr-outdomain/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f194f-58851cdf43b546df42f2e312;6ae64704-a0d3-4d91-b3d0-9f01f7d8cd2e)

Repository Not Found for url: https://huggingface.co/annahaz/xlm-roberta-base-misogyny-sexism-decay0.05-fr-outdomain/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: annahaz/xlm-roberta-base-misogyny-sexism-decay0.05-fr-outdomain is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

dminiotas05/distilbert-base-uncased-finetuned-ft750_reg3 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'dminiotas05/distilbert-base-uncased-finetuned-ft750_reg3'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'dminiotas05/distilbert-base-uncased-finetuned-ft750_reg3' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

dwisaji/bert-base-indonesia-sentiment-analysis failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'dwisaji/bert-base-indonesia-sentiment-analysis'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'dwisaji/bert-base-indonesia-sentiment-analysis' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

ghadeermobasher/BioRED-Chem-WLT-320-PubMedBERT failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BioRED-Chem-WLT-320-PubMedBERT'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BioRED-Chem-WLT-320-PubMedBERT' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

ctkang/m2m100_418M_10 failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1181, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 966, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

kabelomalapane/Helsinki-NLP-opus-finetuned-en-to-zu failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

MickyMike/22-GPT2SP-mesos-usergrid failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'MickyMike/22-GPT2SP-mesos-usergrid'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'MickyMike/22-GPT2SP-mesos-usergrid' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

mezes/finetuned-mt5 failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


junnyu/flashquad_small_wwm_cluecorpussmall failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 100, in <module>
    config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1064, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 761, in __getitem__
    raise KeyError(key)
KeyError: 'flash_quad'

vasudevgupta/bigbird-pegasus-large-arxiv failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/tokenization_pegasus_fast.py", line 147, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
PegasusConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


ytlin/2sk5p244 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mbart/tokenization_mbart_fast.py", line 123, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
MBartConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


snrspeaks/KeyPhraseTransformer failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-tl-de failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-tr-uk failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

subham92/translation_model_by_subham failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

paola-md/recipe-lr1e05-wd0.02-bs16 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'paola-md/recipe-lr1e05-wd0.02-bs16'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'paola-md/recipe-lr1e05-wd0.02-bs16' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

google/t5-efficient-small-dl8 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

romjansen/mbert-base-cased-NER-NL-legislation-refs failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/romjansen/mbert-base-cased-NER-NL-legislation-refs/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-656f4c18-5daf4f1e242d8d986291bab6;6513e96e-0ff2-434e-a414-4f7985f8ec08)

Cannot access gated repo for url https://huggingface.co/romjansen/mbert-base-cased-NER-NL-legislation-refs/resolve/main/tokenizer_config.json.
Repo model romjansen/mbert-base-cased-NER-NL-legislation-refs is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 445, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/romjansen/mbert-base-cased-NER-NL-legislation-refs and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.

airKlizz/mt5-base-wikinewssum-polish failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

paola-md/recipe-lr0.0001-wd0.08-bs64 failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'paola-md/recipe-lr0.0001-wd0.08-bs64'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'paola-md/recipe-lr0.0001-wd0.08-bs64' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

irenelizihui/MarianMT_UFAL_en_fr failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

orendar/en_he_roberta_shared failed
Architectures: ['EncoderDecoderModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 101, in <module>
    model = AutoModel.from_config(config, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 443, in from_config
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig'> for this kind of AutoModel: AutoModel.
Model type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, LlamaConfig, CodeGenConfig, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MistralConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SamConfig, SeamlessM4TConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VitDetConfig, VitsConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig, XLMRobertaConfig.

Helsinki-NLP/opus-mt-sv-ln failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

mesolitica/finetune-zeroshot-ner-t5-small-standard-bahasa-cased failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


AdapterHub/bert-base-uncased-pf-imdb failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/AdapterHub/bert-base-uncased-pf-imdb/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f4c64-636dedee6ea8ced662f50a4c;14af4e1e-20de-4d8e-831b-817564c59489)

Entry Not Found for url: https://huggingface.co/AdapterHub/bert-base-uncased-pf-imdb/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: AdapterHub/bert-base-uncased-pf-imdb does not appear to have a file named config.json. Checkout 'https://huggingface.co/AdapterHub/bert-base-uncased-pf-imdb/main' for available files.

ghadeermobasher/BioRED-Chem-WLT-512-PubMedBERT-t failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BioRED-Chem-WLT-512-PubMedBERT-t'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BioRED-Chem-WLT-512-PubMedBERT-t' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-rw-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Huertas97/es_roberta_base_bne_leetspeak_ner failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/Huertas97/es_roberta_base_bne_leetspeak_ner/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f4c78-48880f1868cb97cd15bb859c;24abb626-9176-43e0-a30a-d20249343f2b)

Entry Not Found for url: https://huggingface.co/Huertas97/es_roberta_base_bne_leetspeak_ner/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: Huertas97/es_roberta_base_bne_leetspeak_ner does not appear to have a file named config.json. Checkout 'https://huggingface.co/Huertas97/es_roberta_base_bne_leetspeak_ner/main' for available files.

chandrasutrisnotjhong/marian-finetuned-kde4-en-to-fr failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

liujxing/pegassus-samsum failed
Architectures: ['PegasusForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1221, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 981, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

misnaej/the-jam-machine failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/misnaej/the-jam-machine/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f4c95-053f1b702679be4445f88a7e;ea80e0b9-6c22-487b-9b07-2c8e42945262)

Repository Not Found for url: https://huggingface.co/misnaej/the-jam-machine/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: misnaej/the-jam-machine is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

cemdenizsel/51k-pretrained-bert-model failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'cemdenizsel/51k-pretrained-bert-model'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'cemdenizsel/51k-pretrained-bert-model' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

EleutherAI/polyglot-ko-1.3b failed
Architectures: ['GPTNeoXForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: GPTNeoXModel.forward() got an unexpected keyword argument 'token_type_ids'

Xenova/sponsorblock-base-v1 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

seoyoung/bart-base-samsum failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: BartModel.forward() got an unexpected keyword argument 'token_type_ids'

skylergrandel/Comcat failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/skylergrandel/Comcat/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f4ce8-5c1548e10d28529f09226328;4e6d5e3e-829f-4135-84bb-58ff0114ae01)

Repository Not Found for url: https://huggingface.co/skylergrandel/Comcat/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: skylergrandel/Comcat is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

WillHeld/t5-base-pointer-adv-top_v2 failed
Architectures: ['AlignedMT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

MartinoMensio/racism-models-regression-w-m-vote-epoch-4 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'MartinoMensio/racism-models-regression-w-m-vote-epoch-4'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'MartinoMensio/racism-models-regression-w-m-vote-epoch-4' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-de-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ghadeermobasher/BC2GM-WLT-320-SciBERT-latest failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC2GM-WLT-320-SciBERT-latest'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC2GM-WLT-320-SciBERT-latest' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

ChaiML/dalio_general_v1_epoch_10_lr_9e-7 failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/ChaiML/dalio_general_v1_epoch_10_lr_9e-7/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f4d22-4fc6067d5185b8cd6589a1a9;9bd05d6c-52c3-4179-ab6f-d6775c07ed56)

Repository Not Found for url: https://huggingface.co/ChaiML/dalio_general_v1_epoch_10_lr_9e-7/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: ChaiML/dalio_general_v1_epoch_10_lr_9e-7 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

meghazisofiane/opus-mt-en-ar-finetuned-en-to-ar-test2-instances failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

matheusvolpon/WE4LKD_AML_distilbert_1921_1985 failed
Architectures: ['DistilBertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: DistilBertModel.forward() got an unexpected keyword argument 'token_type_ids'

Helsinki-NLP/opus-mt-sv-id failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Gunulhona/tbSTmodel_v1 failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: BartModel.forward() got an unexpected keyword argument 'token_type_ids'

m3/m3-experiment-roberta-base-chemprot-back-translation failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-roberta-base-chemprot-back-translation/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f4d4b-1a02faec308afc76661eba2a;2fe8913f-735a-415e-a6e1-caa30d4c4e1c)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-roberta-base-chemprot-back-translation/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-roberta-base-chemprot-back-translation is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

derekbear/xlm-roberta-base-finetuned-panx-de failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/derekbear/xlm-roberta-base-finetuned-panx-de/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f4d4b-71c4becc0e7c0b5e5ed38455;1650160a-fd41-4fc6-9fe6-bad2efa49c40)

Repository Not Found for url: https://huggingface.co/derekbear/xlm-roberta-base-finetuned-panx-de/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: derekbear/xlm-roberta-base-finetuned-panx-de is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

diegozs97/chemprot-seed-2-100k failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/chemprot-seed-2-100k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/chemprot-seed-2-100k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

harish/t5-e2e-10epochs-lr1e4-alpha0-1PLUSalpha0-9-e20 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

parvezmrobin/bugsplainer-t5 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ilos-vigil/bigbird-small-indonesian-summarization failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/ilos-vigil/bigbird-small-indonesian-summarization/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f4d68-373e1ba31d9fd63a10c11e4a;6f4fe9bd-fa20-4960-94b9-37d9a91e5342)

Repository Not Found for url: https://huggingface.co/ilos-vigil/bigbird-small-indonesian-summarization/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: ilos-vigil/bigbird-small-indonesian-summarization is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Helsinki-NLP/opus-mt-rnd-sv failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

masakhane/byt5_mos_fr_news failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

sgugger/test-upload1 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'sgugger/test-upload1'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sgugger/test-upload1' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

microsoft/markuplm-large-finetuned-websrc failed
Architectures: ['XLMRobertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 96, in <module>
    concrete_args = tokenizer(text, return_tensors="pt")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/markuplm/tokenization_markuplm_fast.py", line 369, in __call__
    raise ValueError(
ValueError: Nodes must be of type `List[str]` (single pretokenized example), or `List[List[str]]` (batch of pretokenized examples).

timoneda/roberta-large-legal failed
Architectures: ['ElectraForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'timoneda/roberta-large-legal'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'timoneda/roberta-large-legal' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

Danastos/qacombined_bert_el_3 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/Danastos/qacombined_bert_el_3/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f4d8c-3d6aab5b1be2d8723dc22493;df6f8c62-c38e-4410-8e47-b602e58c0259)

Repository Not Found for url: https://huggingface.co/Danastos/qacombined_bert_el_3/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: Danastos/qacombined_bert_el_3 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

charsiu/g2p_multilingual_byT5_tiny_8_layers_100 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'charsiu/g2p_multilingual_byT5_tiny_8_layers_100'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'charsiu/g2p_multilingual_byT5_tiny_8_layers_100' is the correct path to a directory containing all relevant files for a ByT5Tokenizer tokenizer.

vparytskyy/lucy-base failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Neha2608/pegasus-samsum failed
Architectures: ['PegasusForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1221, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 981, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

VEG3/TLDR-Vegan-Studies failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

flax-community/Bengali-t5 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

hossein20s/bert-base-multilingual-uncased_fine_tuned_for_token_classiciation_es failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'hossein20s/bert-base-multilingual-uncased_fine_tuned_for_token_classiciation_es'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'hossein20s/bert-base-multilingual-uncased_fine_tuned_for_token_classiciation_es' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

cahya/xlm-roberta-large-indonesian-NER failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Matthijs/ane-distilbert-test failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 101, in <module>
    model = AutoModel.from_config(config, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 443, in from_config
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers_modules.Matthijs.ane-distilbert-test.e4c33a637122614bcd26939f1236fe87f853faa2.configuration_distilbert_ane.DistilBertConfig'> for this kind of AutoModel: AutoModel.
Model type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, LlamaConfig, CodeGenConfig, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MistralConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SamConfig, SeamlessM4TConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VitDetConfig, VitsConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig, XLMRobertaConfig.

skynex/DialoGPT-small-batman failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/skynex/DialoGPT-small-batman/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f4de0-453954374cb0668b2ed12d9a;5c005f30-6f42-4486-a5e2-e1440f983f4d)

Entry Not Found for url: https://huggingface.co/skynex/DialoGPT-small-batman/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: skynex/DialoGPT-small-batman does not appear to have a file named config.json. Checkout 'https://huggingface.co/skynex/DialoGPT-small-batman/main' for available files.

AustinCarthy/phishing-distilbert-base-uncased-finetuned-dsV0.1 failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/AustinCarthy/phishing-distilbert-base-uncased-finetuned-dsV0.1/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f4de0-622012ed4ed1c3615adf7061;6a50d31b-facb-464d-a263-ab6fe6ec02a5)

Repository Not Found for url: https://huggingface.co/AustinCarthy/phishing-distilbert-base-uncased-finetuned-dsV0.1/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: AustinCarthy/phishing-distilbert-base-uncased-finetuned-dsV0.1 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Helsinki-NLP/opus-mt-es-ar failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

diegozs97/finetuned-chemprot-seed-3-0k failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/finetuned-chemprot-seed-3-0k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/finetuned-chemprot-seed-3-0k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-en-pqe failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

vumichien/trillsson3-ft-keyword-spotting-11 failed
Architectures: ['LEDForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1064, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 761, in __getitem__
    raise KeyError(key)
KeyError: 'trillsson_efficient'

Payoto/gpt2-finetuned-wikitext2 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Payoto/gpt2-finetuned-wikitext2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Payoto/gpt2-finetuned-wikitext2' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

ShengdingHu/superglue-multirc failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

sanchit-gandhi/bloom-760m-scan failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'sanchit-gandhi/bloom-760m-scan'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sanchit-gandhi/bloom-760m-scan' is the correct path to a directory containing all relevant files for a BloomTokenizerFast tokenizer.

juridics/jurisbert-base-portuguese-uncased2 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/juridics/jurisbert-base-portuguese-uncased2/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f4e14-49ce9d6f10f99d5134faf303;b26f1493-59a8-4691-8989-954813cf4274)

Repository Not Found for url: https://huggingface.co/juridics/jurisbert-base-portuguese-uncased2/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: juridics/jurisbert-base-portuguese-uncased2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

uw-madison/yoso-4096 failed
Architectures: ['YosoForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 31, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/yoso/modeling_yoso.py", line 821, in new_func
    encoder_outputs = self.encoder(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/yoso/modeling_yoso.py", line 571, in new_func
    layer_outputs = layer_module(hidden_states, attention_mask, output_attentions)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/yoso/modeling_yoso.py", line 522, in new_func
    self_attention_outputs = self.attention(hidden_states, attention_mask, output_attentions=output_attentions)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/yoso/modeling_yoso.py", line 474, in new_func
    self_outputs = self.self(hidden_states, attention_mask, output_attentions)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/yoso/modeling_yoso.py", line 403, in new_func
    query_layer, key_layer = normalize([query_layer, key_layer])
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/yoso/modeling_yoso.py", line 97, in new_func
    return nn.functional.normalize(input_tensors, p=2, dim=-1)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1383, in func_wrapper
    return tracer.create_proxy('call_function', to_func, args, kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 444, in create_proxy
    value_unwrapped = self.run_target(kind, target, args_unwrapped, kwargs_unwrapped)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 374, in run_target
    result = run(kind, target, args, kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 347, in run
    return OperatorPatcherContext.patch_run(fn, *args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/functional.py", line 4660, in normalize
    denom = input.norm(p, dim, keepdim=True).clamp_min(eps).expand_as(input)
AttributeError: 'list' object has no attribute 'norm'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 33, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

flair/ner-german-legal failed
Architectures: ['YosoForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/flair/ner-german-legal/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f4e17-1ac4bb0b31ec002010a1ed6c;c6de7414-275c-403e-a1dc-c6940c334663)

Entry Not Found for url: https://huggingface.co/flair/ner-german-legal/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: flair/ner-german-legal does not appear to have a file named config.json. Checkout 'https://huggingface.co/flair/ner-german-legal/main' for available files.

Kevincp560/pegasus-arxiv-finetuned-pubmed failed
Architectures: ['PegasusForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1221, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 981, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

flairbook/flairmodel failed
Architectures: ['PegasusForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/flairbook/flairmodel/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f4e2c-15ad774143c7bc5072cf18d0;a8576d64-63da-414c-b01f-9a07bb28ec16)

Entry Not Found for url: https://huggingface.co/flairbook/flairmodel/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: flairbook/flairmodel does not appear to have a file named config.json. Checkout 'https://huggingface.co/flairbook/flairmodel/main' for available files.

tner/xlm-roberta-base-wnut2017 failed
Architectures: ['PegasusForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Emmawang/fake-news-classifier failed
Architectures: ['DistilBertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Emmawang/fake-news-classifier'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Emmawang/fake-news-classifier' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

ghadeermobasher/NCBI-disease-WLT-384-PubMedBERT-70 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/NCBI-disease-WLT-384-PubMedBERT-70'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/NCBI-disease-WLT-384-PubMedBERT-70' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

mohadz19/sadarjasa failed
Architectures: ['XLMRobertaModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/mohadz19/sadarjasa/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-656f4e45-09151441588737e72758850a;a29736cb-18f3-4959-b1d7-a157af65baab)

Cannot access gated repo for url https://huggingface.co/mohadz19/sadarjasa/resolve/main/tokenizer_config.json.
Repo model mohadz19/sadarjasa is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 445, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/mohadz19/sadarjasa and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.

Helsinki-NLP/opus-mt-ber-es failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

EP9/mt5-small-tuto-mt5-small-2 failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

yliu337/t5_token_nonfilter_bothcontext_padded_ctx failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


m3/m3-experiment-roberta-base-tweet-eval-hate-word-swapping-embedding-0 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-roberta-base-tweet-eval-hate-word-swapping-embedding-0/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f4e5c-3de04c02351adc0e4ced57f1;c4adace7-3f68-42f6-a2ec-60185e8950ee)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-roberta-base-tweet-eval-hate-word-swapping-embedding-0/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-roberta-base-tweet-eval-hate-word-swapping-embedding-0 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

ramsrigouthamg/t5_squad_v1 failed
Architectures: ['ElectraForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


SkolkovoInstitute/ruT5-base-detox failed
Architectures: ['ElectraForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Helsinki-NLP/opus-mt-hr-fi failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-tc-base-en-sh failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Timofey/PubMedBERT_Pathways_Context_Classifier failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/Timofey/PubMedBERT_Pathways_Context_Classifier/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-656f4e9c-5f29dbd019cc86c337f3e5c8;1f790e72-2009-4928-8263-1af428928f8d)

Cannot access gated repo for url https://huggingface.co/Timofey/PubMedBERT_Pathways_Context_Classifier/resolve/main/tokenizer_config.json.
Repo model Timofey/PubMedBERT_Pathways_Context_Classifier is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 445, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/Timofey/PubMedBERT_Pathways_Context_Classifier and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.

Rainiefantasy/GO1984_BERTUncased failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Rainiefantasy/GO1984_BERTUncased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Rainiefantasy/GO1984_BERTUncased' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

google/tapas-small-masklm failed
Architectures: ['DistilBertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 96, in <module>
    concrete_args = tokenizer(text, return_tensors="pt")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/tapas/tokenization_tapas.py", line 643, in __call__
    assert isinstance(table, pd.DataFrame), "Table must be of type pd.DataFrame"
AssertionError: Table must be of type pd.DataFrame

Helsinki-NLP/opus-mt-rw-fr failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

joshanashakya/codebert_sourcecode_nmt_pn2ja_100E_2e-05LR_16B_6E_6D failed
Architectures: ['EncoderDecoderModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 101, in <module>
    model = AutoModel.from_config(config, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 443, in from_config
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig'> for this kind of AutoModel: AutoModel.
Model type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, LlamaConfig, CodeGenConfig, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MistralConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SamConfig, SeamlessM4TConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VitDetConfig, VitsConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig, XLMRobertaConfig.

stanfordnlp/stanza-lv failed
Architectures: ['EncoderDecoderModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/stanfordnlp/stanza-lv/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f4ed1-0227667f00400d9a6977e206;cc907201-7ac0-48c8-ab15-351521d911c1)

Entry Not Found for url: https://huggingface.co/stanfordnlp/stanza-lv/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: stanfordnlp/stanza-lv does not appear to have a file named config.json. Checkout 'https://huggingface.co/stanfordnlp/stanza-lv/main' for available files.

CalvinHuang/mt5-small-finetuned-amazon-en-es failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-es-pl failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

shortneyrules/bert-case-cased failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'shortneyrules/bert-case-cased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'shortneyrules/bert-case-cased' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

nandwalritik/sql_classifier failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'nandwalritik/sql_classifier'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'nandwalritik/sql_classifier' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

josephgatto/paint_doctor_description_identification failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'josephgatto/paint_doctor_description_identification'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'josephgatto/paint_doctor_description_identification' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

hadifar/xlm-roberta-base-ft-CSTwitter failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


adamlin/ml999_metal_num failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/adamlin/ml999_metal_num/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f4f0a-2ab575af41e867d14517c458;9d18298d-2b79-49ce-a131-0dd9c1f1555a)

Repository Not Found for url: https://huggingface.co/adamlin/ml999_metal_num/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: adamlin/ml999_metal_num is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Salesforce/codegen-350M-nl failed
Architectures: ['CodeGenForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 109, in <module>
    after_trace = traced_gm(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 662, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 281, in __call__
    raise e
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 271, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "<eval_with_key>.385", line 77, in forward
    getitem_17 = to_1[unsqueeze];  to_1 = None
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_proxy.py", line 431, in impl
    return tracer.create_proxy('call_function', target, args, kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 444, in create_proxy
    value_unwrapped = self.run_target(kind, target, args_unwrapped, kwargs_unwrapped)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 374, in run_target
    result = run(kind, target, args, kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 347, in run
    return OperatorPatcherContext.patch_run(fn, *args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 291, in patch_run
    assert OperatorPatcherContext.ctx_tracer is not None
AssertionError

masakhane/byt5_pcm_en_news failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

mismayil/comet-gpt2 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2056, in _from_pretrained
    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/gpt2/tokenization_gpt2.py", line 181, in __init__
    with open(vocab_file, encoding="utf-8") as vocab_handle:
TypeError: expected str, bytes or os.PathLike object, not NoneType

m3/m3-experiment-albert-base-v2-tweet-eval-emotion-word-swapping-synonym-2 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-tweet-eval-emotion-word-swapping-synonym-2/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f4f41-110ec18e0b71aabe2fc59cd0;1980b630-104c-490b-8748-88f4f2943cf0)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-tweet-eval-emotion-word-swapping-synonym-2/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-albert-base-v2-tweet-eval-emotion-word-swapping-synonym-2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

dreamerdeo/unisar-t5-3b-sparc failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

marah99/t5-end2end-questions-generation-shorttest failed
Architectures: ['RobertaModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/marah99/t5-end2end-questions-generation-shorttest/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f4f74-0e39b1f70ea3165230bb12a6;c7a2e059-dfae-423b-b35b-ec0843b9456a)

Repository Not Found for url: https://huggingface.co/marah99/t5-end2end-questions-generation-shorttest/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: marah99/t5-end2end-questions-generation-shorttest is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

thivy/flan-t5-base-finetuned-opus_books-en-to-no-test failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

mrm8488/bert2bert_shared-spanish-finetuned-summarization failed
Architectures: ['EncoderDecoderModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 101, in <module>
    model = AutoModel.from_config(config, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 443, in from_config
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig'> for this kind of AutoModel: AutoModel.
Model type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, LlamaConfig, CodeGenConfig, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MistralConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SamConfig, SeamlessM4TConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VitDetConfig, VitsConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig, XLMRobertaConfig.

ndevavarapu/utterance_gen failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ndevavarapu/utterance_gen'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ndevavarapu/utterance_gen' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

strombergnlp/dant5-small failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

tsantosh7/en_BiomedNER_EuropePMC failed
Architectures: ['XLMRobertaForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/tsantosh7/en_BiomedNER_EuropePMC/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f4fb3-5f349b184070a92c76eb552e;b83b561d-b2af-429b-8dc3-dee2c1fc2ad6)

Entry Not Found for url: https://huggingface.co/tsantosh7/en_BiomedNER_EuropePMC/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: tsantosh7/en_BiomedNER_EuropePMC does not appear to have a file named config.json. Checkout 'https://huggingface.co/tsantosh7/en_BiomedNER_EuropePMC/main' for available files.

mboth/klassifizierungLuftVerteilen failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/mboth/klassifizierungLuftVerteilen/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f4fc5-4dfd82be4a206db31a3f49c2;4874df84-383e-4d0b-9585-f3fdc85c2dca)

Repository Not Found for url: https://huggingface.co/mboth/klassifizierungLuftVerteilen/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: mboth/klassifizierungLuftVerteilen is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Stancld/long-t5-tglobal-large failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Stancld/long-t5-tglobal-large'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Stancld/long-t5-tglobal-large' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

Helsinki-NLP/opus-mt-zne-sv failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

tgummadi/t5-11785 failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'tgummadi/t5-11785'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'tgummadi/t5-11785' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

ozcangundes/mt5-multitask-qa-qg-turkish failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Helsinki-NLP/opus-mt-en-itc failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

disenwang1994/bert-finetuned-ner failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/disenwang1994/bert-finetuned-ner/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f4fdd-342de653431a094019885eff;563db03f-c274-413b-9d48-e13879eef025)

Repository Not Found for url: https://huggingface.co/disenwang1994/bert-finetuned-ner/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: disenwang1994/bert-finetuned-ner is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

SEBIS/legal_t5_small_trans_en_fr failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


ghadeermobasher/BC5CDR-disease-WLT-256-BlueBERT-latest-23 failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC5CDR-disease-WLT-256-BlueBERT-latest-23'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC5CDR-disease-WLT-256-BlueBERT-latest-23' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

tau/pegasus_1024_0.3_epoch1_v2 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'tau/pegasus_1024_0.3_epoch1_v2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'tau/pegasus_1024_0.3_epoch1_v2' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

ludfo774/sparql-bart-append-1 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ludfo774/sparql-bart-append-1'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ludfo774/sparql-bart-append-1' is the correct path to a directory containing all relevant files for a BartTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-fj-fr failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

VanessaSchenkel/unicamp-finetuned-en-to-pt-dataset-ted failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Arashasg/WikiBert2WikiBert failed
Architectures: ['EncoderDecoderModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 101, in <module>
    model = AutoModel.from_config(config, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 443, in from_config
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig'> for this kind of AutoModel: AutoModel.
Model type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, LlamaConfig, CodeGenConfig, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MistralConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SamConfig, SeamlessM4TConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VitDetConfig, VitsConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig, XLMRobertaConfig.

imthanhlv/t5vi failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-ase-es failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

MickyMike/000-GPT2SP-talenddataquality-appceleratorstudio failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'MickyMike/000-GPT2SP-talenddataquality-appceleratorstudio'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'MickyMike/000-GPT2SP-talenddataquality-appceleratorstudio' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

NewT5SharedHeadsSharedKeyValues/t5-efficient-small-shkv failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'NewT5SharedHeadsSharedKeyValues/t5-efficient-small-shkv'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'NewT5SharedHeadsSharedKeyValues/t5-efficient-small-shkv' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

m3/m3-experiment-roberta-base-amcd-back-translation failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-roberta-base-amcd-back-translation/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5024-52d4ecfc074635cf115f7a05;0ab91d65-e147-4267-8ce8-96c273a93433)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-roberta-base-amcd-back-translation/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-roberta-base-amcd-back-translation is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

huak95/mt-align-finetuned-LST-en-to-th failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

teleportHQ/predicto_css failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/teleportHQ/predicto_css/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5026-1d0fbfaa030da9c5316b6873;15a8e55c-870d-4c03-a2c6-5ed4c543dd31)

Repository Not Found for url: https://huggingface.co/teleportHQ/predicto_css/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: teleportHQ/predicto_css is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Jungwoo4021/wav2vec2-base-ks-finetuning failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 789, in from_pretrained
    return tokenizer_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Jungwoo4021/wav2vec2-base-ks-finetuning'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Jungwoo4021/wav2vec2-base-ks-finetuning' is the correct path to a directory containing all relevant files for a Wav2Vec2CTCTokenizer tokenizer.

bowphs/ancient-t5-translation failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/bowphs/ancient-t5-translation/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f502f-597da782559cbb665d9e412d;830efa86-a11e-412d-b690-3fb1920c9c62)

Repository Not Found for url: https://huggingface.co/bowphs/ancient-t5-translation/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: bowphs/ancient-t5-translation is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Helsinki-NLP/opus-mt-art-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

gaeunseo/bert-base-finetuned-imdb failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'gaeunseo/bert-base-finetuned-imdb'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'gaeunseo/bert-base-finetuned-imdb' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

jimacasaet/SalamaThanksFIL2ENv3 failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-sv-NORWAY failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-kqn-sv failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-en-sw failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

dominguesm/positive-reframing-en failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-uk-cs failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Tarang1998/autonlp-pegasus-21664560 failed
Architectures: ['PegasusForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1221, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 981, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ADELIB/ANQG failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

miazhao/airberta_airbnb_dat_lang8_roberta_base failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/miazhao/airberta_airbnb_dat_lang8_roberta_base/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5094-36b99ebe60bb9ec8590e8d7e;9513e2d5-1983-4cf7-bf98-1f36d73f33e7)

Repository Not Found for url: https://huggingface.co/miazhao/airberta_airbnb_dat_lang8_roberta_base/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: miazhao/airberta_airbnb_dat_lang8_roberta_base is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

ghadeermobasher/BC4CHEMD-WLT-128-SciBERT failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC4CHEMD-WLT-128-SciBERT'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC4CHEMD-WLT-128-SciBERT' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

hossein20s/bert-base-multilingual-uncased-signature-segmentation-enrun1k failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'hossein20s/bert-base-multilingual-uncased-signature-segmentation-enrun1k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'hossein20s/bert-base-multilingual-uncased-signature-segmentation-enrun1k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

ghadeermobasher/WLT-BlueBERT-BC4CHEMD failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/WLT-BlueBERT-BC4CHEMD'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/WLT-BlueBERT-BC4CHEMD' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Timofey/PubMedBERT_Diseases_Side_Effects_Context_Classifier failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/Timofey/PubMedBERT_Diseases_Side_Effects_Context_Classifier/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-656f50d7-5dffd00b15c49add630df94e;a796979d-25a8-4b9c-b897-ddcd8ea208f1)

Cannot access gated repo for url https://huggingface.co/Timofey/PubMedBERT_Diseases_Side_Effects_Context_Classifier/resolve/main/tokenizer_config.json.
Repo model Timofey/PubMedBERT_Diseases_Side_Effects_Context_Classifier is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 445, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/Timofey/PubMedBERT_Diseases_Side_Effects_Context_Classifier and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.

m3/m3-experiment-albert-base-v2-citation-intent-eda-2 failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-citation-intent-eda-2/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f50d7-4252255650f5ea125083363a;79d02679-7f91-460e-9d8d-c04e8b43070e)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-citation-intent-eda-2/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-albert-base-v2-citation-intent-eda-2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

heliart/PhishingEmailGeneration failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'heliart/PhishingEmailGeneration'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'heliart/PhishingEmailGeneration' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

lmqg/t5-small-subjqa-grocery-qg failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

anas-awadalla/splinter-large-few-shot-k-128-finetuned-squad-seed-2 failed
Architectures: ['Data2VecTextForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/splinter/tokenization_splinter_fast.py", line 118, in __init__
    super().__init__(
TypeError: transformers.tokenization_utils_fast.PreTrainedTokenizerFast.__init__() got multiple values for keyword argument 'additional_special_tokens'

aliosm/sha3bor-rhyme-detector-arabertv02-base failed
Architectures: ['Data2VecTextForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/aliosm/sha3bor-rhyme-detector-arabertv02-base/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f50e0-64865c80185fee1c55d97378;a5f35dea-2c91-44a5-9303-9b3803d1cffa)

Repository Not Found for url: https://huggingface.co/aliosm/sha3bor-rhyme-detector-arabertv02-base/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: aliosm/sha3bor-rhyme-detector-arabertv02-base is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

figurative-nlp/Chinese-Simile-Generation failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: BartModel.forward() got an unexpected keyword argument 'token_type_ids'

sonoisa/t5-base-english-japanese failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


ghadeermobasher/BioRED-Dis-WLT-256-PubMedBERT failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BioRED-Dis-WLT-256-PubMedBERT'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BioRED-Dis-WLT-256-PubMedBERT' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

MartinoMensio/racism-models-m-vote-strict-epoch-4 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'MartinoMensio/racism-models-m-vote-strict-epoch-4'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'MartinoMensio/racism-models-m-vote-strict-epoch-4' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Ayham/ernie_ernie_summarization_cnn_dailymail failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Ayham/ernie_ernie_summarization_cnn_dailymail'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Ayham/ernie_ernie_summarization_cnn_dailymail' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Malaina/mt5-large-spider failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

monobyte/byt5-mono-hierarchical-v1 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

wudi7758521521/kaikai_model2 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2056, in _from_pretrained
    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert.py", line 199, in __init__
    if not os.path.isfile(vocab_file):
  File "/anaconda/envs/amc/lib/python3.10/genericpath.py", line 30, in isfile
    st = os.stat(path)
TypeError: stat: path should be string, bytes, os.PathLike or integer, not NoneType

annahaz/roberta-large-mnli-misogyny-sexism-4tweets-3e-05-0.07 failed
Architectures: ['RobertaForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/annahaz/roberta-large-mnli-misogyny-sexism-4tweets-3e-05-0.07/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5122-0eeab46c2374f6445577faae;6cfa35c9-40c8-41c8-a027-1d35a81603bc)

Repository Not Found for url: https://huggingface.co/annahaz/roberta-large-mnli-misogyny-sexism-4tweets-3e-05-0.07/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: annahaz/roberta-large-mnli-misogyny-sexism-4tweets-3e-05-0.07 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Iwa/bot failed
Architectures: ['RobertaForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/Iwa/bot/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f5122-6ba128322835270365bcb846;449cbc4a-cee3-48b9-9fdc-78c05c52c7ba)

Entry Not Found for url: https://huggingface.co/Iwa/bot/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: Iwa/bot does not appear to have a file named config.json. Checkout 'https://huggingface.co/Iwa/bot/main' for available files.

bowphs/finally failed
Architectures: ['XLMRobertaModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/bowphs/finally/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5133-316d08181cecd797253b45c4;770a4e00-9e99-488d-9a9d-ca50897ecbb3)

Repository Not Found for url: https://huggingface.co/bowphs/finally/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: bowphs/finally is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

tau/False_large_pmi_para0_sent1_span2_itFalse_ssoftmax_rrFalse_8_1024_0.15_1 failed
Architectures: ['XLMRobertaModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


KoboldAI/fairseq-dense-13B-Nerys-v2 failed
Architectures: ['XGLMForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 31, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py", line 585, in new_func
    hidden_states = inputs_embeds + self.embed_positions(position_ids, past_key_values_length)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 114, in new_func
    with ctx_factory():
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 974, in torch_no_grad_enter_wrapper
    return self.create_proxy('call_function', _orig_torch_no_grad_enter, (no_grad,), {})
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 446, in create_proxy
    args_ = self.create_arg(args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 555, in create_arg
    return super().create_arg(a)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/proxy.py", line 239, in create_arg
    return type(a)(self.create_arg(elem) for elem in a)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/proxy.py", line 239, in <genexpr>
    return type(a)(self.create_arg(elem) for elem in a)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 555, in create_arg
    return super().create_arg(a)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/proxy.py", line 267, in create_arg
    raise NotImplementedError(f"argument of type: {type(a)}")
NotImplementedError: argument of type: <class 'torch.autograd.grad_mode.no_grad'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 33, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

prateeky2806/bert-base-uncased-finetuned-cola failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/prateeky2806/bert-base-uncased-finetuned-cola/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f51c4-303356ab6d4333bc1167733c;b47375ee-5f2d-49c4-9e52-efe12f485fda)

Repository Not Found for url: https://huggingface.co/prateeky2806/bert-base-uncased-finetuned-cola/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: prateeky2806/bert-base-uncased-finetuned-cola is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

miguelvictor/python-t5-base failed
Architectures: ['T5WithLMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

MartinoMensio/racism-models-w-m-vote-strict-epoch-2 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'MartinoMensio/racism-models-w-m-vote-strict-epoch-2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'MartinoMensio/racism-models-w-m-vote-strict-epoch-2' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

overgrowth/jokeboy failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/overgrowth/jokeboy/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f5212-7504c04b2e0d8329112683e3;d129b7ff-f343-43cf-98fc-bcb012a6b676)

Entry Not Found for url: https://huggingface.co/overgrowth/jokeboy/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: overgrowth/jokeboy does not appear to have a file named config.json. Checkout 'https://huggingface.co/overgrowth/jokeboy/main' for available files.

anas-awadalla/t5-small-few-shot-k-1024-finetuned-squad-seed-2 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Gunulhona/tbstmodel_v4 failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: BartModel.forward() got an unexpected keyword argument 'token_type_ids'

xlm-mlm-enfr-1024 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm/tokenization_xlm.py", line 617, in __init__
    import sacremoses
ModuleNotFoundError: No module named 'sacremoses'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 789, in from_pretrained
    return tokenizer_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm/tokenization_xlm.py", line 619, in __init__
    raise ImportError(
ImportError: You need to install sacremoses to use XLMTokenizer. See https://pypi.org/project/sacremoses/ for installation.

NLP-Search/final-model failed
Architectures: ['GectorbertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'NLP-Search/final-model'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'NLP-Search/final-model' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

amphora/FinABSA-Longer failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ShwetActhq/autotrain-signature_extraction-1673759349 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/ShwetActhq/autotrain-signature_extraction-1673759349/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f524c-752c28572c465e6536651571;088663ff-212a-46fc-9980-6e4b0a3a72fa)

Repository Not Found for url: https://huggingface.co/ShwetActhq/autotrain-signature_extraction-1673759349/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: ShwetActhq/autotrain-signature_extraction-1673759349 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

atomsspawn/DialoGPT-small-sheldon failed
Architectures: ['DistilBertModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/atomsspawn/DialoGPT-small-sheldon/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f526f-60bbf2604b9992b546722993;eac8bff7-f3b5-418a-8d1f-9e61c1c2af16)

Entry Not Found for url: https://huggingface.co/atomsspawn/DialoGPT-small-sheldon/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: atomsspawn/DialoGPT-small-sheldon does not appear to have a file named config.json. Checkout 'https://huggingface.co/atomsspawn/DialoGPT-small-sheldon/main' for available files.

Helsinki-NLP/opus-mt-de-ase failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

zeineb/BART-LearningQ failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/zeineb/BART-LearningQ/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5292-090d253b545fde8022b348a8;a6937290-acf1-4be7-b216-16e1dfeeda46)

Repository Not Found for url: https://huggingface.co/zeineb/BART-LearningQ/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: zeineb/BART-LearningQ is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

m3/m3-experiment-albert-base-v2-citation-intent-word-swapping-random-1 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-citation-intent-word-swapping-random-1/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f52b1-6744660a20ef5aa60726064e;86571193-53f8-4fc5-b3de-7f54f1604a9a)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-citation-intent-word-swapping-random-1/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-albert-base-v2-citation-intent-word-swapping-random-1 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

ardauzunoglu/mT5-en-to-tr failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/ardauzunoglu/mT5-en-to-tr/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f52b7-32355a980937db2655dc184a;259897c9-3791-41ba-83a4-8aa70c4424bf)

Repository Not Found for url: https://huggingface.co/ardauzunoglu/mT5-en-to-tr/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: ardauzunoglu/mT5-en-to-tr is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

HJOK/free-bart-v5 failed
Architectures: ['CamembertForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/HJOK/free-bart-v5/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f52dc-38124e8b2a1dc3a44e35a8d3;f664ac65-fcf5-4b5e-8bb1-654438da6ca8)

Repository Not Found for url: https://huggingface.co/HJOK/free-bart-v5/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: HJOK/free-bart-v5 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

airesearch/xlm-roberta-base-finetuned failed
Architectures: ['CamembertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'airesearch/xlm-roberta-base-finetuned'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'airesearch/xlm-roberta-base-finetuned' is the correct path to a directory containing all relevant files for a XLMRobertaTokenizerFast tokenizer.

knok/japanese-distilgpt2 failed
Architectures: ['XLMRobertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'knok/japanese-distilgpt2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'knok/japanese-distilgpt2' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

diegozs97/sciie-seed-4-1000k failed
Architectures: ['MPNetForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/sciie-seed-4-1000k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/sciie-seed-4-1000k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Toshifumi/distilbert-base-uncased-finetuned-emotion failed
Architectures: ['MPNetForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/Toshifumi/distilbert-base-uncased-finetuned-emotion/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f52f7-1583dd7b558340e278c26a2f;3588ddce-1b92-4f80-8c56-eedf4e074ec1)

Repository Not Found for url: https://huggingface.co/Toshifumi/distilbert-base-uncased-finetuned-emotion/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: Toshifumi/distilbert-base-uncased-finetuned-emotion is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

prajjwal1/ctrl_discovery_flipped_4 failed
Architectures: ['CTRLLMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_proxy.py", line 79, in __getattr__
    return ConcreteAttrProxy(self, k)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_proxy.py", line 290, in __init__
    self.value = _orig_getattr(root.value, attr)
AttributeError: 'int' object has no attribute 'sqrt'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 31, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/ctrl/modeling_ctrl.py", line 473, in new_func
    outputs = h(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/ctrl/modeling_ctrl.py", line 188, in new_func
    attn_outputs = self.multi_head_attention(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/ctrl/modeling_ctrl.py", line 155, in new_func
    output = scaled_dot_product_attention(q, k, v, mask, attention_mask, head_mask)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/ctrl/modeling_ctrl.py", line 67, in new_func
    scaled_attention_logits = matmul_qk / np.sqrt(dk)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
TypeError: loop of ufunc does not support argument 0 of type ConcreteProxy which has no callable sqrt method

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 33, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

botisan-ai/mt5-translate-yue-zh failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

AlekseyKorshuk/gpt-j-6B failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'AlekseyKorshuk/gpt-j-6B'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'AlekseyKorshuk/gpt-j-6B' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

Bistolero/mix_training_en_du_nl_1 failed
Architectures: ['GectorbertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Einmalumdiewelt/PegasusXSUM_GNAD failed
Architectures: ['PegasusForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1221, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 981, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

prodm93/t5_sum1_modelchkpnt1 failed
Architectures: ['RobertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'prodm93/t5_sum1_modelchkpnt1'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'prodm93/t5_sum1_modelchkpnt1' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

saekomdalkom/t5-small-finetuned-xsum failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Declan/NPR_model_v8 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Declan/NPR_model_v8'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Declan/NPR_model_v8' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

gsliwoski/foodtranslator_tinybert_pretrained failed
Architectures: ['EncoderDecoderModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 101, in <module>
    model = AutoModel.from_config(config, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 443, in from_config
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig'> for this kind of AutoModel: AutoModel.
Model type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, LlamaConfig, CodeGenConfig, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MistralConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SamConfig, SeamlessM4TConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VitDetConfig, VitsConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig, XLMRobertaConfig, LSGBartConfig.

valurank/en_pos_counter failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/valurank/en_pos_counter/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f53d0-5e4957c91bf7f0aa5d535d1b;d895a78f-9fd3-45ec-bfb3-be7df014b0b1)

Entry Not Found for url: https://huggingface.co/valurank/en_pos_counter/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: valurank/en_pos_counter does not appear to have a file named config.json. Checkout 'https://huggingface.co/valurank/en_pos_counter/main' for available files.

ghadeermobasher/BC2GM-WLT-256-PubMedBERT-latest failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC2GM-WLT-256-PubMedBERT-latest'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC2GM-WLT-256-PubMedBERT-latest' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-fr-ro failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

caotouchan/distilbert_uncase_emo failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'caotouchan/distilbert_uncase_emo'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'caotouchan/distilbert_uncase_emo' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

Salesforce/codegen-6B-multi failed
Architectures: ['CodeGenForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 109, in <module>
    after_trace = traced_gm(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 662, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 281, in __call__
    raise e
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 271, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "<eval_with_key>.811", line 77, in forward
    getitem_17 = to_1[unsqueeze];  to_1 = None
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_proxy.py", line 431, in impl
    return tracer.create_proxy('call_function', target, args, kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 444, in create_proxy
    value_unwrapped = self.run_target(kind, target, args_unwrapped, kwargs_unwrapped)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 374, in run_target
    result = run(kind, target, args, kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 347, in run
    return OperatorPatcherContext.patch_run(fn, *args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 291, in patch_run
    assert OperatorPatcherContext.ctx_tracer is not None
AssertionError

Helsinki-NLP/opus-mt-de-ho failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Declan/NewYorkTimes_model_v6 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Declan/NewYorkTimes_model_v6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Declan/NewYorkTimes_model_v6' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

WillHeld/t5-base-pointer-adv-cstop_artificial failed
Architectures: ['AlignedMT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

unicamp-dl/mt5-base-en-msmarco failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


AtulSingh31/t5-small-finetuned-xsum failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

jungyong/FT_cls_adamw_hf failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'jungyong/FT_cls_adamw_hf'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'jungyong/FT_cls_adamw_hf' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

AhiyaB/mt5-small-finetuned-Big-Patent-h failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

fadhilarkan/gq-indo-k failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

riccardode/political_bias failed
Architectures: None
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'riccardode/political_bias'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'riccardode/political_bias' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

allenai/mtk-instruct-11b-def-pos failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


jky594176/recipe_bart2_v3 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'jky594176/recipe_bart2_v3'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'jky594176/recipe_bart2_v3' is the correct path to a directory containing all relevant files for a BartTokenizerFast tokenizer.

unicamp-dl/ptt5-small-portuguese-vocab failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


porpaul/t5-small-finetuned-xsum failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

faust/broken_t5_squad2 failed
Architectures: ['DebertaV2ForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


prodm93/gpt2_rn_ep2_model failed
Architectures: ['DebertaV2ForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'prodm93/gpt2_rn_ep2_model'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'prodm93/gpt2_rn_ep2_model' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

navjordj/flan-t5-small_en-no failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

lmqg/t5-base-squadshifts-nyt-qg failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-sv-hil failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Lya/biobert-ehr-re failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/Lya/biobert-ehr-re/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-656f552a-7ec4279d34f85b0506810ed4;08046f0f-0cf9-47d3-ae52-2c761402318e)

Cannot access gated repo for url https://huggingface.co/Lya/biobert-ehr-re/resolve/main/tokenizer_config.json.
Repo model Lya/biobert-ehr-re is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 445, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/Lya/biobert-ehr-re and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.

ThuanPhong/xlm-roberta-large-finetuned-viquad failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/ThuanPhong/xlm-roberta-large-finetuned-viquad/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f552a-10c15ceb0834153d64eb2643;17cffdf0-f79d-4fbe-9928-c4623bc6333b)

Repository Not Found for url: https://huggingface.co/ThuanPhong/xlm-roberta-large-finetuned-viquad/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: ThuanPhong/xlm-roberta-large-finetuned-viquad is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Helsinki-NLP/opus-mt-es-gaa failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

KennethEnevoldsen/dfm-debertav2-small-v1-2048bsz-1Msteps-80dagw-5twitter-10news-5web failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/KennethEnevoldsen/dfm-debertav2-small-v1-2048bsz-1Msteps-80dagw-5twitter-10news-5web/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5557-4d9e3bad77950f0123d155f1;dcb765b6-207e-43f0-9171-b333f583a3c7)

Repository Not Found for url: https://huggingface.co/KennethEnevoldsen/dfm-debertav2-small-v1-2048bsz-1Msteps-80dagw-5twitter-10news-5web/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: KennethEnevoldsen/dfm-debertav2-small-v1-2048bsz-1Msteps-80dagw-5twitter-10news-5web is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

MorrisPark/twc9-bart-r3f failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: BartModel.forward() got an unexpected keyword argument 'token_type_ids'

Helsinki-NLP/opus-mt-uk-sl failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

asanwari/agriculture-sentence-transformer failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/asanwari/agriculture-sentence-transformer/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f555a-54e3f5de26de26021e864821;e5bfb034-7de6-4a3d-8e13-fa9e28594b77)

Entry Not Found for url: https://huggingface.co/asanwari/agriculture-sentence-transformer/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: asanwari/agriculture-sentence-transformer does not appear to have a file named config.json. Checkout 'https://huggingface.co/asanwari/agriculture-sentence-transformer/main' for available files.

m3/m3-experiment-roberta-base-rct-sample-eda-4 failed
Architectures: ['MPNetModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-roberta-base-rct-sample-eda-4/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f555e-51d3a3e950a155a851374fef;9483e397-c8cd-405f-b06b-165d41552b13)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-roberta-base-rct-sample-eda-4/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-roberta-base-rct-sample-eda-4 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

m3/m3-experiment-albert-base-v2-tweet-eval-hate-vanilla failed
Architectures: ['MPNetModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-tweet-eval-hate-vanilla/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f555e-49e9e8c77d1a5cdc051797d8;dcd52669-6372-4e42-bbc0-562a39152beb)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-tweet-eval-hate-vanilla/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-albert-base-v2-tweet-eval-hate-vanilla is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

SebastianS/marian-finetuned-kde4-en-to-fr-accelerate failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

danyaljj/gpt-j-6B-step-318500 failed
Architectures: ['GPTJForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 109, in <module>
    after_trace = traced_gm(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 662, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 281, in __call__
    raise e
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 271, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "<eval_with_key>.947", line 59, in forward
    getitem_9 = getattr_8[-1];  getattr_8 = None
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_proxy.py", line 431, in impl
    return tracer.create_proxy('call_function', target, args, kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 444, in create_proxy
    value_unwrapped = self.run_target(kind, target, args_unwrapped, kwargs_unwrapped)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 374, in run_target
    result = run(kind, target, args, kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 347, in run
    return OperatorPatcherContext.patch_run(fn, *args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 291, in patch_run
    assert OperatorPatcherContext.ctx_tracer is not None
AssertionError

huggingface-course/mt5-finetuned-amazon-en-es-accelerate failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

evangeloc/t5-small-finetuned-xsum_3epoch_batch8 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

inovex/multi2convai-quality-de-logreg-ft failed
Architectures: ['XLMRobertaModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/inovex/multi2convai-quality-de-logreg-ft/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f55f5-00e360797396821f36d9680f;77699d66-9016-4259-9789-b9b6130d3c93)

Entry Not Found for url: https://huggingface.co/inovex/multi2convai-quality-de-logreg-ft/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: inovex/multi2convai-quality-de-logreg-ft does not appear to have a file named config.json. Checkout 'https://huggingface.co/inovex/multi2convai-quality-de-logreg-ft/main' for available files.

fxmarty/20220713-h08m45s49_example_squad failed
Architectures: ['XLMRobertaModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/fxmarty/20220713-h08m45s49_example_squad/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f55f5-503942262a04619c4c8f071c;9169116b-35a2-4324-b3f4-deeb81d43d49)

Entry Not Found for url: https://huggingface.co/fxmarty/20220713-h08m45s49_example_squad/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: fxmarty/20220713-h08m45s49_example_squad does not appear to have a file named config.json. Checkout 'https://huggingface.co/fxmarty/20220713-h08m45s49_example_squad/main' for available files.

sanchit-gandhi/bloom-1b3-scan failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'sanchit-gandhi/bloom-1b3-scan'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sanchit-gandhi/bloom-1b3-scan' is the correct path to a directory containing all relevant files for a BloomTokenizerFast tokenizer.

TransQuest/monotransquest-hter-en_de-it-nmt failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Helsinki-NLP/opus-mt-fi-ts failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-pl-eo failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Cheatham/xlm-roberta-large-finetuned3 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


mughalk4/mBERT-RomanceLang failed
Architectures: ['XLMRobertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'mughalk4/mBERT-RomanceLang'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'mughalk4/mBERT-RomanceLang' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

SaulLu/cotet5_small_fix failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

lddczcn/pegasus-samsum failed
Architectures: ['PegasusForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1221, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 981, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

masakhane/m2m100_418M_mos_fr_rel failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1181, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 966, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

zhuqing/roberta-base-uncased-AutoModelWithLMHeadnetmums-classification failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'zhuqing/roberta-base-uncased-AutoModelWithLMHeadnetmums-classification'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'zhuqing/roberta-base-uncased-AutoModelWithLMHeadnetmums-classification' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

imjeffhi/syllabizer failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

tanapatentlm/patentdeberta_base_total_1024_pwi failed
Architectures: ['DebertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 31, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 974, in new_func
    encoder_outputs = self.encoder(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 470, in new_func
    hidden_states = layer_module(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 383, in new_func
    attention_output = self.attention(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 316, in new_func
    self_output = self.self(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 661, in new_func
    rel_att = self.disentangled_att_bias(query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 710, in new_func
    c2p_att = torch.gather(c2p_att, dim=-1, index=c2p_dynamic_expand(c2p_pos, query_layer, relative_pos))
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
RuntimeError: c2p_dynamic_expand() Expected a value of type 'Tensor (inferred)' for argument 'c2p_pos' but instead found type 'ConcreteProxy'.
Inferred 'c2p_pos' to be of type 'Tensor' because it was not annotated with an explicit type.
Position: 0
Value: ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]]))
Declaration: c2p_dynamic_expand(Tensor c2p_pos, Tensor query_layer, Tensor relative_pos) -> Tensor
Cast error details: Unable to cast ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]])) to Tensor

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 33, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

Helsinki-NLP/opus-mt-zne-fi failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

hf-internal-testing/tiny-random-CTRLLMHeadModel failed
Architectures: ['CTRLLMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_proxy.py", line 79, in __getattr__
    return ConcreteAttrProxy(self, k)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_proxy.py", line 290, in __init__
    self.value = _orig_getattr(root.value, attr)
AttributeError: 'int' object has no attribute 'sqrt'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 31, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/ctrl/modeling_ctrl.py", line 473, in new_func
    outputs = h(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/ctrl/modeling_ctrl.py", line 188, in new_func
    attn_outputs = self.multi_head_attention(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/ctrl/modeling_ctrl.py", line 155, in new_func
    output = scaled_dot_product_attention(q, k, v, mask, attention_mask, head_mask)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/ctrl/modeling_ctrl.py", line 67, in new_func
    scaled_attention_logits = matmul_qk / np.sqrt(dk)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
TypeError: loop of ufunc does not support argument 0 of type ConcreteProxy which has no callable sqrt method

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 33, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

Katrzyna/bert-base-cased-finetuned-basil failed
Architectures: ['RobertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Katrzyna/bert-base-cased-finetuned-basil'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Katrzyna/bert-base-cased-finetuned-basil' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Jeevesh8/t5-small-cogs_22 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

JavierIA/es-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

semindan/xnli_xlm_r_base_broken failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/semindan/xnli_xlm_r_base_broken/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f56c9-53eba3fc279e28af1f7f7da8;58ac9e08-cc7d-403a-a36d-b120b08a0f53)

Repository Not Found for url: https://huggingface.co/semindan/xnli_xlm_r_base_broken/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: semindan/xnli_xlm_r_base_broken is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

sohamtiwari3120/testing_optimal_20 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'sohamtiwari3120/testing_optimal_20'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sohamtiwari3120/testing_optimal_20' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

facebook/blenderbot-90M failed
Architectures: ['BlenderbotSmallForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py", line 1137, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py", line 921, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

SEBIS/code_trans_t5_small_api_generation failed
Architectures: ['BloomForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


WurmWillem/DialoGPT-medium-RickandMorty3 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/WurmWillem/DialoGPT-medium-RickandMorty3/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f56dc-0de4b6a03b8726726e6ab0d2;b3395589-9912-4467-aed0-ac1c748c8d95)

Entry Not Found for url: https://huggingface.co/WurmWillem/DialoGPT-medium-RickandMorty3/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: WurmWillem/DialoGPT-medium-RickandMorty3 does not appear to have a file named config.json. Checkout 'https://huggingface.co/WurmWillem/DialoGPT-medium-RickandMorty3/main' for available files.

mboth/klassifizierungWaermeVerteilenKHLowercase failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/mboth/klassifizierungWaermeVerteilenKHLowercase/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f56e6-3d97bfbb3f60266f1d3d631e;8847a16c-9e76-411d-8545-03496e66e34e)

Repository Not Found for url: https://huggingface.co/mboth/klassifizierungWaermeVerteilenKHLowercase/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: mboth/klassifizierungWaermeVerteilenKHLowercase is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

jinujinu99/t5-ep3-parabk2 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

negfir/bert_uncased_L-4_H-128_A-2wiki103 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'negfir/bert_uncased_L-4_H-128_A-2wiki103'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'negfir/bert_uncased_L-4_H-128_A-2wiki103' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

ghadeermobasher/BC4CHEMD-WLT-384-PubMedBERT-Trial-30 failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC4CHEMD-WLT-384-PubMedBERT-Trial-30'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC4CHEMD-WLT-384-PubMedBERT-Trial-30' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

paola-md/recipe-distilbert-i failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'paola-md/recipe-distilbert-i'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'paola-md/recipe-distilbert-i' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

QuickRead/pegasus-reddit-7e05-new failed
Architectures: ['PegasusForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1221, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 981, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

diegozs97/finetuned-sciie-seed-0-20k failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/finetuned-sciie-seed-0-20k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/finetuned-sciie-seed-0-20k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

elopezlopez/xlnet-base-cased_fold_10_binary_v1 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlnet/tokenization_xlnet_fast.py", line 150, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLNetConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Salvatore/test-mention-norm failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 789, in from_pretrained
    return tokenizer_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Salvatore/test-mention-norm'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Salvatore/test-mention-norm' is the correct path to a directory containing all relevant files for a MarianTokenizer tokenizer.

ErykWdowiak/GPTalian failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/ErykWdowiak/GPTalian/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5729-612faf3f214a71a90b6bddaa;bcbe9b17-2e89-41ff-a85b-6f3de99e4f72)

Repository Not Found for url: https://huggingface.co/ErykWdowiak/GPTalian/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: ErykWdowiak/GPTalian is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

samwell/marian-finetuned-kde4-en-to-fr failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

afif00/pretrain-bert-imdb failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/afif00/pretrain-bert-imdb/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f573b-37fba54d4f870235424592d8;f752b568-a935-40d7-9325-54eafda366ca)

Repository Not Found for url: https://huggingface.co/afif00/pretrain-bert-imdb/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: afif00/pretrain-bert-imdb is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

mrm8488/t5-base-finetuned-boolq failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


akhooli/xlm-r-large-arabic-sent failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


tmagajna/test failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/tmagajna/test/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f575b-575fb0293d51dbda7d2b8c82;63648810-e8b5-4a30-8377-4cf59df4ec6d)

Entry Not Found for url: https://huggingface.co/tmagajna/test/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: tmagajna/test does not appear to have a file named config.json. Checkout 'https://huggingface.co/tmagajna/test/main' for available files.

Helsinki-NLP/opus-mt-tc-big-fi-zls failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Jeevesh8/t5-small-cogs_11 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

yogeshchandrasekharuni/fast-food-entity-extraction-amazon-v1 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'yogeshchandrasekharuni/fast-food-entity-extraction-amazon-v1'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'yogeshchandrasekharuni/fast-food-entity-extraction-amazon-v1' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-no-uk failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

SetFit/deberta-v3-large__sst2__train-16-7 failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py", line 133, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
DebertaV2Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Helsinki-NLP/opus-mt-es-efi failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-sg-fi failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ghadeermobasher/NCBI-disease-WLT-384-SciBERT-40 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/NCBI-disease-WLT-384-SciBERT-40'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/NCBI-disease-WLT-384-SciBERT-40' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

kangaroo927/en_pipeline failed
Architectures: ['MPNetForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/kangaroo927/en_pipeline/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f57d6-791ccd0e25e991687384972c;d3987437-077a-4f43-939e-44c9e3cfba12)

Entry Not Found for url: https://huggingface.co/kangaroo927/en_pipeline/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: kangaroo927/en_pipeline does not appear to have a file named config.json. Checkout 'https://huggingface.co/kangaroo927/en_pipeline/main' for available files.

krm/mt5-small-OrangeSum-Summarizer failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

pollner/dnabertregressor-prediction failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/pollner/dnabertregressor-prediction/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f57dc-09d9661702bf7a1059623557;5584cc53-cb31-4eb3-ab68-cf9b90f06f49)

Repository Not Found for url: https://huggingface.co/pollner/dnabertregressor-prediction/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: pollner/dnabertregressor-prediction is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

EslamAhmed/google_Job_data_tuned_trial_2_11-2-2022 failed
Architectures: ['DistilBertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: DistilBertModel.forward() got an unexpected keyword argument 'token_type_ids'

Helsinki-NLP/opus-mt-sv-ts failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

casasdorjunior/t5-small-finetuned-xlsum failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

seidel/plsum-base-ptt5 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

m3/m3-experiment-albert-base-v2-amcd-word-swapping-random-2 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-amcd-word-swapping-random-2/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f581c-4698e1af53cd378d5cfb3504;7e1f528f-a84f-4314-aa4a-866938a86ce1)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-amcd-word-swapping-random-2/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-albert-base-v2-amcd-word-swapping-random-2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

masakhane/m2m100_418M_fr_ewe_news failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1181, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 966, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

SetFit/deberta-v3-large__sst2__train-8-5 failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py", line 133, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
DebertaV2Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


diegozs97/sciie-seed-4-400k failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/sciie-seed-4-400k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/sciie-seed-4-400k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

diegozs97/finetuned-chemprot-seed-2-20k failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/finetuned-chemprot-seed-2-20k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/finetuned-chemprot-seed-2-20k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

ChaiML/dalio_combined_epoch_15_lr_9e-7 failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/ChaiML/dalio_combined_epoch_15_lr_9e-7/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5898-6874fd3c77ec1967620a61a9;91b86347-f506-4561-b6a2-19646df74540)

Repository Not Found for url: https://huggingface.co/ChaiML/dalio_combined_epoch_15_lr_9e-7/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: ChaiML/dalio_combined_epoch_15_lr_9e-7 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

RTM/ChatBot failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/RTM/ChatBot/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f58c4-43258f131f86bd512ae41777;e70b5082-97d3-4ad5-b2f6-d7d48a0790ca)

Entry Not Found for url: https://huggingface.co/RTM/ChatBot/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: RTM/ChatBot does not appear to have a file named config.json. Checkout 'https://huggingface.co/RTM/ChatBot/main' for available files.

wandgibaut/opus-mt-en-de-finetuned-en-to-de failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

google/t5-efficient-small-el12 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

facebook/wmt19-ru-en failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/fsmt/tokenization_fsmt.py", line 201, in __init__
    import sacremoses
ModuleNotFoundError: No module named 'sacremoses'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 789, in from_pretrained
    return tokenizer_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/fsmt/tokenization_fsmt.py", line 203, in __init__
    raise ImportError(
ImportError: You need to install sacremoses to use XLMTokenizer. See https://pypi.org/project/sacremoses/ for installation.

ardallie/distilbert-base-uncased-finetuned-emotion failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/ardallie/distilbert-base-uncased-finetuned-emotion/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f58e0-01d2ae50696d758d4774d83f;e09ccd4b-da7e-4cb5-abfc-e887d815ef5b)

Repository Not Found for url: https://huggingface.co/ardallie/distilbert-base-uncased-finetuned-emotion/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: ardallie/distilbert-base-uncased-finetuned-emotion is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

KenP/marian-finetuned-kde4-en-to-fr failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

XLab/rst-word-sense-disambiguation-11b failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

bowphs/morphoberta-universal-perseus failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/bowphs/morphoberta-universal-perseus/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5960-234b28d05a3c518130faabc7;92517030-591b-40e9-b20a-07b1f2302e09)

Repository Not Found for url: https://huggingface.co/bowphs/morphoberta-universal-perseus/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: bowphs/morphoberta-universal-perseus is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

CEBaB/lstm.CEBaB.sa.5-class.exclusive.seed_77 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'CEBaB/lstm.CEBaB.sa.5-class.exclusive.seed_77'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'CEBaB/lstm.CEBaB.sa.5-class.exclusive.seed_77' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

microsoft/codereviewer failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

laxya007/gpt2_TS_DM_AS_CC_TM_HCU_DBS failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/laxya007/gpt2_TS_DM_AS_CC_TM_HCU_DBS/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5975-50a3ab261b0681423183e37f;185dc8ac-fe4e-4c56-90cf-bc1b60a82538)

Repository Not Found for url: https://huggingface.co/laxya007/gpt2_TS_DM_AS_CC_TM_HCU_DBS/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: laxya007/gpt2_TS_DM_AS_CC_TM_HCU_DBS is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

BoyBurin/distilbert-base-uncased-finetuned-emotion failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/BoyBurin/distilbert-base-uncased-finetuned-emotion/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5977-4df4053e6a06553b5c44559f;37f210d9-e4ed-4471-a52a-cd9588a427e9)

Repository Not Found for url: https://huggingface.co/BoyBurin/distilbert-base-uncased-finetuned-emotion/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: BoyBurin/distilbert-base-uncased-finetuned-emotion is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

disenwang1994/bert-base-1e-5 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/disenwang1994/bert-base-1e-5/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5987-08c2aac85d9ad3bc121d7c96;5e919ccc-702e-4f11-9443-69f3b5e94209)

Repository Not Found for url: https://huggingface.co/disenwang1994/bert-base-1e-5/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: disenwang1994/bert-base-1e-5 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Tminus1/SumBot failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/Tminus1/SumBot/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f5987-100ed507578ea3a51157e826;b030aa7f-7cbd-45cd-aff5-23f93647c4e0)

Entry Not Found for url: https://huggingface.co/Tminus1/SumBot/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: Tminus1/SumBot does not appear to have a file named config.json. Checkout 'https://huggingface.co/Tminus1/SumBot/main' for available files.

ptaszynski/japan-orientalization-pl failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/herbert/tokenization_herbert.py", line 338, in __init__
    import sacremoses
ModuleNotFoundError: No module named 'sacremoses'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2056, in _from_pretrained
    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/herbert/tokenization_herbert.py", line 340, in __init__
    raise ImportError(
ImportError: You need to install sacremoses to use HerbertTokenizer. See https://pypi.org/project/sacremoses/ for installation.

research-backup/t5-large-squadshifts-vanilla-reddit-qg failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Rostlab/prot_t5_xl_bfd failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Tritkoman/autotrain-gahhaha-1478754178 failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

SEBIS/legal_t5_small_trans_sv_it failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


frtna/jwt300_mt-Italian-to-Spanish failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

helmi0695/det5-base failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Helsinki-NLP/opus-mt-da-fi failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

PSW/t5-base-samsum-seed33 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

google/t5-v1_1-xl failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Helsinki-NLP/opus-mt-vi-it failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Hoax0930/kyoto_marian failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

hf-internal-testing/tiny-random-RoFormerForCausalLM failed
Architectures: ['RoFormerForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 31, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/roformer/modeling_roformer.py", line 914, in new_func
    encoder_outputs = self.encoder(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/roformer/modeling_roformer.py", line 570, in new_func
    sinusoidal_pos = self.embed_positions(hidden_states.shape[:-1], past_key_values_length)[None, None, :, :]
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 114, in new_func
    with ctx_factory():
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 974, in torch_no_grad_enter_wrapper
    return self.create_proxy('call_function', _orig_torch_no_grad_enter, (no_grad,), {})
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 446, in create_proxy
    args_ = self.create_arg(args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 555, in create_arg
    return super().create_arg(a)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/proxy.py", line 239, in create_arg
    return type(a)(self.create_arg(elem) for elem in a)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/proxy.py", line 239, in <genexpr>
    return type(a)(self.create_arg(elem) for elem in a)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 555, in create_arg
    return super().create_arg(a)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/proxy.py", line 267, in create_arg
    raise NotImplementedError(f"argument of type: {type(a)}")
NotImplementedError: argument of type: <class 'torch.autograd.grad_mode.no_grad'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 33, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

boychaboy/SNLI_bert-large-uncased failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/boychaboy/SNLI_bert-large-uncased/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5a2d-0ef5283232cf1a6559074fe2;54f9db39-cc56-4696-9484-56846959a9d3)

Repository Not Found for url: https://huggingface.co/boychaboy/SNLI_bert-large-uncased/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: boychaboy/SNLI_bert-large-uncased is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

ran/y7 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ran/y7'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ran/y7' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

zhifei/autotrain-chineses-title-summarization-3-1087939403 failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

dbernsohn/algebra_linear_1d_composed failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


davidcechak/DNADeberta_finedemo_coding_vs_intergenomic_seqs failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'davidcechak/DNADeberta_finedemo_coding_vs_intergenomic_seqs'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'davidcechak/DNADeberta_finedemo_coding_vs_intergenomic_seqs' is the correct path to a directory containing all relevant files for a DebertaTokenizerFast tokenizer.

KeLiu/QETRA_CSharp failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


WikinewsSum/bert2bert-multi-en-wiki-news failed
Architectures: ['EncoderDecoderModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 101, in <module>
    model = AutoModel.from_config(config, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 443, in from_config
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig'> for this kind of AutoModel: AutoModel.
Model type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, LlamaConfig, CodeGenConfig, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MistralConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SamConfig, SeamlessM4TConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VitDetConfig, VitsConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig, XLMRobertaConfig, LSGBartConfig.

LYTinn/finetuning-sentiment-model-3000-samples failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/LYTinn/finetuning-sentiment-model-3000-samples/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-656f5aa6-7406bb8d4e6b52bf6c8e4869;96f19711-95d2-405d-a523-2b5973eb4c4b)

Cannot access gated repo for url https://huggingface.co/LYTinn/finetuning-sentiment-model-3000-samples/resolve/main/tokenizer_config.json.
Repo model LYTinn/finetuning-sentiment-model-3000-samples is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 445, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/LYTinn/finetuning-sentiment-model-3000-samples and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.

ChiefTheLord/chief-camembert-base failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/ChiefTheLord/chief-camembert-base/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5aa6-4df385cc1c92c2fc34b35606;2ec0bbab-d821-4242-b145-95a842d6ff2d)

Repository Not Found for url: https://huggingface.co/ChiefTheLord/chief-camembert-base/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: ChiefTheLord/chief-camembert-base is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

scikit-learn/transformers-imdb failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1073, in from_pretrained
    raise ValueError(
ValueError: Unrecognized model in scikit-learn/transformers-imdb. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chinese_clip, clap, clip, clipseg, code_llama, codegen, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, data2vec-audio, data2vec-text, data2vec-vision, deberta, deberta-v2, decision_transformer, deformable_detr, deit, deta, detr, dinat, dinov2, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, git, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, graphormer, groupvit, hubert, ibert, idefics, imagegpt, informer, instructblip, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, longformer, longt5, luke, lxmert, m2m_100, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mistral, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, mpnet, mpt, mra, mt5, musicgen, mvp, nat, nezha, nllb-moe, nougat, nystromformer, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, pegasus, pegasus_x, perceiver, persimmon, pix2struct, plbart, poolformer, pop2piano, prophetnet, pvt, qdqbert, rag, realm, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rwkv, sam, seamless_m4t, segformer, sew, sew-d, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, umt5, unispeech, unispeech-sat, upernet, van, videomae, vilt, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vits, vivit, wav2vec2, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso

hfl/cino-base-v2 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


sociocom/RealMedNLP_CR_JA failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'sociocom/RealMedNLP_CR_JA'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sociocom/RealMedNLP_CR_JA' is the correct path to a directory containing all relevant files for a BertJapaneseTokenizer tokenizer.

HalaIT/RobertaForSequenceClassification failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/HalaIT/RobertaForSequenceClassification/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-656f5ac6-08dae5725c4a0cb417bfa709;3c682dd7-1516-4e37-880b-5fecb4facbe3)

Cannot access gated repo for url https://huggingface.co/HalaIT/RobertaForSequenceClassification/resolve/main/tokenizer_config.json.
Repo model HalaIT/RobertaForSequenceClassification is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 445, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/HalaIT/RobertaForSequenceClassification and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.

m3/m3-experiment-albert-base-v2-chemprot-word-swapping-random-0 failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-chemprot-word-swapping-random-0/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5acc-24e65aab7e5d469d319c1335;585e38b2-ca06-402e-902d-c2d5e0ef3bcf)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-chemprot-word-swapping-random-0/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-albert-base-v2-chemprot-word-swapping-random-0 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Qilex/mt5-small-en-me failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Qilex/mt5-small-en-me'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Qilex/mt5-small-en-me' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

Team-PIXEL/pixel-base-finetuned-masakhaner-pcm failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1064, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 761, in __getitem__
    raise KeyError(key)
KeyError: 'pixel'

TianyuHan/ocamlbertlarge15 failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'TianyuHan/ocamlbertlarge15'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'TianyuHan/ocamlbertlarge15' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

tartuNLP/mtee-military failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/tartuNLP/mtee-military/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f5ad2-0ae09da43bc9ee7b62d65961;e9e45ede-8ba3-4b2a-8088-696b94940ee7)

Entry Not Found for url: https://huggingface.co/tartuNLP/mtee-military/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: tartuNLP/mtee-military does not appear to have a file named config.json. Checkout 'https://huggingface.co/tartuNLP/mtee-military/main' for available files.

alireza7/PEGASUS-persian-base-wiki-summary failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/tokenization_pegasus_fast.py", line 147, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
PegasusConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


diegozs97/chemprot-seed-4-20k failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/chemprot-seed-4-20k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/chemprot-seed-4-20k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

TuhinColumbia/russianpoetrymany failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mbart/tokenization_mbart_fast.py", line 123, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
MBartConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


tner/xlm-roberta-base-uncased-wnut2017 failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


cointegrated/rut5-small-chitchat failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Vivek/GPT2_GSM8k failed
Architectures: ['XLMRobertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Vivek/GPT2_GSM8k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Vivek/GPT2_GSM8k' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

hanseokhyeon/bert-11street failed
Architectures: ['XLMRobertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'hanseokhyeon/bert-11street'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'hanseokhyeon/bert-11street' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

it5/it5-large-headline-generation failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

alexrfelicio/t5-small-finetuned300-en-to-de failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

rxsong/bert-finetuned-ner_stake failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/rxsong/bert-finetuned-ner_stake/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-656f5b27-26287ee16136be877e7ef584;3af9f8d4-f3f2-4b29-8f46-d3562eeb832f)

Cannot access gated repo for url https://huggingface.co/rxsong/bert-finetuned-ner_stake/resolve/main/tokenizer_config.json.
Repo model rxsong/bert-finetuned-ner_stake is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 445, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/rxsong/bert-finetuned-ner_stake and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.

Language-Media-Lab/mt5-small-ain-jpn-mt failed
Architectures: ['XLMRobertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


ghadeermobasher/BC5CDR-disease-WLT-256-PubMedBERT-latest failed
Architectures: ['XLMRobertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC5CDR-disease-WLT-256-PubMedBERT-latest'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC5CDR-disease-WLT-256-PubMedBERT-latest' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-guw-fi failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

huak95/mt-align-LST_classic-th-to-en-pt2 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 789, in from_pretrained
    return tokenizer_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'huak95/mt-align-LST_classic-th-to-en-pt2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'huak95/mt-align-LST_classic-th-to-en-pt2' is the correct path to a directory containing all relevant files for a MarianTokenizer tokenizer.

joe5campbell/Horovod_Tweet_Sentiment_1k_3eps failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'joe5campbell/Horovod_Tweet_Sentiment_1k_3eps'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'joe5campbell/Horovod_Tweet_Sentiment_1k_3eps' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

ytlin/19rdmhqc failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mbart/tokenization_mbart_fast.py", line 123, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
MBartConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


kworts/arxiv-bart failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/kworts/arxiv-bart/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5b92-53278c0d56c092c945773a26;b8fb8904-4f12-450b-943a-f6e21de4310f)

Repository Not Found for url: https://huggingface.co/kworts/arxiv-bart/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: kworts/arxiv-bart is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Helsinki-NLP/opus-mt-fr-ln failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

tlkh/t5-metaphor-large failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

AdapterHub/roberta-base-pf-ud_deprel failed
Architectures: ['CamembertForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/AdapterHub/roberta-base-pf-ud_deprel/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f5ba7-23c225814af60215450a223f;7126960b-5a4a-4096-a9a5-bf1422a66768)

Entry Not Found for url: https://huggingface.co/AdapterHub/roberta-base-pf-ud_deprel/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: AdapterHub/roberta-base-pf-ud_deprel does not appear to have a file named config.json. Checkout 'https://huggingface.co/AdapterHub/roberta-base-pf-ud_deprel/main' for available files.

seduerr/pai_paraph failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

TuhinColumbia/germanpoetrymany failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mbart/tokenization_mbart_fast.py", line 123, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
MBartConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


mboth/klassifizierungDatenpunkteNLI100000 failed
Architectures: ['OPTForCausalLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/mboth/klassifizierungDatenpunkteNLI100000/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5c0a-553ed7786b988883006dfc97;4dadc14a-72df-4eab-b828-f5c5dd80e180)

Repository Not Found for url: https://huggingface.co/mboth/klassifizierungDatenpunkteNLI100000/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: mboth/klassifizierungDatenpunkteNLI100000 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

spacy/es_dep_news_trf failed
Architectures: ['OPTForCausalLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/spacy/es_dep_news_trf/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f5c0a-2b78fc985464d56602fd09c2;50802036-54f2-49fc-804f-b43d1b02e3e3)

Entry Not Found for url: https://huggingface.co/spacy/es_dep_news_trf/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: spacy/es_dep_news_trf does not appear to have a file named config.json. Checkout 'https://huggingface.co/spacy/es_dep_news_trf/main' for available files.

OHenry/OHenry failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'OHenry/OHenry'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'OHenry/OHenry' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

WikinewsSum/t5-base-multi-fr-wiki-news failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


jmassot/distilbert-base-uncased-jm-finetuned-emotion_hub failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/jmassot/distilbert-base-uncased-jm-finetuned-emotion_hub/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5c13-40e931291b0593ff2e6b05f3;90e44d3b-d76b-41a0-9ec8-eb2cb1f0d765)

Repository Not Found for url: https://huggingface.co/jmassot/distilbert-base-uncased-jm-finetuned-emotion_hub/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: jmassot/distilbert-base-uncased-jm-finetuned-emotion_hub is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

CennetOguz/bert-large-uncased-finetuned-youcook_2 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'CennetOguz/bert-large-uncased-finetuned-youcook_2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'CennetOguz/bert-large-uncased-finetuned-youcook_2' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

annahaz/xlm-roberta-base-finetuned-misogyny-sexism2 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/annahaz/xlm-roberta-base-finetuned-misogyny-sexism2/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5c13-41f0ce663d96675d3d7b8770;66a3ee81-0ed7-4a6d-ae83-2621ce9adf43)

Repository Not Found for url: https://huggingface.co/annahaz/xlm-roberta-base-finetuned-misogyny-sexism2/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: annahaz/xlm-roberta-base-finetuned-misogyny-sexism2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Helsinki-NLP/opus-mt-es-ho failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

enelpol/evalatin2022-pos-open failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


osanseviero/my-helsinki-duplicate failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

aks234/t5-small-finetuned-xsum failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

iamdohyun/FT_lcs_adamw_apex_fused failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'iamdohyun/FT_lcs_adamw_apex_fused'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'iamdohyun/FT_lcs_adamw_apex_fused' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-pag-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

google/roberta2roberta_L-24_bbc failed
Architectures: ['EncoderDecoderModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 101, in <module>
    model = AutoModel.from_config(config, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 443, in from_config
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig'> for this kind of AutoModel: AutoModel.
Model type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, LlamaConfig, CodeGenConfig, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MistralConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SamConfig, SeamlessM4TConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VitDetConfig, VitsConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig, XLMRobertaConfig, LSGBartConfig.

amtam0/timer-ner-en failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/amtam0/timer-ner-en/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f5c63-10dc91fa260bf43f627877c0;7a0fa812-9a78-4a04-9779-65b1c9620aeb)

Entry Not Found for url: https://huggingface.co/amtam0/timer-ner-en/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: amtam0/timer-ner-en does not appear to have a file named config.json. Checkout 'https://huggingface.co/amtam0/timer-ner-en/main' for available files.

weijiahaha/t5-small-summarization failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

shashanksingh944/sql-model-generator failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Einmalumdiewelt/T5-Base_GNAD_MaxSamples failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

MickyMike/111-GPT2SP-talendesb-mesos failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'MickyMike/111-GPT2SP-talendesb-mesos'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'MickyMike/111-GPT2SP-talendesb-mesos' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

kmfoda/description_generator_new failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


ziedsb19/tunbert_zied failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2056, in _from_pretrained
    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/roberta/tokenization_roberta.py", line 221, in __init__
    with open(vocab_file, encoding="utf-8") as vocab_handle:
TypeError: expected str, bytes or os.PathLike object, not NoneType

juancavallotti/t5-base-gec failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

seanbethard/autonlp-summarization_model-8771942 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

norefly/opus-mt-ko-en-finetuned-ko-to-en3 failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

cahya/bert2gpt-indonesian-summarization failed
Architectures: ['EncoderDecoderModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 101, in <module>
    model = AutoModel.from_config(config, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 443, in from_config
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig'> for this kind of AutoModel: AutoModel.
Model type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, LlamaConfig, CodeGenConfig, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MistralConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SamConfig, SeamlessM4TConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VitDetConfig, VitsConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig, XLMRobertaConfig, LSGBartConfig.

abdalrahmanshahrour/salty-newt failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/abdalrahmanshahrour/salty-newt/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5cb2-7f7b83b73a95428313434b7a;ee5adc23-97fc-4f7e-a6f4-e64a33631dc9)

Repository Not Found for url: https://huggingface.co/abdalrahmanshahrour/salty-newt/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: abdalrahmanshahrour/salty-newt is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

ghadeermobasher/BC4CHEMD-WLT-384-PubMedBERT-Trial-50 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC4CHEMD-WLT-384-PubMedBERT-Trial-50'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC4CHEMD-WLT-384-PubMedBERT-Trial-50' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

junnyu/roformer_chinese_small failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/roformer/tokenization_roformer.py", line 398, in __init__
    import rjieba
ModuleNotFoundError: No module named 'rjieba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2056, in _from_pretrained
    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/roformer/tokenization_roformer.py", line 400, in __init__
    raise ImportError(
ImportError: You need to install rjieba to use RoFormerTokenizer. See https://pypi.org/project/rjieba/ for installation.

Helsinki-NLP/opus-mt-sv-ase failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

anas-awadalla/t5-small-few-shot-k-16-finetuned-squad-seed-4 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Alireza1044/MobileBERT_Theseus-sst-2 failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Alireza1044/MobileBERT_Theseus-sst-2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Alireza1044/MobileBERT_Theseus-sst-2' is the correct path to a directory containing all relevant files for a MobileBertTokenizerFast tokenizer.

BigSalmon/Rowerta failed
Architectures: ['AlbertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'BigSalmon/Rowerta'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'BigSalmon/Rowerta' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-tc-big-en-ar failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

annahaz/distilbert-base-uncased-misogyny-sexism-4tweets-2e-05-0.05 failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/annahaz/distilbert-base-uncased-misogyny-sexism-4tweets-2e-05-0.05/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5cf2-315bad3a4699f50641ed7bf5;fe251e4c-ec78-420b-8478-464d64bf6704)

Repository Not Found for url: https://huggingface.co/annahaz/distilbert-base-uncased-misogyny-sexism-4tweets-2e-05-0.05/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: annahaz/distilbert-base-uncased-misogyny-sexism-4tweets-2e-05-0.05 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

milmor/t5-small-spanish-nahuatl failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/milmor/t5-small-spanish-nahuatl/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5cf9-59f1aa682bcc3749706a0289;188f7870-dad3-4f48-9591-34c6b024109c)

Repository Not Found for url: https://huggingface.co/milmor/t5-small-spanish-nahuatl/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: milmor/t5-small-spanish-nahuatl is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

textattack/albert-base-v2-MRPC failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/albert/tokenization_albert_fast.py", line 148, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
AlbertConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


canovich/myprivateee failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

simecek/DNAMobileBert failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'simecek/DNAMobileBert'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'simecek/DNAMobileBert' is the correct path to a directory containing all relevant files for a MobileBertTokenizerFast tokenizer.

nytimesrd/distilbert-base-uncased failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/nytimesrd/distilbert-base-uncased/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5d2b-2c9927c15ec0651f79e9ac36;05fdbc05-861e-4d45-b082-0d67b25a42ab)

Repository Not Found for url: https://huggingface.co/nytimesrd/distilbert-base-uncased/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: nytimesrd/distilbert-base-uncased is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

PontifexMaximus/ArabicTranslator failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-es-ht failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

m3/m3-experiment-roberta-base-tweet-eval-emotion-add-v5 failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-roberta-base-tweet-eval-emotion-add-v5/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5d83-1f2067d34274527f103d82ef;5890373f-7fc5-413d-bf1f-937d030db461)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-roberta-base-tweet-eval-emotion-add-v5/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-roberta-base-tweet-eval-emotion-add-v5 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

tner/xlm-roberta-large-panx-dataset-es failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


spy24/autotrain-expand-928531583 failed
Architectures: ['PegasusForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1221, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 981, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-mfs-es failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

masakhane/m2m100_418M_fr_bam_rel_news_ft failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1181, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 966, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

shahrukhx01/smole-bart-muv-5x-permute failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bart/tokenization_bart_fast.py", line 179, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 365, in converted
    BPE(
Exception: Error while initializing BPE: Token `` out of vocabulary

reichenbach/fake-news-detector failed
Architectures: ['SqueezeBertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'reichenbach/fake-news-detector'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'reichenbach/fake-news-detector' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

flax-community/code-mt5-base-batch-mix failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

0x7194633/keyt5-large failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


mousaazari/t5-test2sql failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'mousaazari/t5-test2sql'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'mousaazari/t5-test2sql' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

Helsinki-NLP/opus-mt-uk-sv failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

sileod/roberta-base-mnli failed
Architectures: ['MPNetModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/sileod/roberta-base-mnli/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5de0-0041e8ce40ba625b3d3165ed;f7ad1fe4-df98-4f63-a542-e17adc21259c)

Repository Not Found for url: https://huggingface.co/sileod/roberta-base-mnli/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: sileod/roberta-base-mnli is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Artifact-AI/en_spacy_ontonotes_distilroberta_base_ner failed
Architectures: ['BertModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/Artifact-AI/en_spacy_ontonotes_distilroberta_base_ner/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5de6-70a77b657a62f26e70592e27;32873276-3754-4412-a64e-a4a48fe96d75)

Repository Not Found for url: https://huggingface.co/Artifact-AI/en_spacy_ontonotes_distilroberta_base_ner/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: Artifact-AI/en_spacy_ontonotes_distilroberta_base_ner is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

tner/xlm-roberta-large-uncased-conll2003 failed
Architectures: ['BertModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


muhtasham/bert-small-finer-longer80 failed
Architectures: ['DistilBertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'muhtasham/bert-small-finer-longer80'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'muhtasham/bert-small-finer-longer80' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

AdapterHub/bert-base-uncased-pf-hotpotqa failed
Architectures: ['DistilBertForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/AdapterHub/bert-base-uncased-pf-hotpotqa/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f5df8-0bba97aa58058a5e52b3731b;064921d0-c44e-46a0-aac7-60ed7403ef49)

Entry Not Found for url: https://huggingface.co/AdapterHub/bert-base-uncased-pf-hotpotqa/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: AdapterHub/bert-base-uncased-pf-hotpotqa does not appear to have a file named config.json. Checkout 'https://huggingface.co/AdapterHub/bert-base-uncased-pf-hotpotqa/main' for available files.

Helsinki-NLP/opus-mt-mh-fi failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

m3/m3-experiment-roberta-base-citation-intent-eda-4 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-roberta-base-citation-intent-eda-4/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5e1b-377394ce0e1f2ac840139da2;ceb2f987-650a-494f-9927-803f3a3d4928)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-roberta-base-citation-intent-eda-4/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-roberta-base-citation-intent-eda-4 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

maxidl/iML-distilbert-base-uncased-predict failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'maxidl/iML-distilbert-base-uncased-predict'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'maxidl/iML-distilbert-base-uncased-predict' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

Nicki/gpt3-base failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/Nicki/gpt3-base/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5e25-1445dfe05f0cb548708e6172;ef4a9612-3382-47e0-9693-bf9d0f81fc1f)

Repository Not Found for url: https://huggingface.co/Nicki/gpt3-base/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: Nicki/gpt3-base is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Helsinki-NLP/opus-mt-crs-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

TransQuest/monotransquest-hter-en_lv-it-nmt failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Helsinki-NLP/opus-mt-eo-fi failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Vlasta/humandna_bert_default failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Vlasta/humandna_bert_default'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Vlasta/humandna_bert_default' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Jiexing/spider_relation_t5_3b-2624 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

notexist/tttw failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'notexist/tttw'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'notexist/tttw' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

stanfordnlp/stanza-hr failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/stanfordnlp/stanza-hr/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f5e5b-5dea523b3fff90484c967cd4;ff303cb3-6458-4811-ad8e-20abfb140c95)

Entry Not Found for url: https://huggingface.co/stanfordnlp/stanza-hr/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: stanfordnlp/stanza-hr does not appear to have a file named config.json. Checkout 'https://huggingface.co/stanfordnlp/stanza-hr/main' for available files.

nlp-waseda/roberta-base-japanese-with-auto-jumanpp failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py", line 649, in __init__
    import rhoknp
ModuleNotFoundError: No module named 'rhoknp'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py", line 200, in __init__
    self.word_tokenizer = JumanppTokenizer(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py", line 651, in __init__
    raise ImportError(
ImportError: You need to install rhoknp to use JumanppTokenizer. See https://github.com/ku-nlp/rhoknp for installation.

EP9/mt5-small-MT5-Intento2 failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-fr-to failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

buvnswrn/daml-t5-pretrain failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

arinze/t5_finetuned_xsum_hr failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

spacy/mk_core_news_lg failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/spacy/mk_core_news_lg/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f5e9d-2e9e3ecf17d75d4f1020f3cb;a72adecf-263c-4ff9-b8fc-064a24ae7d14)

Entry Not Found for url: https://huggingface.co/spacy/mk_core_news_lg/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: spacy/mk_core_news_lg does not appear to have a file named config.json. Checkout 'https://huggingface.co/spacy/mk_core_news_lg/main' for available files.

SophieTr/results failed
Architectures: ['RobertaForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/tokenization_pegasus_fast.py", line 147, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
PegasusConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


SGaleshchuk/t5-large-ua-news failed
Architectures: ['RobertaForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


jhu-clsp/bibert-ende failed
Architectures: ['RobertaForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2056, in _from_pretrained
    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/roberta/tokenization_roberta.py", line 221, in __init__
    with open(vocab_file, encoding="utf-8") as vocab_handle:
TypeError: expected str, bytes or os.PathLike object, not NoneType

Ayham/ernie_gpt2_summarization_cnn_dailymail failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Ayham/ernie_gpt2_summarization_cnn_dailymail'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Ayham/ernie_gpt2_summarization_cnn_dailymail' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

AdapterHub/roberta-base-pf-duorc_s failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/AdapterHub/roberta-base-pf-duorc_s/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f5ecd-3b7203ee3e2367f706c44fcf;2afc97b7-7cec-4705-80b0-34f20e71faac)

Entry Not Found for url: https://huggingface.co/AdapterHub/roberta-base-pf-duorc_s/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: AdapterHub/roberta-base-pf-duorc_s does not appear to have a file named config.json. Checkout 'https://huggingface.co/AdapterHub/roberta-base-pf-duorc_s/main' for available files.

choosistant/seq2seqmodel failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'choosistant/seq2seqmodel'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'choosistant/seq2seqmodel' is the correct path to a directory containing all relevant files for a BartTokenizerFast tokenizer.

leokai/distilroberta-base-wikitextepoch_350 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'leokai/distilroberta-base-wikitextepoch_350'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'leokai/distilroberta-base-wikitextepoch_350' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

YassineToughrai/Domain_adapted_ARBERT_GOUDMA_BERT_GPT failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'YassineToughrai/Domain_adapted_ARBERT_GOUDMA_BERT_GPT'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'YassineToughrai/Domain_adapted_ARBERT_GOUDMA_BERT_GPT' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

lct-rug-2022/edos-2023-baseline-xlm-roberta-base-label_sexist failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/lct-rug-2022/edos-2023-baseline-xlm-roberta-base-label_sexist/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5f0d-6988103626580154715ede95;abc300cd-eaac-4a95-a012-f9d9c6c7aec0)

Repository Not Found for url: https://huggingface.co/lct-rug-2022/edos-2023-baseline-xlm-roberta-base-label_sexist/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: lct-rug-2022/edos-2023-baseline-xlm-roberta-base-label_sexist is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

jplu/tf-flaubert-small-cased failed
Architectures: ['DistilBertForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/flaubert/tokenization_flaubert.py", line 262, in __init__
    import sacremoses
ModuleNotFoundError: No module named 'sacremoses'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 789, in from_pretrained
    return tokenizer_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/flaubert/tokenization_flaubert.py", line 264, in __init__
    raise ImportError(
ImportError: You need to install sacremoses to use FlaubertTokenizer. See https://pypi.org/project/sacremoses/ for installation.

MaRiOrOsSi/t5-base-finetuned-question-answering failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


persiannlp/mt5-base-parsinlu-qqp-query-paraphrasing failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Haakf/allsides_right_headline_padded_overfit failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/Haakf/allsides_right_headline_padded_overfit/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5f38-48a2fe1e5df2810a06988490;f98d8df9-a1ae-498a-8c3b-9a7afff18c29)

Repository Not Found for url: https://huggingface.co/Haakf/allsides_right_headline_padded_overfit/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: Haakf/allsides_right_headline_padded_overfit is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Ayham/roberta_gpt2_new_max64_summarization_cnndm failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Ayham/roberta_gpt2_new_max64_summarization_cnndm'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Ayham/roberta_gpt2_new_max64_summarization_cnndm' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

deepanway/snli-checkpoint-301090 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/deepanway/snli-checkpoint-301090/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5f49-4e40174a3680469c1804f75b;bc3f0399-c99c-42cb-97e3-8ff47c552399)

Repository Not Found for url: https://huggingface.co/deepanway/snli-checkpoint-301090/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: deepanway/snli-checkpoint-301090 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

mwp/token-absolute-lm-stage1 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/mwp/token-absolute-lm-stage1/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5f49-6e71a090602a384e4c483f93;899b9955-383c-420f-ae4b-46397e89f87a)

Repository Not Found for url: https://huggingface.co/mwp/token-absolute-lm-stage1/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: mwp/token-absolute-lm-stage1 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

leslie/bert_cn_finetuning failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2056, in _from_pretrained
    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert.py", line 199, in __init__
    if not os.path.isfile(vocab_file):
  File "/anaconda/envs/amc/lib/python3.10/genericpath.py", line 30, in isfile
    st = os.stat(path)
TypeError: stat: path should be string, bytes, os.PathLike or integer, not NoneType

dbernsohn/algebra_linear_1d failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


SEBIS/code_trans_t5_large_code_documentation_generation_php_transfer_learning_finetune failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


MickyMike/777-GPT2SP-talenddataquality-aptanastudio failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'MickyMike/777-GPT2SP-talenddataquality-aptanastudio'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'MickyMike/777-GPT2SP-talenddataquality-aptanastudio' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

kevincstowe/readaloud-1m-lists failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/kevincstowe/readaloud-1m-lists/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f5f7d-0b2ef5b40b659a0d6d3ebd14;6507cf32-173d-42e6-afab-bfcca87cddde)

Repository Not Found for url: https://huggingface.co/kevincstowe/readaloud-1m-lists/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: kevincstowe/readaloud-1m-lists is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

ck46/t5-base-hotpot-qa-qg failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-iso-sv failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

jky594176/BART1_GRU failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'jky594176/BART1_GRU'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'jky594176/BART1_GRU' is the correct path to a directory containing all relevant files for a BartTokenizerFast tokenizer.

simecek/DNADeberta failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'simecek/DNADeberta'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'simecek/DNADeberta' is the correct path to a directory containing all relevant files for a DebertaTokenizerFast tokenizer.

AdapterHub/roberta-base-pf-boolq failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/AdapterHub/roberta-base-pf-boolq/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f6006-45a9624f32a30b5303f41681;ee1bb7d5-5314-45aa-aff7-e3721a73aa83)

Entry Not Found for url: https://huggingface.co/AdapterHub/roberta-base-pf-boolq/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: AdapterHub/roberta-base-pf-boolq does not appear to have a file named config.json. Checkout 'https://huggingface.co/AdapterHub/roberta-base-pf-boolq/main' for available files.

Lav/gpt2-wikitext2 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/Lav/gpt2-wikitext2/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f601d-180f64cb1389a0bf2877dd88;6fe3c4c8-2352-4575-9d6c-b53c0e9182a1)

Repository Not Found for url: https://huggingface.co/Lav/gpt2-wikitext2/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: Lav/gpt2-wikitext2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

carlbert/spatial-mlm failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/carlbert/spatial-mlm/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f6033-6c4cee9b15a09a731d9841f4;6c071c2f-b10c-4a2b-9c00-bffccee8b4e5)

Repository Not Found for url: https://huggingface.co/carlbert/spatial-mlm/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: carlbert/spatial-mlm is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

neal49/distilbert-yelp failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'neal49/distilbert-yelp'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'neal49/distilbert-yelp' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

maelfabien/marcel_customer_service_xlarge failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/camembert/tokenization_camembert_fast.py", line 127, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
CamembertConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


pszemraj/Ballpark-Trivia-M failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/pszemraj/Ballpark-Trivia-M/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f6047-5fbb0b876831692e4ebd05fd;9394d208-2a4b-4a6c-8735-9bab39f8b122)

Repository Not Found for url: https://huggingface.co/pszemraj/Ballpark-Trivia-M/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: pszemraj/Ballpark-Trivia-M is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

stevemobs/deberta-base-combined-squad1-aqa-newsqa failed
Architectures: ['DebertaForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 31, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 974, in new_func
    encoder_outputs = self.encoder(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 470, in new_func
    hidden_states = layer_module(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 383, in new_func
    attention_output = self.attention(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 316, in new_func
    self_output = self.self(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 661, in new_func
    rel_att = self.disentangled_att_bias(query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 710, in new_func
    c2p_att = torch.gather(c2p_att, dim=-1, index=c2p_dynamic_expand(c2p_pos, query_layer, relative_pos))
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
RuntimeError: c2p_dynamic_expand() Expected a value of type 'Tensor (inferred)' for argument 'c2p_pos' but instead found type 'ConcreteProxy'.
Inferred 'c2p_pos' to be of type 'Tensor' because it was not annotated with an explicit type.
Position: 0
Value: ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]]))
Declaration: c2p_dynamic_expand(Tensor c2p_pos, Tensor query_layer, Tensor relative_pos) -> Tensor
Cast error details: Unable to cast ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]])) to Tensor

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 33, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

conorhastings/stillconor failed
Architectures: ['DebertaForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2056, in _from_pretrained
    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/gpt2/tokenization_gpt2.py", line 187, in __init__
    with open(merges_file, encoding="utf-8") as merges_handle:
TypeError: expected str, bytes or os.PathLike object, not NoneType

m3hrdadfi/albert-fa-base-v2-ner-arman failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/albert/tokenization_albert_fast.py", line 148, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
AlbertConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


NLPSTCEXP/Affect-Roberta failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/NLPSTCEXP/Affect-Roberta/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f6062-03213df313ee7b962cbbbeba;a0a82f9e-ba68-4f1c-8599-7fc8005b7ecd)

Repository Not Found for url: https://huggingface.co/NLPSTCEXP/Affect-Roberta/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: NLPSTCEXP/Affect-Roberta is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

moussaKam/AraBART failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/barthez/tokenization_barthez_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
BarthezConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


abecode/t5-small-finetuned-emo20q failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

lafias/dataset-references failed
Architectures: ['XLMRobertaModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/lafias/dataset-references/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f60ae-1e3df44623b405f82826d2b9;4c9edfbd-b523-40ab-bf5b-337c145240d8)

Entry Not Found for url: https://huggingface.co/lafias/dataset-references/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: lafias/dataset-references does not appear to have a file named config.json. Checkout 'https://huggingface.co/lafias/dataset-references/main' for available files.

masakhane/m2m100_418M_en_lug_rel failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1181, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 966, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

k4black/bert-base-uncased-offensive-lm-tapt failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/k4black/bert-base-uncased-offensive-lm-tapt/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f60b9-4cda7125547b2357732e4363;999df8fd-18fa-4878-b821-481793d993f2)

Repository Not Found for url: https://huggingface.co/k4black/bert-base-uncased-offensive-lm-tapt/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: k4black/bert-base-uncased-offensive-lm-tapt is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

yliu337/t5_neg_nonfilter_bothcontext failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


TransQuest/monotransquest-da-any_en failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


spandan96/T5_SEO_Title_Generator failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


ConvLab/t5-small-dst-multiwoz21_sgd_tm1_tm2_tm3 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

textattack/albert-base-v2-STS-B failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/albert/tokenization_albert_fast.py", line 148, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
AlbertConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


negfir/Distill_SQuAD failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'negfir/Distill_SQuAD'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'negfir/Distill_SQuAD' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

alaggung/bart-pretrained failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: BartModel.forward() got an unexpected keyword argument 'token_type_ids'

matheusvolpon/WE4LKD_AML_distilbert_1921_1969 failed
Architectures: ['DistilBertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: DistilBertModel.forward() got an unexpected keyword argument 'token_type_ids'

bayartsogt/albert-mongolian failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/albert/tokenization_albert_fast.py", line 148, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
AlbertConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Helsinki-NLP/opus-mt-ar-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

stanfordnlp/stanza-sv failed
Architectures: ['XLMRobertaModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/stanfordnlp/stanza-sv/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f611d-2e3309dd73f29dfb6a2e0656;e9b0387f-da08-44df-91f3-fc1c5725f263)

Entry Not Found for url: https://huggingface.co/stanfordnlp/stanza-sv/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: stanfordnlp/stanza-sv does not appear to have a file named config.json. Checkout 'https://huggingface.co/stanfordnlp/stanza-sv/main' for available files.

sonoisa/t5-base-japanese-question-generation failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Sebabrata/lmv2-2022-05-24 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 96, in <module>
    concrete_args = tokenizer(text, return_tensors="pt")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py", line 253, in __call__
    raise ValueError(
ValueError: Words must be of type `List[str]` (single pretokenized example), or `List[List[str]]` (batch of pretokenized examples).

lmqg/t5-large-tweetqa-qag failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Maelstrom77/roblclass failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Maelstrom77/roblclass'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Maelstrom77/roblclass' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

m3/m3-experiment-roberta-base-sciie-word-swapping-random-2 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-roberta-base-sciie-word-swapping-random-2/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f6152-6a2f76e97bd7bfb80e623ff4;b9612798-a8ca-4f68-b954-7f4538dacf68)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-roberta-base-sciie-word-swapping-random-2/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-roberta-base-sciie-word-swapping-random-2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Gantenbein/ADDI-CH-XLM-R failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


yhavinga/byt5-small-ccmatrix-en-nl failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

matheusvolpon/WE4LKD_AML_distilbert_1921_1990 failed
Architectures: ['DistilBertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: DistilBertModel.forward() got an unexpected keyword argument 'token_type_ids'

ouiame/autotrain-Robertatogpt2-995132944 failed
Architectures: ['EncoderDecoderModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 101, in <module>
    model = AutoModel.from_config(config, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 443, in from_config
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig'> for this kind of AutoModel: AutoModel.
Model type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, LlamaConfig, CodeGenConfig, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MistralConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SamConfig, SeamlessM4TConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VitDetConfig, VitsConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig, XLMRobertaConfig, LSGBartConfig.

NahedAbdelgaber/distilbert-base-uncased-finetuned-evaluating-student-writing failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'NahedAbdelgaber/distilbert-base-uncased-finetuned-evaluating-student-writing'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'NahedAbdelgaber/distilbert-base-uncased-finetuned-evaluating-student-writing' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

any0019/text_style_mlm_positive failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'any0019/text_style_mlm_positive'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'any0019/text_style_mlm_positive' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-ru-af failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

abyerly2jh/t5-base-finetuned-eli5 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

lhoestq/distilbert-base-uncased-finetuned-absa-as failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'lhoestq/distilbert-base-uncased-finetuned-absa-as'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'lhoestq/distilbert-base-uncased-finetuned-absa-as' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

izumi-lab/electra-small-japanese-fin-generator failed
Architectures: ['DistilBertForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py", line 457, in __init__
    import fugashi
ModuleNotFoundError: No module named 'fugashi'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py", line 192, in __init__
    self.word_tokenizer = MecabTokenizer(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py", line 459, in __init__
    raise error.__class__(
ModuleNotFoundError: You need to install fugashi to use MecabTokenizer. See https://pypi.org/project/fugashi/ for installation.

ConvLab/t5-small-nlu-multiwoz21-context3 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ps29/distilbert-base-uncased-finetuned-emotion failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/ps29/distilbert-base-uncased-finetuned-emotion/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f61cd-2a13a0bd04a871b44d685e23;b567ae3a-d1a0-4ade-b8c9-32830d83f2b1)

Repository Not Found for url: https://huggingface.co/ps29/distilbert-base-uncased-finetuned-emotion/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: ps29/distilbert-base-uncased-finetuned-emotion is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

enelpol/evalatin2022-feats-closed failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


danasone/bart-small-ru-en failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py", line 1445, in forward
    encoder_outputs = self.encoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py", line 1024, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self

ghadeermobasher/linnaeus-WLT-256-BlueBERT-Trial-60 failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/linnaeus-WLT-256-BlueBERT-Trial-60'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/linnaeus-WLT-256-BlueBERT-Trial-60' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

MorrisPark/twc12-bart-pretrain failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'MorrisPark/twc12-bart-pretrain'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'MorrisPark/twc12-bart-pretrain' is the correct path to a directory containing all relevant files for a BartTokenizerFast tokenizer.

mrm8488/bert2bert-small_shared-question-generation failed
Architectures: ['EncoderDecoderModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 101, in <module>
    model = AutoModel.from_config(config, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 443, in from_config
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig'> for this kind of AutoModel: AutoModel.
Model type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, LlamaConfig, CodeGenConfig, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MistralConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SamConfig, SeamlessM4TConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VitDetConfig, VitsConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig, XLMRobertaConfig, LSGBartConfig.

Jinchen/roberta-base-finetuned-mrpc failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Jinchen/roberta-base-finetuned-mrpc'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Jinchen/roberta-base-finetuned-mrpc' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-fi-hr failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

suksun1412/wangchanberta-ner-2 failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/camembert/tokenization_camembert_fast.py", line 127, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
CamembertConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


renjithks/layoutlmv2-er-ner failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 96, in <module>
    concrete_args = tokenizer(text, return_tensors="pt")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py", line 253, in __call__
    raise ValueError(
ValueError: Words must be of type `List[str]` (single pretokenized example), or `List[List[str]]` (batch of pretokenized examples).

lenses/distilroberta-base-finetuned-assignment2 failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'lenses/distilroberta-base-finetuned-assignment2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'lenses/distilroberta-base-finetuned-assignment2' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

WillHeld/t5-base-vanilla-cstop_artificial failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

monobyte/byt5-mono-zh-v1 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

sonalily/distilgpt2-finetuned-wikitext2 failed
Architectures: ['XLNetLMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'sonalily/distilgpt2-finetuned-wikitext2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sonalily/distilgpt2-finetuned-wikitext2' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

Helsinki-NLP/opus-mt-fr-ase failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

tscholak/2e826ioa failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

vencortex/DeepFeatEcosystemSummarizerLarge failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/vencortex/DeepFeatEcosystemSummarizerLarge/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-656f62da-18a8ef15631b25926fd9aad4;0e6f8f12-c70f-4c89-ba95-889ca568ebb1)

Cannot access gated repo for url https://huggingface.co/vencortex/DeepFeatEcosystemSummarizerLarge/resolve/main/tokenizer_config.json.
Repo model vencortex/DeepFeatEcosystemSummarizerLarge is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 445, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/vencortex/DeepFeatEcosystemSummarizerLarge and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.

ghadeermobasher/BC5CDR-chem-WLT-512-BioBERT-latest failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC5CDR-chem-WLT-512-BioBERT-latest'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC5CDR-chem-WLT-512-BioBERT-latest' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

hellennamulinda/agric-eng-lug failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

djulian13/distilbert-base-uncased-finetuned-imdb failed
Architectures: ['SelfDebiasingGPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'djulian13/distilbert-base-uncased-finetuned-imdb'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'djulian13/distilbert-base-uncased-finetuned-imdb' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

Surendran/tf_model.h5 failed
Architectures: ['SelfDebiasingGPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Surendran/tf_model.h5'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Surendran/tf_model.h5' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-en-sg failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Chemsseddine/bert2gpt2_med_v3 failed
Architectures: ['EncoderDecoderModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 101, in <module>
    model = AutoModel.from_config(config, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 443, in from_config
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig'> for this kind of AutoModel: AutoModel.
Model type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, LlamaConfig, CodeGenConfig, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MistralConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SamConfig, SeamlessM4TConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VitDetConfig, VitsConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig, XLMRobertaConfig, LSGBartConfig.

eunjin/kobart_gyeongsang_translator failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: BartModel.forward() got an unexpected keyword argument 'token_type_ids'

hugo/byt5-mono-es-v1 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ZakaryaRouzki/t5-punctuation failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 111, in __init__
    fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)
Exception: Permission denied (os error 13)

derekbear/xlm-roberta-base-finetuned-panx-en failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/derekbear/xlm-roberta-base-finetuned-panx-en/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f6329-71327386424565ca6b10b600;b0fcfde8-45d4-4f3b-8d83-1165765440cc)

Repository Not Found for url: https://huggingface.co/derekbear/xlm-roberta-base-finetuned-panx-en/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: derekbear/xlm-roberta-base-finetuned-panx-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

explosion/hr_udv25_croatianset_trf failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/explosion/hr_udv25_croatianset_trf/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f6346-207768da598461d321cbc019;a29a9c89-9caf-4490-894c-99544b2ef527)

Entry Not Found for url: https://huggingface.co/explosion/hr_udv25_croatianset_trf/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: explosion/hr_udv25_croatianset_trf does not appear to have a file named config.json. Checkout 'https://huggingface.co/explosion/hr_udv25_croatianset_trf/main' for available files.

josmunpen/mt5-small-spanish-summarization failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Atharvgarg/bert-small2bert-small-finetuned-cnn_daily_mail-summarization-finetuned-bbc-news-extracted-sumy failed
Architectures: ['EncoderDecoderModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 101, in <module>
    model = AutoModel.from_config(config, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 443, in from_config
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig'> for this kind of AutoModel: AutoModel.
Model type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, LlamaConfig, CodeGenConfig, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MistralConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SamConfig, SeamlessM4TConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VitDetConfig, VitsConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig, XLMRobertaConfig, LSGBartConfig.

tkuye/t5-ost failed
Architectures: ['DistilBertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


efederici/sentence-it5-base failed
Architectures: ['T5EncoderModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

yhavinga/t5-v1.1-large-dutch-cased failed
Architectures: ['T5EncoderModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/yhavinga/t5-v1.1-large-dutch-cased/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f6359-078dca743de42e580dead9cf;bff17e1f-928c-4d2b-be36-4f1bb45e589b)

Repository Not Found for url: https://huggingface.co/yhavinga/t5-v1.1-large-dutch-cased/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: yhavinga/t5-v1.1-large-dutch-cased is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

mollypak/bert-multilingual-base failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/mollypak/bert-multilingual-base/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f635c-389ec11524f140140e12bd62;5511da7d-0482-45ac-8aa8-3d9de404c538)

Repository Not Found for url: https://huggingface.co/mollypak/bert-multilingual-base/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: mollypak/bert-multilingual-base is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

abdulmatinomotoso/article_title failed
Architectures: ['PegasusForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1221, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 981, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

m3/m3-experiment-roberta-base-tweet-eval-hate-word-swapping-synonym-4 failed
Architectures: ['RobertaForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-roberta-base-tweet-eval-hate-word-swapping-synonym-4/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f6370-31acbb1d655fe79f3444b3bf;5494672c-bf91-460a-b6f1-199237a33997)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-roberta-base-tweet-eval-hate-word-swapping-synonym-4/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-roberta-base-tweet-eval-hate-word-swapping-synonym-4 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Helsinki-NLP/opus-mt-de-uk failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

milyiyo/paraphraser-spanish-t5-small failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

joe5campbell/ROBERTA_Tweet_Sentiment_50k_2eps failed
Architectures: ['LongformerForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'joe5campbell/ROBERTA_Tweet_Sentiment_50k_2eps'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'joe5campbell/ROBERTA_Tweet_Sentiment_50k_2eps' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

mental/mental-roberta-base failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/mental/mental-roberta-base/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-656f63b8-0bcac7a13c3e163126240e54;754960cc-d880-4b67-b3a3-f6fb411288e4)

Cannot access gated repo for url https://huggingface.co/mental/mental-roberta-base/resolve/main/tokenizer_config.json.
Repo model mental/mental-roberta-base is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 445, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/mental/mental-roberta-base and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.

Rocketknight1/checkpoint_test failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Rocketknight1/checkpoint_test'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Rocketknight1/checkpoint_test' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-de-pap failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

m3/m3-experiment-roberta-base-tweet-eval-hate-add-v3 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-roberta-base-tweet-eval-hate-add-v3/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f63d7-0bc4bb6a0bb3d7fc4be2683c;b5efa55a-a283-42ea-b521-faf4580bd4c8)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-roberta-base-tweet-eval-hate-add-v3/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-roberta-base-tweet-eval-hate-add-v3 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

ghadeermobasher/BC5CDR-disease-WLT-512-SciBERT-latest-105 failed
Architectures: ['RobertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC5CDR-disease-WLT-512-SciBERT-latest-105'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC5CDR-disease-WLT-512-SciBERT-latest-105' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

stuartmesham/deberta-large_basetags_5k_5_p3 failed
Architectures: ['GectorbertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 31, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 974, in new_func
    encoder_outputs = self.encoder(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 470, in new_func
    hidden_states = layer_module(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 383, in new_func
    attention_output = self.attention(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 316, in new_func
    self_output = self.self(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 661, in new_func
    rel_att = self.disentangled_att_bias(query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 710, in new_func
    c2p_att = torch.gather(c2p_att, dim=-1, index=c2p_dynamic_expand(c2p_pos, query_layer, relative_pos))
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
RuntimeError: c2p_dynamic_expand() Expected a value of type 'Tensor (inferred)' for argument 'c2p_pos' but instead found type 'ConcreteProxy'.
Inferred 'c2p_pos' to be of type 'Tensor' because it was not annotated with an explicit type.
Position: 0
Value: ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]]))
Declaration: c2p_dynamic_expand(Tensor c2p_pos, Tensor query_layer, Tensor relative_pos) -> Tensor
Cast error details: Unable to cast ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]])) to Tensor

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 33, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

r1ck/bi-encoder-vi_wikiqa failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/r1ck/bi-encoder-vi_wikiqa/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f63f6-39a21edf5962efe52793bb60;1b14330f-a5cf-4799-9db3-bad2e1287e84)

Repository Not Found for url: https://huggingface.co/r1ck/bi-encoder-vi_wikiqa/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: r1ck/bi-encoder-vi_wikiqa is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

diegozs97/finetuned-chemprot-seed-1-60k failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/finetuned-chemprot-seed-1-60k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/finetuned-chemprot-seed-1-60k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Artifact-AI/en_spacy_wnut_distilroberta_base_ner failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/Artifact-AI/en_spacy_wnut_distilroberta_base_ner/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f6404-0bd4d0b15ce8e3803636ef55;8209e4b6-3bd3-45f0-97db-0f6be6e67042)

Repository Not Found for url: https://huggingface.co/Artifact-AI/en_spacy_wnut_distilroberta_base_ner/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: Artifact-AI/en_spacy_wnut_distilroberta_base_ner is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Helsinki-NLP/opus-mt-fi-mk failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

tonydiana1/distilgpt2-finetuned-wikitext2 failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'tonydiana1/distilgpt2-finetuned-wikitext2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'tonydiana1/distilgpt2-finetuned-wikitext2' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

Kyungill/221130_LexcodeNMT_FineTuning_Helsinki_ko-en_v1 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/Kyungill/221130_LexcodeNMT_FineTuning_Helsinki_ko-en_v1/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f6422-3587af70025434637efe898f;cdd90467-9891-4446-b8d8-4c7edeb83ccd)

Repository Not Found for url: https://huggingface.co/Kyungill/221130_LexcodeNMT_FineTuning_Helsinki_ko-en_v1/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: Kyungill/221130_LexcodeNMT_FineTuning_Helsinki_ko-en_v1 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

mfigurski80/relation-distilbert-inv failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'mfigurski80/relation-distilbert-inv'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'mfigurski80/relation-distilbert-inv' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

diegozs97/sciie-seed-4-1500k failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/sciie-seed-4-1500k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/sciie-seed-4-1500k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

devkushal75/medtextclassifier failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'devkushal75/medtextclassifier'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'devkushal75/medtextclassifier' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

ShooterRon/mt5-small_summarization failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ghadeermobasher/BC5CDR-chem-WLT-128-BlueBERT-latest failed
Architectures: ['XLMRobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC5CDR-chem-WLT-128-BlueBERT-latest'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC5CDR-chem-WLT-128-BlueBERT-latest' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

dscoursetechnion/t5-small-finetuned-xsum failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-fr-kqn failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

it5/it5-efficient-small-el32-news-summarization failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

hf-internal-testing/tiny-random-LiltForQuestionAnswering failed
Architectures: ['XLMRobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 96, in <module>
    concrete_args = tokenizer(text, return_tensors="pt")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/layoutlmv3/tokenization_layoutlmv3_fast.py", line 299, in __call__
    raise ValueError(
ValueError: Words must be of type `List[str]` (single pretokenized example), or `List[List[str]]` (batch of pretokenized examples).

SEBIS/legal_t5_small_cls_cs failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


ShengdingHu/superglue-copa failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

VMware/vinilm-2021-qa-evaluator failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 111, in <module>
    assert check_align(before_trace, after_trace), "Traced model does not match the original model"
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 49, in check_align
    for key in after_trace.keys():
AttributeError: 'tuple' object has no attribute 'keys'

khizon/bert-unreliable-news-eng failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'khizon/bert-unreliable-news-eng'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'khizon/bert-unreliable-news-eng' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Aleksandar1932/gpt2-hip-hop failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Aleksandar1932/gpt2-hip-hop'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Aleksandar1932/gpt2-hip-hop' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

sagittariusA/gender_classifier_cs failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'sagittariusA/gender_classifier_cs'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sagittariusA/gender_classifier_cs' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

Kaleab1999/Distilbert_flyrthiopian failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Kaleab1999/Distilbert_flyrthiopian'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Kaleab1999/Distilbert_flyrthiopian' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

vasudevgupta/dl-hack-pegasus-large failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/tokenization_pegasus_fast.py", line 147, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
PegasusConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


batubayk/combined_tr_berturk32k_cased_summary failed
Architectures: ['EncoderDecoderModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 101, in <module>
    model = AutoModel.from_config(config, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 443, in from_config
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig'> for this kind of AutoModel: AutoModel.
Model type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, LlamaConfig, CodeGenConfig, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MistralConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SamConfig, SeamlessM4TConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VitDetConfig, VitsConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig, XLMRobertaConfig, LSGBartConfig.

m3/m3-experiment-roberta-base-tweet-eval-emotion-add-v2 failed
Architectures: ['EncoderDecoderModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-roberta-base-tweet-eval-emotion-add-v2/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f64db-5c14470a0394e8b22e330388;b41397d3-6191-4bac-8b5f-4ed451bb961e)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-roberta-base-tweet-eval-emotion-add-v2/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-roberta-base-tweet-eval-emotion-add-v2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

quincyqiang/distilbert-base-uncased-finetuned-emotion failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/quincyqiang/distilbert-base-uncased-finetuned-emotion/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f64e8-3ac36cc134fa3e6b0eca798b;ba8a63bb-e5e5-4134-8877-a53c354d9a1d)

Repository Not Found for url: https://huggingface.co/quincyqiang/distilbert-base-uncased-finetuned-emotion/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: quincyqiang/distilbert-base-uncased-finetuned-emotion is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

johnny9604/pbl_electra failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'johnny9604/pbl_electra'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'johnny9604/pbl_electra' is the correct path to a directory containing all relevant files for a ElectraTokenizerFast tokenizer.

k4black/edos-2023-baseline-bert-base-uncased-label_sexist failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/k4black/edos-2023-baseline-bert-base-uncased-label_sexist/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f652f-7007ccda7d5bd5cc7084e9a5;6165db9e-4f9d-490d-9bfc-d8e805acb0a0)

Repository Not Found for url: https://huggingface.co/k4black/edos-2023-baseline-bert-base-uncased-label_sexist/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: k4black/edos-2023-baseline-bert-base-uncased-label_sexist is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

jinhybr/layoutlm-funsd-tf failed
Architectures: ['ElectraForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 96, in <module>
    concrete_args = tokenizer(text, return_tensors="pt")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py", line 253, in __call__
    raise ValueError(
ValueError: Words must be of type `List[str]` (single pretokenized example), or `List[List[str]]` (batch of pretokenized examples).

liat-nakayama/roberta_base_ja_20190121_m10000_v24000_u500000 failed
Architectures: ['ElectraForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'liat-nakayama/roberta_base_ja_20190121_m10000_v24000_u500000'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'liat-nakayama/roberta_base_ja_20190121_m10000_v24000_u500000' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

bettertextapp/m2m-tai-en-de-gen-1.2B-1k-steps failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1181, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 966, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

m3/m3-experiment-albert-base-v2-sciie-vanilla failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-sciie-vanilla/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f65db-68379092311a073f34d5db0a;7ce7f143-15e1-4c16-a5b0-c87d861321aa)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-sciie-vanilla/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-albert-base-v2-sciie-vanilla is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

stuartmesham/deberta-large_lemon-spell_10k_1_p3 failed
Architectures: ['GectorbertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 31, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 974, in new_func
    encoder_outputs = self.encoder(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 470, in new_func
    hidden_states = layer_module(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 383, in new_func
    attention_output = self.attention(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 316, in new_func
    self_output = self.self(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 661, in new_func
    rel_att = self.disentangled_att_bias(query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 710, in new_func
    c2p_att = torch.gather(c2p_att, dim=-1, index=c2p_dynamic_expand(c2p_pos, query_layer, relative_pos))
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
RuntimeError: c2p_dynamic_expand() Expected a value of type 'Tensor (inferred)' for argument 'c2p_pos' but instead found type 'ConcreteProxy'.
Inferred 'c2p_pos' to be of type 'Tensor' because it was not annotated with an explicit type.
Position: 0
Value: ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]]))
Declaration: c2p_dynamic_expand(Tensor c2p_pos, Tensor query_layer, Tensor relative_pos) -> Tensor
Cast error details: Unable to cast ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]])) to Tensor

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 33, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

yhavinga/t5-small-24L-dutch-english failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-es-NORWAY failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-pap-fi failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Hate-speech-CNERG/deoffxlmr-mono-kannada failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


facebook/nllb-200-distilled-600M failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1181, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 966, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-en-sk failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

CEBaB/lstm.CEBaB.sa.2-class.exclusive.seed_66 failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'CEBaB/lstm.CEBaB.sa.2-class.exclusive.seed_66'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'CEBaB/lstm.CEBaB.sa.2-class.exclusive.seed_66' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

adityay1221/Xegho.30.4 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

google/t5-efficient-base-kv32 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-ms-it failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

meghazisofiane/opus-mt-en-ar-evaluated-en-to-ar-2000instancesopus-leaningRate2e-05-batchSize8-11epoch-3 failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

zyr212mj/my-yelp-model failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'zyr212mj/my-yelp-model'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'zyr212mj/my-yelp-model' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Modfiededition/t5-base-fine-tuned-on-jfleg failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

FatemahAlsubaiei/distilbert-base-uncased-finetuned-squad failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/FatemahAlsubaiei/distilbert-base-uncased-finetuned-squad/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f665b-08bac2346ace66f035a1f3a5;21d268b3-104c-4df8-8b2d-438a4737f2df)

Repository Not Found for url: https://huggingface.co/FatemahAlsubaiei/distilbert-base-uncased-finetuned-squad/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: FatemahAlsubaiei/distilbert-base-uncased-finetuned-squad is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

google/tapas-mini-masklm failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 96, in <module>
    concrete_args = tokenizer(text, return_tensors="pt")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/tapas/tokenization_tapas.py", line 643, in __call__
    assert isinstance(table, pd.DataFrame), "Table must be of type pd.DataFrame"
AssertionError: Table must be of type pd.DataFrame

textattack/albert-base-v2-imdb failed
Architectures: ['DistilBertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/albert/tokenization_albert_fast.py", line 148, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
AlbertConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


varun3dec/Pbi-Summarization-model failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

masakhane/mt5_wol_fr_news failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

PSW/t5-base-dialogsum-seed42 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ricardo-filho/bert-base-portuguese-cased-finetuned-ner failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/ricardo-filho/bert-base-portuguese-cased-finetuned-ner/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f66be-772109bc2d3c6bf06bfbe922;2e2462ec-e4fe-41a6-b9c6-6f74f86b1cc2)

Repository Not Found for url: https://huggingface.co/ricardo-filho/bert-base-portuguese-cased-finetuned-ner/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: ricardo-filho/bert-base-portuguese-cased-finetuned-ner is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

neurocode/Icelandic-NER-large failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'neurocode/Icelandic-NER-large'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'neurocode/Icelandic-NER-large' is the correct path to a directory containing all relevant files for a XLMRobertaTokenizerFast tokenizer.

Padomin/t5-base-TEDxJP-2front-1body-0rear failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

0x7194633/keyt5-base failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


vachevkd/dg-t5sm-race-v01 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Stancld/xglm-564M failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Stancld/xglm-564M'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Stancld/xglm-564M' is the correct path to a directory containing all relevant files for a XGLMTokenizerFast tokenizer.

cahya/t5-base-indonesian-summarization-cased failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


p123/autotrain-my-sum-1040935781 failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-fr-mfe failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

imosnoi/ro_bon_all_v2 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'imosnoi/ro_bon_all_v2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'imosnoi/ro_bon_all_v2' is the correct path to a directory containing all relevant files for a LayoutLMTokenizerFast tokenizer.

AI-Lab-Makerere/lg_en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-sv-pap failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

NbAiLab/nb-t5-base-v3 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

SamAct/PromptGeneration-base failed
Architectures: ['PegasusForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1221, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 981, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ffsouza/t5-tiny-random-length-96-learning_rate-2e-05-weight_decay-0.01-finetuned-en-to-ro failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

t5-large failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-fi-mt failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

UrukHan/t5-russian-spell failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

DeskDown/MarianMixFT_en-vi failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 789, in from_pretrained
    return tokenizer_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'DeskDown/MarianMixFT_en-vi'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'DeskDown/MarianMixFT_en-vi' is the correct path to a directory containing all relevant files for a MarianTokenizer tokenizer.

SEBIS/legal_t5_small_summ_fr failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


AntonClaesson/movie-plot-generator failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'AntonClaesson/movie-plot-generator'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'AntonClaesson/movie-plot-generator' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

kindly-generous/codet5-codeg failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

anikethjr/PromoGen_K562_V100_32GB_rerun failed
Architectures: ['ProphetNetForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/prophetnet/modeling_prophetnet.py", line 1828, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/prophetnet/modeling_prophetnet.py", line 1465, in forward
    raise ValueError("Either `decoder_input_ids` or `decoder_inputs_embeds` has to be passed.")
ValueError: Either `decoder_input_ids` or `decoder_inputs_embeds` has to be passed.

AvengingPrime/Reddit_and_Procon failed
Architectures: ['EncoderDecoderModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 101, in <module>
    model = AutoModel.from_config(config, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 443, in from_config
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig'> for this kind of AutoModel: AutoModel.
Model type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, LlamaConfig, CodeGenConfig, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MistralConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SamConfig, SeamlessM4TConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VitDetConfig, VitsConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig, XLMRobertaConfig, LSGBartConfig.

rahul77/t5-small-finetuned-xsum-rahul2 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

mrm8488/xlm-multi-finetuned-xquadv1 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm/tokenization_xlm.py", line 617, in __init__
    import sacremoses
ModuleNotFoundError: No module named 'sacremoses'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 789, in from_pretrained
    return tokenizer_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm/tokenization_xlm.py", line 619, in __init__
    raise ImportError(
ImportError: You need to install sacremoses to use XLMTokenizer. See https://pypi.org/project/sacremoses/ for installation.

Chirayu/subject-generator-t5-base failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

somesh212/Harry_Potter_botDialoGPT_Som3 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/somesh212/Harry_Potter_botDialoGPT_Som3/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-656f6818-369e37b64034fdf802c3316e;29452e98-51cb-4298-ade0-4cf918191df1)

Cannot access gated repo for url https://huggingface.co/somesh212/Harry_Potter_botDialoGPT_Som3/resolve/main/tokenizer_config.json.
Repo model somesh212/Harry_Potter_botDialoGPT_Som3 is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 445, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/somesh212/Harry_Potter_botDialoGPT_Som3 and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.

Violetto/my-dialogue-summarization-model failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 765, in from_pretrained
    raise ValueError(
ValueError: Tokenizer class MyTokenizer does not exist or is not currently imported.

masakhane/byt5_en_zul_news failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

mrgiraffe/grflayoutv1 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'mrgiraffe/grflayoutv1'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'mrgiraffe/grflayoutv1' is the correct path to a directory containing all relevant files for a LayoutLMv3TokenizerFast tokenizer.

muhtasham/bert-tiny-finetuned-parsed failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'muhtasham/bert-tiny-finetuned-parsed'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'muhtasham/bert-tiny-finetuned-parsed' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

theta/test_trainer failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/theta/test_trainer/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f6837-423e4fb93de6c7aa6dfc84fc;fbf734e8-5fdb-4989-8167-eb0c985bb60a)

Repository Not Found for url: https://huggingface.co/theta/test_trainer/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: theta/test_trainer is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

ghadeermobasher/BioRED-Dis-WLT-512-BlueBERT failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BioRED-Dis-WLT-512-BlueBERT'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BioRED-Dis-WLT-512-BlueBERT' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

SEBIS/code_trans_t5_large_code_documentation_generation_go_multitask failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


MingZhong/unieval-intermediate failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

m3/m3-experiment-roberta-base-tweet-eval-hate-word-swapping-embedding-3 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-roberta-base-tweet-eval-hate-word-swapping-embedding-3/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f684e-5f78d9c216f5443e3a279a0d;1093c0ee-32b0-4370-b6dc-01e699aa8ece)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-roberta-base-tweet-eval-hate-word-swapping-embedding-3/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-roberta-base-tweet-eval-hate-word-swapping-embedding-3 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

davidcechak/DNADeberta_finehuman_nontata_promoters failed
Architectures: ['DistilBertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'davidcechak/DNADeberta_finehuman_nontata_promoters'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'davidcechak/DNADeberta_finehuman_nontata_promoters' is the correct path to a directory containing all relevant files for a DebertaTokenizerFast tokenizer.

mughalk4/mBERT-indonesian-Mono failed
Architectures: ['DistilBertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'mughalk4/mBERT-indonesian-Mono'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'mughalk4/mBERT-indonesian-Mono' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

google/t5-efficient-small-nl20 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

shed-e/scipaper-summary failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Team-PIXEL/pixel-base-finetuned-masakhaner-wol failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1064, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 761, in __getitem__
    raise KeyError(key)
KeyError: 'pixel'

chrishuber/roberta-kaggledev-testing failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'chrishuber/roberta-kaggledev-testing'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'chrishuber/roberta-kaggledev-testing' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

Lowin/chinese-bigbird-mini-1024 failed
Architectures: ['CamembertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Lowin/chinese-bigbird-mini-1024'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Lowin/chinese-bigbird-mini-1024' is the correct path to a directory containing all relevant files for a BigBirdTokenizerFast tokenizer.

Ahmed007/BERT failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Ahmed007/BERT'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Ahmed007/BERT' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Splend1dchan/byt5small-squad1024-from6000steps failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Splend1dchan/byt5small-squad1024-from6000steps'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Splend1dchan/byt5small-squad1024-from6000steps' is the correct path to a directory containing all relevant files for a ByT5Tokenizer tokenizer.

edwardgowsmith/roberta-base-unigram-prime failed
Architectures: ['XLMRobertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'edwardgowsmith/roberta-base-unigram-prime'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'edwardgowsmith/roberta-base-unigram-prime' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

thu-coai/LongLM-small failed
Architectures: ['DistilBertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


ran/c9 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ran/c9'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ran/c9' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

SimoC/distilbert-base-uncased-finetuned-emotion failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/SimoC/distilbert-base-uncased-finetuned-emotion/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f68cb-04576c471f1b53944a92c612;dec5abb3-6f31-4e72-aa80-8523fee23f8c)

Repository Not Found for url: https://huggingface.co/SimoC/distilbert-base-uncased-finetuned-emotion/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: SimoC/distilbert-base-uncased-finetuned-emotion is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

diegozs97/sciie-seed-0-2000k failed
Architectures: ['GPTNeoXForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/sciie-seed-0-2000k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/sciie-seed-0-2000k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

AdapterHub/bert-base-uncased-pf-yelp_polarity failed
Architectures: ['GPTNeoXForCausalLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/AdapterHub/bert-base-uncased-pf-yelp_polarity/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f68ee-2327738c63611fd67292293f;4423faa8-2193-4c3b-91a8-fdf288e61eec)

Entry Not Found for url: https://huggingface.co/AdapterHub/bert-base-uncased-pf-yelp_polarity/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: AdapterHub/bert-base-uncased-pf-yelp_polarity does not appear to have a file named config.json. Checkout 'https://huggingface.co/AdapterHub/bert-base-uncased-pf-yelp_polarity/main' for available files.

Mr-Wick/xlnet-base-cased failed
Architectures: ['GPTNeoXForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Mr-Wick/xlnet-base-cased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Mr-Wick/xlnet-base-cased' is the correct path to a directory containing all relevant files for a XLNetTokenizerFast tokenizer.

koenvdv/my-test-model failed
Architectures: ['GPTNeoForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'koenvdv/my-test-model'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'koenvdv/my-test-model' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

Sebabrata/lmv2-g-dl-243-doc-09-13 failed
Architectures: ['GPTNeoForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 96, in <module>
    concrete_args = tokenizer(text, return_tensors="pt")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py", line 253, in __call__
    raise ValueError(
ValueError: Words must be of type `List[str]` (single pretokenized example), or `List[List[str]]` (batch of pretokenized examples).

misnaej/the-jam-machine-1024 failed
Architectures: ['LukeForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/misnaej/the-jam-machine-1024/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f690b-10f357bf70a227fe7084ec02;f63932ca-a04e-4215-9b5c-bff6b0a1c306)

Repository Not Found for url: https://huggingface.co/misnaej/the-jam-machine-1024/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: misnaej/the-jam-machine-1024 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

circulus/kobart-trans-chungcheong-v1 failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: BartModel.forward() got an unexpected keyword argument 'token_type_ids'

j0hngou/2teachersdistilllowresource failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

KY/KY_test_model failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'KY/KY_test_model'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'KY/KY_test_model' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

rossanez/t5-small-finetuned-de-en-final failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

FelipeAD/mt5-small-finetuned-amazon-en-es failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Tritkoman/RussiantoChukchi failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

m3/m3-experiment-albert-base-v2-amcd-word-swapping-embedding-1 failed
Architectures: ['XLMRobertaForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-amcd-word-swapping-embedding-1/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f6956-703e2774485461c02d1cd853;6bf9caf6-1f9d-4cb7-af07-c46d6b92e233)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-amcd-word-swapping-embedding-1/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-albert-base-v2-amcd-word-swapping-embedding-1 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

SEBIS/code_trans_t5_base_code_documentation_generation_javascript_transfer_learning_finetune failed
Architectures: ['XLMRobertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


ydshieh/tiny-random-LiltForTokenClassification failed
Architectures: ['XLMRobertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 96, in <module>
    concrete_args = tokenizer(text, return_tensors="pt")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/layoutlmv3/tokenization_layoutlmv3_fast.py", line 299, in __call__
    raise ValueError(
ValueError: Words must be of type `List[str]` (single pretokenized example), or `List[List[str]]` (batch of pretokenized examples).

ghadeermobasher/BC2GM-WLT-320-BioBERT-latest failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC2GM-WLT-320-BioBERT-latest'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC2GM-WLT-320-BioBERT-latest' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

KeLiu/QETRA_HTML failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Helsinki-NLP/opus-mt-to-fr failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

HJOK/free-bart-v1 failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: BartModel.forward() got an unexpected keyword argument 'token_type_ids'

north/t5_base_NCC_lm failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

BeIR/query-gen-msmarco-t5-base-v1 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


SEBIS/code_trans_t5_large_code_documentation_generation_php_multitask_finetune failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


hossein20s/bert-base-multilingual-cased-enrun10k-train0.1-validation0.1-test0.1-revisit failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'hossein20s/bert-base-multilingual-cased-enrun10k-train0.1-validation0.1-test0.1-revisit'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'hossein20s/bert-base-multilingual-cased-enrun10k-train0.1-validation0.1-test0.1-revisit' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

dapang/gpt2-medium failed
Architectures: ['XLMRobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'dapang/gpt2-medium'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'dapang/gpt2-medium' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

masakhane/m2m100_418M_fr_bam_rel_news failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1181, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 966, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

aari1995/gBERT-large-STS_v2 failed
Architectures: ['OPTForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/aari1995/gBERT-large-STS_v2/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f6a19-30e3bd0d3aec234c115e501d;7252d1dc-b664-446e-aaba-ce166f137c9e)

Repository Not Found for url: https://huggingface.co/aari1995/gBERT-large-STS_v2/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: aari1995/gBERT-large-STS_v2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

adamlin/zero-shot-domain_cls failed
Architectures: ['OPTForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/adamlin/zero-shot-domain_cls/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f6a19-7d7b01731dce6c4a52238b63;51cb75f0-18e2-4d3b-b3cf-cac9da5560a7)

Repository Not Found for url: https://huggingface.co/adamlin/zero-shot-domain_cls/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: adamlin/zero-shot-domain_cls is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

harish/t5-e2e-10epochs-lr1e4-alpha0-1PLUSalpha0-9-e10 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

chinhon/bart-large-chinese-cnhdwriter failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: BartModel.forward() got an unexpected keyword argument 'token_type_ids'

stanfordnlp/stanza-te failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/stanfordnlp/stanza-te/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f6a2c-57c3bf547fe491e473be5357;ea78db68-0233-4800-849e-173fd8a21ceb)

Entry Not Found for url: https://huggingface.co/stanfordnlp/stanza-te/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: stanfordnlp/stanza-te does not appear to have a file named config.json. Checkout 'https://huggingface.co/stanfordnlp/stanza-te/main' for available files.

castorini/doc2query-t5-large-msmarco failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


anas-awadalla/t5-small-few-shot-k-1024-finetuned-squad-seed-0 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

premrawat/en_ner_skills failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/premrawat/en_ner_skills/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f6a42-6ed20978095941e4760d5c85;0868d742-ea7d-44de-8b4a-3d3cead09f44)

Entry Not Found for url: https://huggingface.co/premrawat/en_ner_skills/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: premrawat/en_ner_skills does not appear to have a file named config.json. Checkout 'https://huggingface.co/premrawat/en_ner_skills/main' for available files.

aopstudio/my-summary failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'aopstudio/my-summary'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'aopstudio/my-summary' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

AtherMob/my_Med failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'AtherMob/my_Med'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'AtherMob/my_Med' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

pistachiocow/product_description_generator_bad failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'pistachiocow/product_description_generator_bad'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'pistachiocow/product_description_generator_bad' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

akhooli/mbart-large-cc25-en-ar failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mbart/tokenization_mbart_fast.py", line 123, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
MBartConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


tal-yifat/injury-report-test failed
Architectures: ['BertModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'tal-yifat/injury-report-test'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'tal-yifat/injury-report-test' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

ghadeermobasher/BC2GM-WLT-384-PubMedBERT-latest failed
Architectures: ['BertModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC2GM-WLT-384-PubMedBERT-latest'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC2GM-WLT-384-PubMedBERT-latest' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

valurank/Pegasus_cnn_news_headline_generator failed
Architectures: ['PegasusForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1221, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 981, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Batool/en_pipeline failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/Batool/en_pipeline/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f6a72-1432d8cb7b31de96553d59a3;665ee96d-1148-4232-913f-9f4b5b828d07)

Entry Not Found for url: https://huggingface.co/Batool/en_pipeline/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: Batool/en_pipeline does not appear to have a file named config.json. Checkout 'https://huggingface.co/Batool/en_pipeline/main' for available files.

ririying/mt5-small-finetuned-mt5-class1 failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

FutureFanatik/DialoGPT-small-rick failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'FutureFanatik/DialoGPT-small-rick'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'FutureFanatik/DialoGPT-small-rick' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

laituan245/molt5-small-caption2smiles failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Gillner/SciGPT2 failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Gillner/SciGPT2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Gillner/SciGPT2' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

it5/it5-base-ilgiornale-to-repubblica failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

SEBIS/legal_t5_small_cls_fr failed
Architectures: ['BartForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


mrm8488/mbart-large-finetuned-bible-es-en-translation failed
Architectures: ['BartForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mbart/tokenization_mbart_fast.py", line 123, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
MBartConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


seduerr/pai_m2f failed
Architectures: ['ElectraForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'seduerr/pai_m2f'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'seduerr/pai_m2f' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

hossein20s/distilbert-base-multilingual-cased-eml_en failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/hossein20s/distilbert-base-multilingual-cased-eml_en/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f6abf-149b0cda1d2058a5393d7828;865cfabb-e0f6-4b7c-b91d-decc96e7abbe)

Repository Not Found for url: https://huggingface.co/hossein20s/distilbert-base-multilingual-cased-eml_en/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: hossein20s/distilbert-base-multilingual-cased-eml_en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

chiendvhust/bert-finetuned-squad failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/chiendvhust/bert-finetuned-squad/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f6ac6-3c8fdc9d48afffa14c70ec99;e38b0ac6-1546-4fe1-b21b-ddc62be2885b)

Repository Not Found for url: https://huggingface.co/chiendvhust/bert-finetuned-squad/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: chiendvhust/bert-finetuned-squad is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Lvxue/distilled-mt5-small-010099-0.5 failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

juns/imdb_finetuned_distilbert-base-uncased-finetuned-sst-2-english failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'juns/imdb_finetuned_distilbert-base-uncased-finetuned-sst-2-english'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'juns/imdb_finetuned_distilbert-base-uncased-finetuned-sst-2-english' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

Edomonndo/opus-mt-ja-en-finetuned-ja-to-en_test failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

sorryhyun/koreancomp2022-entattr-model-seed-123 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/sorryhyun/koreancomp2022-entattr-model-seed-123/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f6ad9-79b60405414467af7038c9b6;49ee1034-1ab0-46e9-9e43-545f3c9ee88c)

Repository Not Found for url: https://huggingface.co/sorryhyun/koreancomp2022-entattr-model-seed-123/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: sorryhyun/koreancomp2022-entattr-model-seed-123 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

aayu/bert-large-uncased-finetuned-JD_CV failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'aayu/bert-large-uncased-finetuned-JD_CV'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'aayu/bert-large-uncased-finetuned-JD_CV' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-fr-ny failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

lmxhappy/xiaosenlin-bert failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/lmxhappy/xiaosenlin-bert/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-656f6ae0-5236454c60897afe206c1c96;90e2305b-5ed5-4b41-b5aa-e4207306eaca)

Cannot access gated repo for url https://huggingface.co/lmxhappy/xiaosenlin-bert/resolve/main/tokenizer_config.json.
Repo model lmxhappy/xiaosenlin-bert is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 445, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/lmxhappy/xiaosenlin-bert and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.

Rocketknight1/gpt2-wikitext2 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Rocketknight1/gpt2-wikitext2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Rocketknight1/gpt2-wikitext2' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

PontifexMaximus/opus-mt-en-de-finetuned-de-to-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

castorini/monot5-small-msmarco-10k failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


xfbai/AMRBART-large-finetuned-AMR2.0-AMRParsing failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'xfbai/AMRBART-large-finetuned-AMR2.0-AMRParsing'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'xfbai/AMRBART-large-finetuned-AMR2.0-AMRParsing' is the correct path to a directory containing all relevant files for a BartTokenizerFast tokenizer.

longhoang06/hgf-guide-fine-tune-xlm-viquad failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/longhoang06/hgf-guide-fine-tune-xlm-viquad/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f6b3f-46a5649745ee091f7dd26e51;837b29a2-55b2-4675-a5ff-d5148674d3a2)

Repository Not Found for url: https://huggingface.co/longhoang06/hgf-guide-fine-tune-xlm-viquad/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: longhoang06/hgf-guide-fine-tune-xlm-viquad is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

ddebnath/layoutlmv3-finetuned-invoice failed
Architectures: ['XLMRobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 96, in <module>
    concrete_args = tokenizer(text, return_tensors="pt")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/layoutlmv3/tokenization_layoutlmv3_fast.py", line 299, in __call__
    raise ValueError(
ValueError: Words must be of type `List[str]` (single pretokenized example), or `List[List[str]]` (batch of pretokenized examples).

Helsinki-NLP/opus-mt-sv-umb failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

snrspeaks/t5-one-line-summary failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

beston91/gpt2-xl_ft_mult_25k failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/beston91/gpt2-xl_ft_mult_25k/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-656f6b9f-55ea53b0338e370a3be53e13;eacd8e9b-5c3c-4f72-be21-6309654559c7)

Cannot access gated repo for url https://huggingface.co/beston91/gpt2-xl_ft_mult_25k/resolve/main/tokenizer_config.json.
Repo model beston91/gpt2-xl_ft_mult_25k is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 445, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/beston91/gpt2-xl_ft_mult_25k and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.

m3/m3-experiment-albert-base-v2-sciie-word-swapping-random-1 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-sciie-word-swapping-random-1/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f6ba9-529869d246443e616dc160da;d840e91b-99fd-4c33-a3e3-f9af95c6c977)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-sciie-word-swapping-random-1/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-albert-base-v2-sciie-word-swapping-random-1 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

stevemobs/deberta-base-finetuned-aqa-newsqa failed
Architectures: ['DebertaForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 31, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 974, in new_func
    encoder_outputs = self.encoder(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 470, in new_func
    hidden_states = layer_module(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 383, in new_func
    attention_output = self.attention(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 316, in new_func
    self_output = self.self(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 661, in new_func
    rel_att = self.disentangled_att_bias(query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 710, in new_func
    c2p_att = torch.gather(c2p_att, dim=-1, index=c2p_dynamic_expand(c2p_pos, query_layer, relative_pos))
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
RuntimeError: c2p_dynamic_expand() Expected a value of type 'Tensor (inferred)' for argument 'c2p_pos' but instead found type 'ConcreteProxy'.
Inferred 'c2p_pos' to be of type 'Tensor' because it was not annotated with an explicit type.
Position: 0
Value: ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]]))
Declaration: c2p_dynamic_expand(Tensor c2p_pos, Tensor query_layer, Tensor relative_pos) -> Tensor
Cast error details: Unable to cast ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]])) to Tensor

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 33, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

facebook/xglm-7.5B failed
Architectures: ['XGLMForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 31, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py", line 585, in new_func
    hidden_states = inputs_embeds + self.embed_positions(position_ids, past_key_values_length)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 114, in new_func
    with ctx_factory():
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 974, in torch_no_grad_enter_wrapper
    return self.create_proxy('call_function', _orig_torch_no_grad_enter, (no_grad,), {})
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 446, in create_proxy
    args_ = self.create_arg(args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 555, in create_arg
    return super().create_arg(a)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/proxy.py", line 239, in create_arg
    return type(a)(self.create_arg(elem) for elem in a)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/proxy.py", line 239, in <genexpr>
    return type(a)(self.create_arg(elem) for elem in a)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 555, in create_arg
    return super().create_arg(a)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/proxy.py", line 267, in create_arg
    raise NotImplementedError(f"argument of type: {type(a)}")
NotImplementedError: argument of type: <class 'torch.autograd.grad_mode.no_grad'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 33, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

dh-unibe/koenigsfelden-ner-v1 failed
Architectures: ['XGLMForCausalLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/dh-unibe/koenigsfelden-ner-v1/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f6c13-389395ff6a3999471ecf21be;eeb503dd-9346-4bb8-ab43-58abbf7c1aee)

Entry Not Found for url: https://huggingface.co/dh-unibe/koenigsfelden-ner-v1/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: dh-unibe/koenigsfelden-ner-v1 does not appear to have a file named config.json. Checkout 'https://huggingface.co/dh-unibe/koenigsfelden-ner-v1/main' for available files.

hackathon-pln-es/poem-gen-spanish-t5-small failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

nile/koBERT-finetuned-wholemasking20 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/nile/koBERT-finetuned-wholemasking20/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f6c22-6b8d707b7d2ae9e4627e3d4d;ba808028-80d5-43da-bd29-6949babf1458)

Repository Not Found for url: https://huggingface.co/nile/koBERT-finetuned-wholemasking20/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: nile/koBERT-finetuned-wholemasking20 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Jellywibble/dalio-pretrain-finetuned-restruct-v4 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Jellywibble/dalio-pretrain-finetuned-restruct-v4'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Jellywibble/dalio-pretrain-finetuned-restruct-v4' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

FritzOS/TEdetection_distiBERT_NER_V4 failed
Architectures: ['PLBartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'FritzOS/TEdetection_distiBERT_NER_V4'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'FritzOS/TEdetection_distiBERT_NER_V4' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

junnyu/roformer_base_wwm_cluecorpussmall failed
Architectures: ['RoFormerForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 31, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/roformer/modeling_roformer.py", line 914, in new_func
    encoder_outputs = self.encoder(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/roformer/modeling_roformer.py", line 570, in new_func
    sinusoidal_pos = self.embed_positions(hidden_states.shape[:-1], past_key_values_length)[None, None, :, :]
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 114, in new_func
    with ctx_factory():
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 974, in torch_no_grad_enter_wrapper
    return self.create_proxy('call_function', _orig_torch_no_grad_enter, (no_grad,), {})
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 446, in create_proxy
    args_ = self.create_arg(args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 555, in create_arg
    return super().create_arg(a)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/proxy.py", line 239, in create_arg
    return type(a)(self.create_arg(elem) for elem in a)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/proxy.py", line 239, in <genexpr>
    return type(a)(self.create_arg(elem) for elem in a)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 555, in create_arg
    return super().create_arg(a)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/proxy.py", line 267, in create_arg
    raise NotImplementedError(f"argument of type: {type(a)}")
NotImplementedError: argument of type: <class 'torch.autograd.grad_mode.no_grad'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 33, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

eagles/focus_sum_mT5_minshi failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

dippatel11/autotrain-dippatel_summarizer-2331873598 failed
Architectures: ['PegasusForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1221, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 981, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

spacy/nb_core_news_md failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/spacy/nb_core_news_md/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f6c6d-526e65141d13967c1d1b3b29;890537d1-7a2a-477d-bd18-d640493612f0)

Entry Not Found for url: https://huggingface.co/spacy/nb_core_news_md/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: spacy/nb_core_news_md does not appear to have a file named config.json. Checkout 'https://huggingface.co/spacy/nb_core_news_md/main' for available files.

felipetanios/opus-mt-de-en-finetuned-de-to-en-second failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

prodm93/T5Dynamic_text_model_v1 failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'prodm93/T5Dynamic_text_model_v1'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'prodm93/T5Dynamic_text_model_v1' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

Zeno-PT/ner-model-1 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/camembert/tokenization_camembert_fast.py", line 127, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
CamembertConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


ghadeermobasher/WLT-SciBERT-NCBI failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/WLT-SciBERT-NCBI'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/WLT-SciBERT-NCBI' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

diegozs97/finetuned-chemprot-seed-4-20k failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/finetuned-chemprot-seed-4-20k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/finetuned-chemprot-seed-4-20k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-ln-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

AravindKumarRajendran/t5-small-xsum failed
Architectures: ['XLMRobertaForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/AravindKumarRajendran/t5-small-xsum/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f6cae-2996105b4f08cce01ad638a8;52b9fd91-b90e-4b9f-a6d7-33e3c6c458f0)

Repository Not Found for url: https://huggingface.co/AravindKumarRajendran/t5-small-xsum/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: AravindKumarRajendran/t5-small-xsum is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Helsinki-NLP/opus-mt-cpp-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

google/mt5-xxl failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


SiriRRR/mt5-small-finetuned-test failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

google/t5-efficient-mini-nl8 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

p208p2002/bart-drcd-qg-hl-v2 failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: BartModel.forward() got an unexpected keyword argument 'token_type_ids'

anas-awadalla/t5-base-few-shot-k-1024-finetuned-squad-seed-4 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

research-backup/t5-small-squadshifts-vanilla-nyt-qg failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

twieland/MIX2_ja-en_helsinki failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-sv-el failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

danyaljj/gpt-j-6B-step-383000 failed
Architectures: ['GPTJForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 109, in <module>
    after_trace = traced_gm(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 662, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 281, in __call__
    raise e
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 271, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "<eval_with_key>.3057", line 59, in forward
    getitem_9 = getattr_8[-1];  getattr_8 = None
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_proxy.py", line 431, in impl
    return tracer.create_proxy('call_function', target, args, kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 444, in create_proxy
    value_unwrapped = self.run_target(kind, target, args_unwrapped, kwargs_unwrapped)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 374, in run_target
    result = run(kind, target, args, kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 347, in run
    return OperatorPatcherContext.patch_run(fn, *args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 291, in patch_run
    assert OperatorPatcherContext.ctx_tracer is not None
AssertionError

ybelkada/t5-3b-sharded failed
Architectures: ['T5WithLMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

bertin-project/bertin-gpt-j-6B failed
Architectures: ['GPTJForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 109, in <module>
    after_trace = traced_gm(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 662, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 281, in __call__
    raise e
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 271, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "<eval_with_key>.3059", line 59, in forward
    getitem_9 = getattr_8[-1];  getattr_8 = None
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_proxy.py", line 431, in impl
    return tracer.create_proxy('call_function', target, args, kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 444, in create_proxy
    value_unwrapped = self.run_target(kind, target, args_unwrapped, kwargs_unwrapped)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 374, in run_target
    result = run(kind, target, args, kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 347, in run
    return OperatorPatcherContext.patch_run(fn, *args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 291, in patch_run
    assert OperatorPatcherContext.ctx_tracer is not None
AssertionError

hyunwoongko/asian-bart-ja failed
Architectures: ['MBartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py", line 1413, in forward
    encoder_outputs = self.encoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py", line 999, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self

ghadeermobasher/BC2GM-WLT-256-PubMedBERT-latest-30 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC2GM-WLT-256-PubMedBERT-latest-30'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC2GM-WLT-256-PubMedBERT-latest-30' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

widyanto/IndoT5-small-qg failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

BeardedJohn/bert-finetuned-ner-ubb-endava-2 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/BeardedJohn/bert-finetuned-ner-ubb-endava-2/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f6dbe-3dc1e0113f5f36f120a22ec5;0795e45f-bee7-4d6c-89c3-c7f38b2daf74)

Repository Not Found for url: https://huggingface.co/BeardedJohn/bert-finetuned-ner-ubb-endava-2/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: BeardedJohn/bert-finetuned-ner-ubb-endava-2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Elaine/mbart_pruned_zh failed
Architectures: ['MPNetModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/Elaine/mbart_pruned_zh/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f6dca-5634b506407012aa547191c0;36b4ec3f-8eac-43c7-89c8-5f826d2f36ac)

Repository Not Found for url: https://huggingface.co/Elaine/mbart_pruned_zh/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: Elaine/mbart_pruned_zh is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

kimcando/para_test_4800 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'kimcando/para_test_4800'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'kimcando/para_test_4800' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

shibing624/bart4csc-base-chinese failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: BartModel.forward() got an unexpected keyword argument 'token_type_ids'

Salesforce/qaconv-unifiedqa-t5-large failed
Architectures: ['MPNetModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Vlasta/L3UOT_best_K6Stride1Wide1epoch10percent_size failed
Architectures: ['GPTNeoForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Vlasta/L3UOT_best_K6Stride1Wide1epoch10percent_size'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Vlasta/L3UOT_best_K6Stride1Wide1epoch10percent_size' is the correct path to a directory containing all relevant files for a DebertaTokenizerFast tokenizer.

Nerdward/pegasus-tf-finetuned-model failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/tokenization_pegasus_fast.py", line 147, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
PegasusConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


laituan245/t5-v1_1-small-smiles2caption-ft-from-pretrained-zinc failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


jinhybr/OCR-LayoutLMv3 failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 96, in <module>
    concrete_args = tokenizer(text, return_tensors="pt")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/layoutlmv3/tokenization_layoutlmv3_fast.py", line 299, in __call__
    raise ValueError(
ValueError: Words must be of type `List[str]` (single pretokenized example), or `List[List[str]]` (batch of pretokenized examples).

strombergnlp/dant5-large failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Ogayo/mt-adh-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ucabqfe/roberta_AAE_bio failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ucabqfe/roberta_AAE_bio'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ucabqfe/roberta_AAE_bio' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

ghadeermobasher/linnaeus-WLT-512-PubMedBERT failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/linnaeus-WLT-512-PubMedBERT'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/linnaeus-WLT-512-PubMedBERT' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

ghadeermobasher/BioRED-Dis-WLT-128-BioBERT failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BioRED-Dis-WLT-128-BioBERT'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BioRED-Dis-WLT-128-BioBERT' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

PSW/bart-base-samsumgen-xsum-conv-bertscore-sorted-top25-3rd failed
Architectures: ['GectorbertaForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/PSW/bart-base-samsumgen-xsum-conv-bertscore-sorted-top25-3rd/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f6ea0-2c218c4e55935ff5308ce3fe;47563039-a0f4-4ffd-9311-98859076a5be)

Repository Not Found for url: https://huggingface.co/PSW/bart-base-samsumgen-xsum-conv-bertscore-sorted-top25-3rd/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: PSW/bart-base-samsumgen-xsum-conv-bertscore-sorted-top25-3rd is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

enaserian/distilbert-base-uncased-finetuned failed
Architectures: ['GectorbertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'enaserian/distilbert-base-uncased-finetuned'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'enaserian/distilbert-base-uncased-finetuned' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

zeineb/LearningQ-t5-Answer-agnostic-QG failed
Architectures: ['GectorbertaForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/zeineb/LearningQ-t5-Answer-agnostic-QG/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f6ea1-67002fa5543aad4755b3669e;223538b7-9f05-4c25-9f63-82608e6c109c)

Repository Not Found for url: https://huggingface.co/zeineb/LearningQ-t5-Answer-agnostic-QG/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: zeineb/LearningQ-t5-Answer-agnostic-QG is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

MarioCarmona/gpt-j-pretrained failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'MarioCarmona/gpt-j-pretrained'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'MarioCarmona/gpt-j-pretrained' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

m3/m3-experiment-albert-base-v2-amcd-add-v4 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-amcd-add-v4/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f6ec1-4abc2aa330bca1d719092bfe;43bcd1a0-724f-4fe4-9b96-8de751de1344)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-amcd-add-v4/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-albert-base-v2-amcd-add-v4 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

EMBEDDIA/sloberta-tweetsentiment failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/camembert/tokenization_camembert_fast.py", line 127, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
CamembertConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


MickyMike/6-GPT2SP-usergrid failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'MickyMike/6-GPT2SP-usergrid'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'MickyMike/6-GPT2SP-usergrid' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

kykim/albert-kor-base failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2056, in _from_pretrained
    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/albert/tokenization_albert.py", line 168, in __init__
    self.sp_model.Load(vocab_file)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/sentencepiece/__init__.py", line 905, in Load
    return self.LoadFromFile(model_file)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/sentencepiece/__init__.py", line 310, in LoadFromFile
    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg)
TypeError: not a string

pfactorial/checkpoint-50-epoch-2 failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


tau/False_large_pmi_para0_sent1_span2_itTrue_sargmax_rrFalse_8_1024_0.3_best failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


danny911kr/calm-mix-large failed
Architectures: ['XLMRobertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


eleldar/marian-finetuned-kde4-en-to-fr failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

mdineshk/distilbert-base-uncased-meded failed
Architectures: ['DistilBertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: DistilBertModel.forward() got an unexpected keyword argument 'token_type_ids'

negfir/Distill_4L_2ep failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'negfir/Distill_4L_2ep'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'negfir/Distill_4L_2ep' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

rmihaylov/roberta2roberta-shared-nmt-bg failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


trangdieu/roberta-base-retrained-6-epochs failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'trangdieu/roberta-base-retrained-6-epochs'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'trangdieu/roberta-base-retrained-6-epochs' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

paola-md/distilr2-lr1e05-wd0.05-bs64 failed
Architectures: ['DistilBertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'paola-md/distilr2-lr1e05-wd0.05-bs64'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'paola-md/distilr2-lr1e05-wd0.05-bs64' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

ghadeermobasher/Modified-BlueBERT-BioRED-Chem failed
Architectures: ['DistilBertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/Modified-BlueBERT-BioRED-Chem'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/Modified-BlueBERT-BioRED-Chem' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

boychaboy/MNLI_roberta-large failed
Architectures: ['DistilBertForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/boychaboy/MNLI_roberta-large/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f6f3a-03992e9b2c629ad9779d3c28;47521dcd-8f80-4790-9db9-c4511c5b66f6)

Repository Not Found for url: https://huggingface.co/boychaboy/MNLI_roberta-large/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: boychaboy/MNLI_roberta-large is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

vidhur2k/mBERT-Italian-Mono failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'vidhur2k/mBERT-Italian-Mono'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'vidhur2k/mBERT-Italian-Mono' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

ghadeermobasher/BC5CDR-disease-WLT-512-SciBERT-latest-85 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC5CDR-disease-WLT-512-SciBERT-latest-85'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC5CDR-disease-WLT-512-SciBERT-latest-85' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Linguist/t5-small-Linguists_summariser failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

aristotletan/t5-small-finetuned-xsum failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

KM4STfulltext/CSS_BERT failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/KM4STfulltext/CSS_BERT/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f6f70-7c6ce0d66aca11194335dce0;26357e2a-12ac-4e23-9c6b-e23ff1a332f5)

Repository Not Found for url: https://huggingface.co/KM4STfulltext/CSS_BERT/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: KM4STfulltext/CSS_BERT is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Mohan515/t5-small-finetuned-medical failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

springml111/T5_Paraphrase_model failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


dropout05/t5-realnewslike-super-tiny failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ghadeermobasher/BioRED-Dis-WLT-128-BlueBERT-90 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BioRED-Dis-WLT-128-BlueBERT-90'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BioRED-Dis-WLT-128-BlueBERT-90' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-en-da failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

kobkrit/wangchanberta-ner-2 failed
Architectures: ['DistilBertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/camembert/tokenization_camembert_fast.py", line 127, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
CamembertConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


asakawa/gpt2-wikitext2 failed
Architectures: ['DistilBertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'asakawa/gpt2-wikitext2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'asakawa/gpt2-wikitext2' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

hf-internal-testing/tiny-random-SwitchTransformersForConditionalGeneration failed
Architectures: ['SwitchTransformersForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 1437, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 966, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

prajdabre/morisien_english failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/albert/tokenization_albert_fast.py", line 148, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
AlbertConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


chinhon/pegasus-newsroom-headline_writer_oct22 failed
Architectures: ['PegasusForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1221, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 981, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

actionpace/pegasus-samsum failed
Architectures: ['PegasusForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1221, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 981, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

neerajp/en_core_web_lg failed
Architectures: ['PegasusForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/neerajp/en_core_web_lg/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f7000-6a66d9183dce0af55ed83479;63c076e3-1118-4a0c-a2d4-ef8ed69bdd68)

Entry Not Found for url: https://huggingface.co/neerajp/en_core_web_lg/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: neerajp/en_core_web_lg does not appear to have a file named config.json. Checkout 'https://huggingface.co/neerajp/en_core_web_lg/main' for available files.

rcorkill/BERTRAM_bert failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/rcorkill/BERTRAM_bert/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-656f7009-0ad203304df3eef62f6784b0;d5bef8a6-95d3-48a8-8659-18bd86e278d0)

Cannot access gated repo for url https://huggingface.co/rcorkill/BERTRAM_bert/resolve/main/tokenizer_config.json.
Repo model rcorkill/BERTRAM_bert is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 445, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/rcorkill/BERTRAM_bert and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.

anas-awadalla/t5-base-few-shot-k-512-finetuned-squad-infilling-seed-0 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

lmqg/t5-base-squadshifts-new_wiki-qg failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

MickyMike/77-GPT2SP-mesos-usergrid failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'MickyMike/77-GPT2SP-mesos-usergrid'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'MickyMike/77-GPT2SP-mesos-usergrid' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

mrm8488/b2b-en-paraphrasing-questions failed
Architectures: ['EncoderDecoderModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 101, in <module>
    model = AutoModel.from_config(config, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 443, in from_config
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig'> for this kind of AutoModel: AutoModel.
Model type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, LlamaConfig, CodeGenConfig, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MistralConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SamConfig, SeamlessM4TConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VitDetConfig, VitsConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig, XLMRobertaConfig, LSGBartConfig.

gokcesrci/distillbert-base-uncased-finetuned-squad-d5716d28 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'gokcesrci/distillbert-base-uncased-finetuned-squad-d5716d28'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'gokcesrci/distillbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

Boglinger/mt5-small-klex failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ArafatBHossain/bert-base-mrpc failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ArafatBHossain/bert-base-mrpc'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ArafatBHossain/bert-base-mrpc' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Vlasta/humandna_distillbert_default_ failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Vlasta/humandna_distillbert_default_'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Vlasta/humandna_distillbert_default_' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

Aleksandar1932/gpt2-country failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Aleksandar1932/gpt2-country'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Aleksandar1932/gpt2-country' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

google/t5-efficient-large-dm512 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

m3/m3-experiment-albert-base-v2-chemprot-word-swapping-random-4 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-chemprot-word-swapping-random-4/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f7073-38e428385d7b83082a565f4c;cf3172a8-ab20-45fa-afe1-b16068a47054)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-chemprot-word-swapping-random-4/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-albert-base-v2-chemprot-word-swapping-random-4 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

ricardo-filho/bert_base_tcm_no_objeto_0.8 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/ricardo-filho/bert_base_tcm_no_objeto_0.8/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f7077-1196baf648cf70cb1eeb3163;22829e42-b69f-48f4-a32f-434e1db6f0b5)

Repository Not Found for url: https://huggingface.co/ricardo-filho/bert_base_tcm_no_objeto_0.8/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: ricardo-filho/bert_base_tcm_no_objeto_0.8 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

browndw/en_docusco_spacy failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/browndw/en_docusco_spacy/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f707e-09c9c99d6bf77ae838cb10d6;7b66d3f2-4942-4889-b776-cd420c11e124)

Entry Not Found for url: https://huggingface.co/browndw/en_docusco_spacy/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: browndw/en_docusco_spacy does not appear to have a file named config.json. Checkout 'https://huggingface.co/browndw/en_docusco_spacy/main' for available files.

diegozs97/finetuned-chemprot-seed-4-2000k failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/finetuned-chemprot-seed-4-2000k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/finetuned-chemprot-seed-4-2000k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Chemsseddine/flaubert_base_cased-finetuned-DOP6 failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/flaubert/tokenization_flaubert.py", line 262, in __init__
    import sacremoses
ModuleNotFoundError: No module named 'sacremoses'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/flaubert/tokenization_flaubert.py", line 264, in __init__
    raise ImportError(
ImportError: You need to install sacremoses to use FlaubertTokenizer. See https://pypi.org/project/sacremoses/ for installation.

jvdzwaan/ocrpostcorrection-task-1 failed
Architectures: ['DistilBertModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'jvdzwaan/ocrpostcorrection-task-1'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'jvdzwaan/ocrpostcorrection-task-1' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

ConvLab/t5-small-nlu-multiwoz21 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

tau/fewsion_4_1024_0.3_epoch1 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'tau/fewsion_4_1024_0.3_epoch1'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'tau/fewsion_4_1024_0.3_epoch1' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

erickfm/t5-large-finetuned-bias-m failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


MrBananaHuman/konec_baseline failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert_fast.py", line 221, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 111, in __init__
    fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)
Exception: Permission denied (os error 13)

paola-md/distilr2-lr1e05-wd0.08-bs16 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'paola-md/distilr2-lr1e05-wd0.08-bs16'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'paola-md/distilr2-lr1e05-wd0.08-bs16' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

flair/chunk-english failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/flair/chunk-english/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f70da-1f3be7db4bff2711603d0007;8ff01413-e62e-4bfd-ad82-8d4bfec2248f)

Entry Not Found for url: https://huggingface.co/flair/chunk-english/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: flair/chunk-english does not appear to have a file named config.json. Checkout 'https://huggingface.co/flair/chunk-english/main' for available files.

seyonec/SmilesTokenizer_ChemBERTa_zinc250k_40k failed
Architectures: ['MPNetModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2056, in _from_pretrained
    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/roberta/tokenization_roberta.py", line 221, in __init__
    with open(vocab_file, encoding="utf-8") as vocab_handle:
TypeError: expected str, bytes or os.PathLike object, not NoneType

m3/m3-experiment-roberta-base-amcd-word-swapping-embedding-0 failed
Architectures: ['MPNetModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-roberta-base-amcd-word-swapping-embedding-0/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f70e8-61d2cf3556c26a7b6a92713a;56f1fb9a-a4e7-4e76-be70-42657874fdb0)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-roberta-base-amcd-word-swapping-embedding-0/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-roberta-base-amcd-word-swapping-embedding-0 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

IDEA-CCNL/Randeng-Pegasus-238M-Chinese failed
Architectures: ['MPNetModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2056, in _from_pretrained
    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/tokenization_pegasus.py", line 149, in __init__
    self.sp_model.Load(vocab_file)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/sentencepiece/__init__.py", line 905, in Load
    return self.LoadFromFile(model_file)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/sentencepiece/__init__.py", line 310, in LoadFromFile
    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg)
TypeError: not a string

rossanez/t5-small-finetuned-de-en-nofp16 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Haakf/allsides_left_headline_conc failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/Haakf/allsides_left_headline_conc/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f70fa-57de1c13726c02f406f1ed60;7de89911-f5eb-41b4-96cc-f3c6cce45e0b)

Repository Not Found for url: https://huggingface.co/Haakf/allsides_left_headline_conc/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: Haakf/allsides_left_headline_conc is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Helsinki-NLP/opus-mt-kg-fr failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

anahitapld/t5-DBD failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'anahitapld/t5-DBD'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'anahitapld/t5-DBD' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

matheusvolpon/WE4LKD_AML_distilbert_1921_1977 failed
Architectures: ['DistilBertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: DistilBertModel.forward() got an unexpected keyword argument 'token_type_ids'

allenai/System3_DREAM_FLUTE_all_dimensions_FigLang2022 failed
Architectures: ['XLMRobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'allenai/System3_DREAM_FLUTE_all_dimensions_FigLang2022'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'allenai/System3_DREAM_FLUTE_all_dimensions_FigLang2022' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

JasperD-UGent/roberta-base-bne-complexity-classifier-v1 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'JasperD-UGent/roberta-base-bne-complexity-classifier-v1'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'JasperD-UGent/roberta-base-bne-complexity-classifier-v1' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

shiqing/opus-mt-en-zh-finetuned-en-to-zh failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

astremo/JAINU failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


sshleifer/bb3b-tok failed
Architectures: ['FNetForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/blenderbot/tokenization_blenderbot_fast.py", line 157, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1110, in converted
    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=ot.add_prefix_space)
TypeError: argument 'add_prefix_space': 'str' object cannot be converted to 'PyBool'

Rainiefantasy/GO1984_DistilBERT failed
Architectures: ['FNetForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Rainiefantasy/GO1984_DistilBERT'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Rainiefantasy/GO1984_DistilBERT' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

mdroth/dummy-model_R91m failed
Architectures: ['DistilBertForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/mdroth/dummy-model_R91m/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f7151-6823d6a51c1e0f9448fb73b9;d390a9bd-f132-40e8-b9fc-a6d916812152)

Repository Not Found for url: https://huggingface.co/mdroth/dummy-model_R91m/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: mdroth/dummy-model_R91m is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Nattapong/ISL-wangchanberta-NER-LST20-fineTune failed
Architectures: ['RobertaForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/camembert/tokenization_camembert_fast.py", line 127, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
CamembertConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


WillHeld/t5-small-pointer-adv-cstop_artificial failed
Architectures: ['RobertaForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/WillHeld/t5-small-pointer-adv-cstop_artificial/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f7155-7380a6233f8d89f07884843e;c8bc4f8d-fb01-42b8-9f70-0ba2d1990dc0)

Repository Not Found for url: https://huggingface.co/WillHeld/t5-small-pointer-adv-cstop_artificial/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: WillHeld/t5-small-pointer-adv-cstop_artificial is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Sebabrata/lmv2-g-paystb-999-doc-09-11 failed
Architectures: ['RobertaForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 96, in <module>
    concrete_args = tokenizer(text, return_tensors="pt")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py", line 253, in __call__
    raise ValueError(
ValueError: Words must be of type `List[str]` (single pretokenized example), or `List[List[str]]` (batch of pretokenized examples).

InfoCoV/Senti-Cro-CoV-cseBERT failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'InfoCoV/Senti-Cro-CoV-cseBERT'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'InfoCoV/Senti-Cro-CoV-cseBERT' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

masakhane/m2m100_418M_swa_en_rel_ft failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1181, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 966, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

anahitapld/DABert failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'anahitapld/DABert'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'anahitapld/DABert' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

ammarpl/t5-small-finetuned-xsum failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

juancopi81/mt5-small-finetuned-amazon-en-es failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

KoboldAI/GPT-J-6B-Skein failed
Architectures: ['GPTJForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 109, in <module>
    after_trace = traced_gm(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 662, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 281, in __call__
    raise e
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 271, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "<eval_with_key>.3453", line 59, in forward
    getitem_9 = getattr_8[-1];  getattr_8 = None
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_proxy.py", line 431, in impl
    return tracer.create_proxy('call_function', target, args, kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 444, in create_proxy
    value_unwrapped = self.run_target(kind, target, args_unwrapped, kwargs_unwrapped)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 374, in run_target
    result = run(kind, target, args, kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 347, in run
    return OperatorPatcherContext.patch_run(fn, *args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 291, in patch_run
    assert OperatorPatcherContext.ctx_tracer is not None
AssertionError

bthomas/article2KW_test2.0c_barthez-orangesum-title_finetuned_for_mlm_77153 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'bthomas/article2KW_test2.0c_barthez-orangesum-title_finetuned_for_mlm_77153'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bthomas/article2KW_test2.0c_barthez-orangesum-title_finetuned_for_mlm_77153' is the correct path to a directory containing all relevant files for a BarthezTokenizerFast tokenizer.

unicamp-dl/mt5-13b-mmarco-100k failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


kimy1119/GCU_T5_1 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/kimy1119/GCU_T5_1/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-656f71e6-76160d5c607dd8c07040818b;12fc12e6-02c3-4a03-b924-ec37fb5af471)

Cannot access gated repo for url https://huggingface.co/kimy1119/GCU_T5_1/resolve/main/tokenizer_config.json.
Repo model kimy1119/GCU_T5_1 is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 445, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/kimy1119/GCU_T5_1 and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.

allenai/wmt16-en-de-dist-12-1 failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/fsmt/tokenization_fsmt.py", line 201, in __init__
    import sacremoses
ModuleNotFoundError: No module named 'sacremoses'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 789, in from_pretrained
    return tokenizer_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/fsmt/tokenization_fsmt.py", line 203, in __init__
    raise ImportError(
ImportError: You need to install sacremoses to use XLMTokenizer. See https://pypi.org/project/sacremoses/ for installation.

WillHeld/t5-small-pointer-adv-mtop failed
Architectures: ['AlignedMT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ThomasGerald/MBARTHEZ-QG failed
Architectures: ['AlignedMT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/ThomasGerald/MBARTHEZ-QG/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-656f7200-61ecea83719d23180193d699;97665114-ea92-43c2-8fe2-af70e4bf3fdc)

Cannot access gated repo for url https://huggingface.co/ThomasGerald/MBARTHEZ-QG/resolve/main/tokenizer_config.json.
Repo model ThomasGerald/MBARTHEZ-QG is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 445, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/ThomasGerald/MBARTHEZ-QG and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.

Zaib/Vulnerability-detection failed
Architectures: ['XLMRobertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2056, in _from_pretrained
    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/roberta/tokenization_roberta.py", line 221, in __init__
    with open(vocab_file, encoding="utf-8") as vocab_handle:
TypeError: expected str, bytes or os.PathLike object, not NoneType

tner/xlm-roberta-base-panx-dataset-ar failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


north/t5_small_NCC_lm failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

mrm8488/byt5-small-tweet-hate-detection failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

it5/mt5-small-news-summarization failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

thaonh/vietnews-summarization failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Helsinki-NLP/opus-mt-fi-sw failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Culmenus/opus-mt-de-is-finetuned-de-to-is failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

countrysideid/opus-mt-en-zh-chk1 failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 789, in from_pretrained
    return tokenizer_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'countrysideid/opus-mt-en-zh-chk1'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'countrysideid/opus-mt-en-zh-chk1' is the correct path to a directory containing all relevant files for a MarianTokenizer tokenizer.

N8Daawg/chat_bot failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/N8Daawg/chat_bot/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f7243-13c7948c38569c785f0dbaec;fe60dd93-faaa-49df-8de4-0a0448d8f3ee)

Entry Not Found for url: https://huggingface.co/N8Daawg/chat_bot/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: N8Daawg/chat_bot does not appear to have a file named config.json. Checkout 'https://huggingface.co/N8Daawg/chat_bot/main' for available files.

StonyBrookNLP/teabreac-nt5-small failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


z5ying/distilgpt2-finetuned-wikitext2 failed
Architectures: ['XLMRobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'z5ying/distilgpt2-finetuned-wikitext2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'z5ying/distilgpt2-finetuned-wikitext2' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

xfbai/AMRBART-large-finetuned-AMR2.0-AMR2Text failed
Architectures: ['LongformerForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'xfbai/AMRBART-large-finetuned-AMR2.0-AMR2Text'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'xfbai/AMRBART-large-finetuned-AMR2.0-AMR2Text' is the correct path to a directory containing all relevant files for a BartTokenizerFast tokenizer.

m3/m3-experiment-roberta-base-citation-intent-add-v3-greedy failed
Architectures: ['LongformerForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-roberta-base-citation-intent-add-v3-greedy/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f7261-0b9e46cc0c342d9e6a6d65eb;d0eb943c-8684-4c80-a34b-a20107097fd1)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-roberta-base-citation-intent-add-v3-greedy/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-roberta-base-citation-intent-add-v3-greedy is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

ruiqi-zhong/t5verifier_0514 failed
Architectures: ['LongformerForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ruiqi-zhong/t5verifier_0514'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ruiqi-zhong/t5verifier_0514' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

Team-PIXEL/pixel-base-finetuned-qnli failed
Architectures: ['DistilBertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1064, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 761, in __getitem__
    raise KeyError(key)
KeyError: 'pixel'

chuaziheng/metaphor-id-bert failed
Architectures: ['DistilBertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'chuaziheng/metaphor-id-bert'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'chuaziheng/metaphor-id-bert' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-sv-fi failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

begar/distilgpt2-finetuned failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'begar/distilgpt2-finetuned'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'begar/distilgpt2-finetuned' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

muhtasham/bert-tiny-mlm-finetuned-emotion failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'muhtasham/bert-tiny-mlm-finetuned-emotion'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'muhtasham/bert-tiny-mlm-finetuned-emotion' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

spacy/da_core_news_lg failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/spacy/da_core_news_lg/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f7294-6ab2486b0cfcb9252827c406;3430806c-3774-462d-b7ce-df0deb6391f3)

Entry Not Found for url: https://huggingface.co/spacy/da_core_news_lg/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: spacy/da_core_news_lg does not appear to have a file named config.json. Checkout 'https://huggingface.co/spacy/da_core_news_lg/main' for available files.

gustavecortal/gpt-j-fr-covid-news failed
Architectures: ['LongformerForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'gustavecortal/gpt-j-fr-covid-news'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'gustavecortal/gpt-j-fr-covid-news' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

LinaR/Prediccion_titulos failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-zh-nl failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

rurupang/roberta-base-finetuned-sts-f1_ failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'rurupang/roberta-base-finetuned-sts-f1_'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'rurupang/roberta-base-finetuned-sts-f1_' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

hisaoka/t5-large_dataset_radiology_summary20221129.tsv failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-vi-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

masakhane/m2m100_418M_lug_en_rel_ft failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1181, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 966, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

PSW/t5-base-dialogsum-seed17 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

gayanin/t5-small-paraphrasing-mlm failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Lvxue/distilled-mt5-small-b1.25 failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

kompactss/JeBERT_je_ko failed
Architectures: ['EncoderDecoderModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 101, in <module>
    model = AutoModel.from_config(config, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 443, in from_config
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig'> for this kind of AutoModel: AutoModel.
Model type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, LlamaConfig, CodeGenConfig, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MistralConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SamConfig, SeamlessM4TConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VitDetConfig, VitsConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig, XLMRobertaConfig, LSGBartConfig.

SoLID/sgd-response-generator failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Hoax0930/kyoto_marian_mod_5 failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

diegozs97/finetuned-chemprot-seed-1-2000k failed
Architectures: ['DistilBertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/finetuned-chemprot-seed-1-2000k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/finetuned-chemprot-seed-1-2000k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

logoyazilim/qna_model_0010 failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/logoyazilim/qna_model_0010/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f7342-43a5cfdd65c6fe4315fc1f97;7bd6df8b-b396-4514-9673-baa29aa417cd)

Repository Not Found for url: https://huggingface.co/logoyazilim/qna_model_0010/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: logoyazilim/qna_model_0010 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

alireza7/ARMAN-SS-80-persian-base failed
Architectures: ['DistilBertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/tokenization_pegasus_fast.py", line 147, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
PegasusConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


mesolitica/finetune-noisy-translation-t5-base-bahasa-cased failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/mesolitica/finetune-noisy-translation-t5-base-bahasa-cased/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f7360-761e2880583f77dd51dbdc49;477d3632-245b-4deb-87b5-78430a4eec32)

Repository Not Found for url: https://huggingface.co/mesolitica/finetune-noisy-translation-t5-base-bahasa-cased/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: mesolitica/finetune-noisy-translation-t5-base-bahasa-cased is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

rossanez/t5-small-finetuned-de-en-256-epochs2 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ghadeermobasher/BC5CDR-chem-WLT-384-BlueBERT-latest-10 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC5CDR-chem-WLT-384-BlueBERT-latest-10'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC5CDR-chem-WLT-384-BlueBERT-latest-10' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

celinelee/answer-extraction failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'celinelee/answer-extraction'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'celinelee/answer-extraction' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

abhinavkulkarni/bigbird-roberta-base-finetuned-squad failed
Architectures: ['ElectraForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/abhinavkulkarni/bigbird-roberta-base-finetuned-squad/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f737b-3b2b22574746bfbf68d5e00d;36c05c6f-d0a8-4bc7-af73-f2ed309a84d3)

Repository Not Found for url: https://huggingface.co/abhinavkulkarni/bigbird-roberta-base-finetuned-squad/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: abhinavkulkarni/bigbird-roberta-base-finetuned-squad is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

anonsubms/msrp_ratio failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

lmqg/t5-base-tweetqa-qa failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

KoichiYasuoka/deberta-large-japanese-unidic-luw-upos failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py", line 457, in __init__
    import fugashi
ModuleNotFoundError: No module named 'fugashi'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py", line 192, in __init__
    self.word_tokenizer = MecabTokenizer(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py", line 459, in __init__
    raise error.__class__(
ModuleNotFoundError: You need to install fugashi to use MecabTokenizer. See https://pypi.org/project/fugashi/ for installation.

Helsinki-NLP/opus-mt-fi-zne failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

akreal/mbart-large-50-finetuned-portmedia-lang failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mbart50/tokenization_mbart50_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
MBart50Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


NlpHUST/t5-small-vi-summarization failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


ghadeermobasher/BC4CHEMD-WLT-256-PubMedBERT-Trial-latest-40 failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC4CHEMD-WLT-256-PubMedBERT-Trial-latest-40'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC4CHEMD-WLT-256-PubMedBERT-Trial-latest-40' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

mrm8488/t5-base-finetuned-AESLC-summarization failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


SEBIS/code_trans_t5_small_code_comment_generation_java_multitask failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


philschmid/distilbert-neuron failed
Architectures: ['DistilBertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 111, in <module>
    assert check_align(before_trace, after_trace), "Traced model does not match the original model"
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 49, in check_align
    for key in after_trace.keys():
AttributeError: 'tuple' object has no attribute 'keys'

huggingtweets/projectalpha22 failed
Architectures: ['DistilBertForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/huggingtweets/projectalpha22/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f73b4-3848dbab1acdf8d2514ef696;e05ee869-2cdc-4a93-a074-86ef5646bc13)

Repository Not Found for url: https://huggingface.co/huggingtweets/projectalpha22/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: huggingtweets/projectalpha22 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

avuhong/ESM1b_AAV2_classification failed
Architectures: ['DistilBertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 765, in from_pretrained
    raise ValueError(
ValueError: Tokenizer class ESMTokenizer does not exist or is not currently imported.

iamholmes/cuad_test failed
Architectures: ['DistilBertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'iamholmes/cuad_test'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'iamholmes/cuad_test' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

paola-md/distil-tIs-upper failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'paola-md/distil-tIs-upper'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'paola-md/distil-tIs-upper' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

mbarnig/marianNMT-tatoeba-lb-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Tritkoman/EnglishtoAncientGreek failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

google/pegasus-xsum failed
Architectures: ['PegasusForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1221, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 981, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

jwieting/paraphrastic_test failed
Architectures: ['ParagramSPModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 111, in <module>
    assert check_align(before_trace, after_trace), "Traced model does not match the original model"
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 49, in check_align
    for key in after_trace.keys():
AttributeError: 'tuple' object has no attribute 'keys'

MoritzLaurer/xtremedistil-l6-h256-mnli-fever-anli-ling-binary failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

PontifexMaximus/opus-mt-tr-en-finetuned-az-to-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

bullmount/it_nerIta_trf failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/bullmount/it_nerIta_trf/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f73e5-2a7db88b4c5f97940c776464;ca7409c0-e745-446b-a0a8-656a5dfed2af)

Entry Not Found for url: https://huggingface.co/bullmount/it_nerIta_trf/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: bullmount/it_nerIta_trf does not appear to have a file named config.json. Checkout 'https://huggingface.co/bullmount/it_nerIta_trf/main' for available files.

d4niel92/t5-reddit failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Jeevesh8/bert-base-uncased_mnli_ft_5 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

connectivity/bert_ft_qqp-30 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

mitiku/AmharicWICPostag10Tags failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

anas-awadalla/spanbert-base-finetuned-squad-r3f failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

51la5/bert-base-NER failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

Ebtihal/AraDiaBERTo_V2 failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

ghadeermobasher/BC4CHEMD-Chem-Original-PubMedBERT-384 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

Helsinki-NLP/opus-mt-pl-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Jinchen/my-awesome-model failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Jinchen/my-awesome-model'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Jinchen/my-awesome-model' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

jweb/japanese-soseki-gpt2-1b failed
Architectures: ['OPTForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Jeevesh8/std_pnt_04_feather_berts-84 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

circulus/kobart-chat-v2 failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: BartModel.forward() got an unexpected keyword argument 'token_type_ids'

joyebright/Top5-without-mixing failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/joyebright/Top5-without-mixing/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f7431-32b9b2df10a236c617fd458c;00944297-c749-4006-b45e-08e8e14c0fb6)

Entry Not Found for url: https://huggingface.co/joyebright/Top5-without-mixing/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: joyebright/Top5-without-mixing does not appear to have a file named config.json. Checkout 'https://huggingface.co/joyebright/Top5-without-mixing/main' for available files.

wintee/FilteringModelOutput failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'wintee/FilteringModelOutput'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'wintee/FilteringModelOutput' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

liujxing/distilgpt2-finetuned-wikitext2 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'liujxing/distilgpt2-finetuned-wikitext2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'liujxing/distilgpt2-finetuned-wikitext2' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

ShwetActhq/autotrain-signature_extraction-1673759350 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/ShwetActhq/autotrain-signature_extraction-1673759350/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f7432-39ce900f395354b86b523c58;01a0813c-a696-4bec-a7f0-71126c3f3089)

Repository Not Found for url: https://huggingface.co/ShwetActhq/autotrain-signature_extraction-1673759350/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: ShwetActhq/autotrain-signature_extraction-1673759350 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Chen1999/distilbert-base-uncased-finetuned-imdb failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Chen1999/distilbert-base-uncased-finetuned-imdb'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Chen1999/distilbert-base-uncased-finetuned-imdb' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

mboth/klassifizierungErzeugen failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/mboth/klassifizierungErzeugen/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f7432-4199857961a4495010d1db11;58ec32ac-27b6-4e0d-a6d7-9e7722019653)

Repository Not Found for url: https://huggingface.co/mboth/klassifizierungErzeugen/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: mboth/klassifizierungErzeugen is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

DioLiu/distilroberta-base-Shake-Taylor failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'DioLiu/distilroberta-base-Shake-Taylor'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'DioLiu/distilroberta-base-Shake-Taylor' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

zhuqing/comparison-roberta-base-uncased-netmums-feminist failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'zhuqing/comparison-roberta-base-uncased-netmums-feminist'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'zhuqing/comparison-roberta-base-uncased-netmums-feminist' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

erfangc/mt5-small-finetuned-amazon-en-es failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

connectivity/feather_berts_65 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

masakhane/m2m100_418M_fr_bbj_rel_ft failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1181, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 966, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-hy-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

vencortex/DeepFeatEcosystemAnsweringEngineXLM failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/vencortex/DeepFeatEcosystemAnsweringEngineXLM/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f746c-1a10818e61150a6f2dea3f82;5f3cfaca-b770-4731-b9c7-872fe1c5180c)

Repository Not Found for url: https://huggingface.co/vencortex/DeepFeatEcosystemAnsweringEngineXLM/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: vencortex/DeepFeatEcosystemAnsweringEngineXLM is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Jeevesh8/multiberts-seed_0-step_800k_mnli_ft_16 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

negfir/bert_uncased_L-8_H-128_A-2wiki103 failed
Architectures: ['GPTNeoForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'negfir/bert_uncased_L-8_H-128_A-2wiki103'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'negfir/bert_uncased_L-8_H-128_A-2wiki103' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

ghadeermobasher/BC4CHEMD-WLT-256-SciBERT-Trial-latest-80 failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC4CHEMD-WLT-256-SciBERT-Trial-latest-80'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC4CHEMD-WLT-256-SciBERT-Trial-latest-80' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

aakorolyova/reported_outcome_extraction failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

ChronicTronic/distilgpt2_finetuned_hacks failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ChronicTronic/distilgpt2_finetuned_hacks'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ChronicTronic/distilgpt2_finetuned_hacks' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

MickyMike/2-GPT2SP-titanium failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'MickyMike/2-GPT2SP-titanium'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'MickyMike/2-GPT2SP-titanium' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

Jeevesh8/multiberts-seed_0-step_1400k_mnli_ft_36 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

connectivity/bert_ft_qqp-80 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

mfigurski80/relation-distilbert-em failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'mfigurski80/relation-distilbert-em'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'mfigurski80/relation-distilbert-em' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

EslamAhmed/customer_data_tuned_trial_1 failed
Architectures: ['DistilBertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: DistilBertModel.forward() got an unexpected keyword argument 'token_type_ids'

Jeevesh8/lecun_feather_berts-51 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

mwp/keybert-lm-finetuned-stage2 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/mwp/keybert-lm-finetuned-stage2/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f74a4-2729061f39c80ea71da5bcae;dea11276-6089-41de-9cbd-0e44f55a97dd)

Repository Not Found for url: https://huggingface.co/mwp/keybert-lm-finetuned-stage2/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: mwp/keybert-lm-finetuned-stage2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

kangnichaluo/mnli-3 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

masakhane/byt5_lug_en_news failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Lvxue/distilled-mt5-small-b10 failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

huggingface/gpt2-wikitext2 failed
Architectures: ['XLMRobertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'huggingface/gpt2-wikitext2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'huggingface/gpt2-wikitext2' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

google/t5-efficient-base-nl40 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

mateusqc/ner-bert-base-cased-pt-lenerbr failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

Jeevesh8/clipped_warmed_wd0_pnt_01_seq_len_128_bert-base-uncased_mnli_ft_0 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

Geotrend/bert-base-en-fr-cased failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

jgammack/MTL-bert-base-uncased-ww-squad failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

redwoodresearch/classifier_12aug_50k_labels failed
Architectures: ['DebertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 31, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 974, in new_func
    encoder_outputs = self.encoder(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 470, in new_func
    hidden_states = layer_module(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 383, in new_func
    attention_output = self.attention(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 316, in new_func
    self_output = self.self(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 661, in new_func
    rel_att = self.disentangled_att_bias(query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 710, in new_func
    c2p_att = torch.gather(c2p_att, dim=-1, index=c2p_dynamic_expand(c2p_pos, query_layer, relative_pos))
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
RuntimeError: c2p_dynamic_expand() Expected a value of type 'Tensor (inferred)' for argument 'c2p_pos' but instead found type 'ConcreteProxy'.
Inferred 'c2p_pos' to be of type 'Tensor' because it was not annotated with an explicit type.
Position: 0
Value: ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]]))
Declaration: c2p_dynamic_expand(Tensor c2p_pos, Tensor query_layer, Tensor relative_pos) -> Tensor
Cast error details: Unable to cast ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]])) to Tensor

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 33, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

MickyMike/7-GPT2SP-talendesb failed
Architectures: ['DebertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'MickyMike/7-GPT2SP-talendesb'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'MickyMike/7-GPT2SP-talendesb' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

ralcanta/do_nothing_bert failed
Architectures: ['AlbertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 789, in from_pretrained
    return tokenizer_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ralcanta/do_nothing_bert'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ralcanta/do_nothing_bert' is the correct path to a directory containing all relevant files for a BertGenerationTokenizer tokenizer.

joshanashakya/codebert_sourcecode_nmt_ja2pn_100E_5e-05LR failed
Architectures: ['EncoderDecoderModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 101, in <module>
    model = AutoModel.from_config(config, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 443, in from_config
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig'> for this kind of AutoModel: AutoModel.
Model type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, LlamaConfig, CodeGenConfig, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MistralConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SamConfig, SeamlessM4TConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VitDetConfig, VitsConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig, XLMRobertaConfig, LSGBartConfig, BertConfig.

dbernsohn/t5_measurement_time failed
Architectures: ['EncoderDecoderModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


sinhala-nlp/mbert-hasoc-hi-sold-si failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

Neulvo/marian-finetuned-kde4-en-to-fr-accelerate failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

connectivity/feather_berts_49 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

anas-awadalla/spanbert-base-cased-few-shot-k-512-finetuned-squad-seed-2 failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

royeis/T5-FlowNLG-Planner failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Akihiro2/akihiro2-finetuned-kde4-en-to-jp-accelerate failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

srir4m/fairguest-bert-opt-all failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/srir4m/fairguest-bert-opt-all/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f7531-7b9d0da806a8715e08c10453;ffb8ad4c-ac80-48b2-9a96-0a9198b3b075)

Repository Not Found for url: https://huggingface.co/srir4m/fairguest-bert-opt-all/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: srir4m/fairguest-bert-opt-all is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

anas-awadalla/t5-base-finetuned-squad-infilling-lr-5e-5 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

emrecan/bert-base-turkish-cased-multinli_tr failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

junaidamk/mlner-mlwptok-muril failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

teacookies/autotrain-15112022-cert3-2101567677 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

Helsinki-NLP/opus-mt-fi-tr failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

kornosk/bert-election2020-twitter-stance-trump failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

qazx0104/x-x failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/qazx0104/x-x/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f7555-69634be562b16d5c20d64538;8b37bf6b-e87e-4b4c-b0bc-589788d827c7)

Repository Not Found for url: https://huggingface.co/qazx0104/x-x/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: qazx0104/x-x is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Helsinki-NLP/opus-mt-guw-fr failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

DOOGLAK/01_SR_100v7_NER_Model_3Epochs_AUGMENTED failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

Maaly/body-site failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

autoevaluate/translation failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Theivaprakasham/layoutlmv2-finetuned-sroie_mod failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Theivaprakasham/layoutlmv2-finetuned-sroie_mod'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Theivaprakasham/layoutlmv2-finetuned-sroie_mod' is the correct path to a directory containing all relevant files for a LayoutLMv2TokenizerFast tokenizer.

Lvxue/distilled-mt5-small-hiddentest failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

yoshitomo-matsubara/bert-base-uncased-sst2_from_bert-large-uncased-sst2 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

stanfordnlp/stanza-hy failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/stanfordnlp/stanza-hy/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f75f5-26b7e69c5429b48c1a2c5b7a;bf3e7054-0998-4088-90b8-f2d8f57c3b52)

Entry Not Found for url: https://huggingface.co/stanfordnlp/stanza-hy/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: stanfordnlp/stanza-hy does not appear to have a file named config.json. Checkout 'https://huggingface.co/stanfordnlp/stanza-hy/main' for available files.

teacookies/autotrain-171022-update_label2-1788462049 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

svsokol/opus-mt-ru-en-finetuned-en-to-ru failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Jeevesh8/lp1e-3_ft_corr_init_shuff_clipped_warmed_wd0_pnt_01_seq_len_128_bert-base-uncased_mnli_ft_70 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

Jeevesh8/clipped_warmed_wd0_pnt_01_seq_len_128_bert-base-uncased_mnli_ft_13 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

stanfordnlp/stanza-ug failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/stanfordnlp/stanza-ug/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f7607-1fa67948621a669b47d8896b;0bd1d706-6104-4296-8c66-0aabbc530d21)

Entry Not Found for url: https://huggingface.co/stanfordnlp/stanza-ug/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: stanfordnlp/stanza-ug does not appear to have a file named config.json. Checkout 'https://huggingface.co/stanfordnlp/stanza-ug/main' for available files.

mobashgr/BC2GM-WLT-256-PubMedBERT-latest-INS failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

axiomepic/hub_model_id failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/axiomepic/hub_model_id/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f760a-3f11cd9572b50ccd52fd3176;ffd62ca3-5a14-41df-8e92-1319bba4f346)

Repository Not Found for url: https://huggingface.co/axiomepic/hub_model_id/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: axiomepic/hub_model_id is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

2020Austin/rockbook-finetuned-legalbert failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

ucabqfe/bigBird_PER_bieo failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ucabqfe/bigBird_PER_bieo'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ucabqfe/bigBird_PER_bieo' is the correct path to a directory containing all relevant files for a BigBirdTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-pis-sv failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

masakhane/afrimt5_lug_en_news failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

gokceuludogan/t2t-adeX-prompt failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


ctheodoris/Geneformer failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ctheodoris/Geneformer'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ctheodoris/Geneformer' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Jeevesh8/6ep_bert_ft_cola-1 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

connectivity/cola_6ep_ft-7 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

paola-md/recipe-distilbert-upper-Is failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'paola-md/recipe-distilbert-upper-Is'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'paola-md/recipe-distilbert-upper-Is' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

HYM/bert-base-chinese-ws-finetuned-ner failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

edgertej/poebert-checkpoint-finetuned-poetry-foundation failed
Architectures: ['MPNetForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/edgertej/poebert-checkpoint-finetuned-poetry-foundation/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f7652-09bd343d1eea8ab5792c06dd;f551f5ab-2b02-4565-9f11-4b07a4fb8387)

Repository Not Found for url: https://huggingface.co/edgertej/poebert-checkpoint-finetuned-poetry-foundation/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: edgertej/poebert-checkpoint-finetuned-poetry-foundation is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

m3/m3-experiment-albert-base-v2-sciie-eda-4 failed
Architectures: ['MBartForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-sciie-eda-4/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f7656-4fb654b51514c85e5b40ba60;d2364009-30c6-409c-a98a-1a8cc2484c58)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-sciie-eda-4/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-albert-base-v2-sciie-eda-4 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Helsinki-NLP/opus-mt-dra-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ItcastAI/bert_finetunning_test failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

MartinoMensio/racism-models-regression-w-m-vote-epoch-3 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'MartinoMensio/racism-models-regression-w-m-vote-epoch-3'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'MartinoMensio/racism-models-regression-w-m-vote-epoch-3' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

seduerr/soccer failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'seduerr/soccer'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'seduerr/soccer' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

BogdanKuloren/checkpoint-10500-finetuned-ner failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/BogdanKuloren/checkpoint-10500-finetuned-ner/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f766a-5f9b9eec0de31c745b770fde;51457efe-c91a-46e1-9446-8bb34024d078)

Repository Not Found for url: https://huggingface.co/BogdanKuloren/checkpoint-10500-finetuned-ner/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: BogdanKuloren/checkpoint-10500-finetuned-ner is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

kevinr/Confidence-bert-base-uncased-Loss_CrossEntropy-Bin_01-2345 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

ghadeermobasher/linnaeus-WLT-512 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/linnaeus-WLT-512'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/linnaeus-WLT-512' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

DeskDown/MarianMix_en-zh_to_vi-ms-hi-ja failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Tomohiro/RealMedNLP_CR_JA failed
Architectures: ['BartForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Tomohiro/RealMedNLP_CR_JA'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Tomohiro/RealMedNLP_CR_JA' is the correct path to a directory containing all relevant files for a BertJapaneseTokenizer tokenizer.

Atharvgarg/bert-small2bert-small-finetuned-cnn_daily_mail-summarization-finetuned-bbc-news-old failed
Architectures: ['EncoderDecoderModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 101, in <module>
    model = AutoModel.from_config(config, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 443, in from_config
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig'> for this kind of AutoModel: AutoModel.
Model type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, LlamaConfig, CodeGenConfig, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MistralConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SamConfig, SeamlessM4TConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VitDetConfig, VitsConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig, XLMRobertaConfig, LSGBartConfig, BertConfig, LSGCamembertConfig.

MoritzLaurer/MiniLM-L6-mnli failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

google/t5-efficient-tiny failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

kabelomalapane/en_tn_ukuxhumana_model2 failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

textattack/xlnet-base-cased-rotten-tomatoes failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlnet/tokenization_xlnet_fast.py", line 150, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLNetConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


SharpAI/mal-tls-bert-base-w1q8 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

harish/PT-v3-dev-test-all-PreTrain-e5-all failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

diegozs97/chemprot-seed-4-2000k failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/chemprot-seed-4-2000k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/chemprot-seed-4-2000k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

JoonJoon/bert-base-cased-wikitext2 failed
Architectures: ['MobileBertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'JoonJoon/bert-base-cased-wikitext2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'JoonJoon/bert-base-cased-wikitext2' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

planhanasan/hana-model failed
Architectures: ['MobileBertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'planhanasan/hana-model'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'planhanasan/hana-model' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Splend1dchan/bert-base-uncased-slue-goldtrascription-e3-lr1e-4 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

hsohn3/cchs-bert-visit-uncased-wordlevel-block512-batch4-ep60 failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

clam004/emerg-intent-consistent-good-gpt2-xl-v1 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/clam004/emerg-intent-consistent-good-gpt2-xl-v1/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f76bf-4da0ec065fdf16543838cff5;33830225-a471-47ad-abff-4486cc5f31af)

Repository Not Found for url: https://huggingface.co/clam004/emerg-intent-consistent-good-gpt2-xl-v1/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: clam004/emerg-intent-consistent-good-gpt2-xl-v1 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

arincon/mbart-es-paraphrase failed
Architectures: ['LongformerForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/arincon/mbart-es-paraphrase/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f76d1-7c4a0aea4043a5362d086938;c7cf223d-8427-4533-8825-db348eb1cd3a)

Repository Not Found for url: https://huggingface.co/arincon/mbart-es-paraphrase/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: arincon/mbart-es-paraphrase is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

ltrctelugu/tree_topconstituents failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

Jeevesh8/warmed_wd0_pnt_01_seq_len_128_bert-base-uncased_mnli_ft_5 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

RonEliav/QA_discourse_v2 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

redwoodresearch/classifier-18aug-train failed
Architectures: ['DebertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 31, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 974, in new_func
    encoder_outputs = self.encoder(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 470, in new_func
    hidden_states = layer_module(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 383, in new_func
    attention_output = self.attention(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 316, in new_func
    self_output = self.self(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 661, in new_func
    rel_att = self.disentangled_att_bias(query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 710, in new_func
    c2p_att = torch.gather(c2p_att, dim=-1, index=c2p_dynamic_expand(c2p_pos, query_layer, relative_pos))
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
RuntimeError: c2p_dynamic_expand() Expected a value of type 'Tensor (inferred)' for argument 'c2p_pos' but instead found type 'ConcreteProxy'.
Inferred 'c2p_pos' to be of type 'Tensor' because it was not annotated with an explicit type.
Position: 0
Value: ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]]))
Declaration: c2p_dynamic_expand(Tensor c2p_pos, Tensor query_layer, Tensor relative_pos) -> Tensor
Cast error details: Unable to cast ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]])) to Tensor

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 33, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

peterhsu/bert-finetuned-ner-accelerate failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

thetatez/distilbert-rater failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'thetatez/distilbert-rater'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'thetatez/distilbert-rater' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

Xuan-Rui/ipet-1000-all failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

zhuqing/roberta-large-uncased-exp3-parent failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'zhuqing/roberta-large-uncased-exp3-parent'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'zhuqing/roberta-large-uncased-exp3-parent' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

jackh1995/albert-base failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

Rookie-06/distilbert-base-uncased-finetuned-imdb failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Rookie-06/distilbert-base-uncased-finetuned-imdb'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Rookie-06/distilbert-base-uncased-finetuned-imdb' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

bthomas/tuto-bert-finetuned-ner failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

Mozart-coder/BERT_winter-6_tokenized failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

baykenney/bert-large-gpt2detector-topp92 failed
Architectures: ['ElectraForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/baykenney/bert-large-gpt2detector-topp92/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f76fc-230fdf811c5de98c1d0f5a53;c7ce5180-403c-4812-ae55-a2c78d5f5ef2)

Repository Not Found for url: https://huggingface.co/baykenney/bert-large-gpt2detector-topp92/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: baykenney/bert-large-gpt2detector-topp92 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

StivenLancheros/spanberta-base-cased-ner-conll02-finetuned-ner failed
Architectures: ['ElectraForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/StivenLancheros/spanberta-base-cased-ner-conll02-finetuned-ner/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f76fc-64a9542733f39c602507a13b;711e8f69-d4bd-40e5-8aaa-7a1d400b3a82)

Repository Not Found for url: https://huggingface.co/StivenLancheros/spanberta-base-cased-ner-conll02-finetuned-ner/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: StivenLancheros/spanberta-base-cased-ner-conll02-finetuned-ner is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Helsinki-NLP/opus-mt-he-it failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

fxmarty/20220911-h13m58s51_conll2003_distilbert_quantization failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/fxmarty/20220911-h13m58s51_conll2003_distilbert_quantization/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f7702-7163c9d32305e9451885e665;8230608d-4ead-4b95-9f3d-d34e40b37586)

Entry Not Found for url: https://huggingface.co/fxmarty/20220911-h13m58s51_conll2003_distilbert_quantization/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: fxmarty/20220911-h13m58s51_conll2003_distilbert_quantization does not appear to have a file named config.json. Checkout 'https://huggingface.co/fxmarty/20220911-h13m58s51_conll2003_distilbert_quantization/main' for available files.

avichr/ar_hd failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

sismetanin/xlm_roberta_base-ru-sentiment-liniscrowd failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Hate-speech-CNERG/marathi-codemixed-abusive-MuRIL failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

KingCodeSquid/Octavian2 failed
Architectures: ['RobertaForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/KingCodeSquid/Octavian2/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f7719-56e0443c63c1823800bc1521;496a406b-26bd-4fe0-a2af-1be5046a5ef9)

Entry Not Found for url: https://huggingface.co/KingCodeSquid/Octavian2/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: KingCodeSquid/Octavian2 does not appear to have a file named config.json. Checkout 'https://huggingface.co/KingCodeSquid/Octavian2/main' for available files.

MariamD/my-t5-qa-legal failed
Architectures: ['RobertaForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://cdn-lfs.huggingface.co/MariamD/my-t5-qa-legal/8edc79cc6c67f9b7b41e6d38a3e5f314b3ca52f83d45833c1ffcc1fdec6832c5?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27config.json%3B+filename%3D%22config.json%22%3B&response-content-type=application%2Fjson&Expires=1702063001&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMjA2MzAwMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9NYXJpYW1EL215LXQ1LXFhLWxlZ2FsLzhlZGM3OWNjNmM2N2Y5YjdiNDFlNmQzOGEzZTVmMzE0YjNjYTUyZjgzZDQ1ODMzYzFmZmNjMWZkZWM2ODMyYzU~cmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=NJ54brol~ntTkC9mPNVgSjsMhaFaUTTV8Dfi6FtZTFMVnNqEHtDKTVAG2RircEkbZN6brgoGeGU7IzQT4bdyGI9p3qnyEtWU-DTKAlBDtVBtOEe8bBstAkxRr-0YDVE-nPDgdrTeuv68zbbyZpscGbP680ol8daAyyNbPneT0~sU9rG2eZlmOTjeBVV0tS8KE47cF1meRrSUTXAQWBkvRS45MzIP8zT4C3NihVZmikUiLAURR853DeaeL5xROlcsEWePizUVqbDRGxvzZxIvpMck7sPNScafZcR5uZHxWgmTQVql1wphbnuIBVHRQws1hRhpw-2utiYawqu7RLjjwA__&Key-Pair-Id=KVTP0A1DKRTAX

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1461, in hf_hub_download
    http_get(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 468, in http_get
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 330, in hf_raise_for_status
    raise HfHubHTTPError(str(e), response=response) from e
huggingface_hub.utils._errors.HfHubHTTPError: 403 Client Error: Forbidden for url: https://cdn-lfs.huggingface.co/MariamD/my-t5-qa-legal/8edc79cc6c67f9b7b41e6d38a3e5f314b3ca52f83d45833c1ffcc1fdec6832c5?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27config.json%3B+filename%3D%22config.json%22%3B&response-content-type=application%2Fjson&Expires=1702063001&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMjA2MzAwMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9NYXJpYW1EL215LXQ1LXFhLWxlZ2FsLzhlZGM3OWNjNmM2N2Y5YjdiNDFlNmQzOGEzZTVmMzE0YjNjYTUyZjgzZDQ1ODMzYzFmZmNjMWZkZWM2ODMyYzU~cmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=NJ54brol~ntTkC9mPNVgSjsMhaFaUTTV8Dfi6FtZTFMVnNqEHtDKTVAG2RircEkbZN6brgoGeGU7IzQT4bdyGI9p3qnyEtWU-DTKAlBDtVBtOEe8bBstAkxRr-0YDVE-nPDgdrTeuv68zbbyZpscGbP680ol8daAyyNbPneT0~sU9rG2eZlmOTjeBVV0tS8KE47cF1meRrSUTXAQWBkvRS45MzIP8zT4C3NihVZmikUiLAURR853DeaeL5xROlcsEWePizUVqbDRGxvzZxIvpMck7sPNScafZcR5uZHxWgmTQVql1wphbnuIBVHRQws1hRhpw-2utiYawqu7RLjjwA__&Key-Pair-Id=KVTP0A1DKRTAX

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 493, in cached_file
    raise EnvironmentError(f"There was a specific connection error when trying to load {path_or_repo_id}:\n{err}")
OSError: There was a specific connection error when trying to load MariamD/my-t5-qa-legal:
403 Client Error: Forbidden for url: https://cdn-lfs.huggingface.co/MariamD/my-t5-qa-legal/8edc79cc6c67f9b7b41e6d38a3e5f314b3ca52f83d45833c1ffcc1fdec6832c5?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27config.json%3B+filename%3D%22config.json%22%3B&response-content-type=application%2Fjson&Expires=1702063001&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMjA2MzAwMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9NYXJpYW1EL215LXQ1LXFhLWxlZ2FsLzhlZGM3OWNjNmM2N2Y5YjdiNDFlNmQzOGEzZTVmMzE0YjNjYTUyZjgzZDQ1ODMzYzFmZmNjMWZkZWM2ODMyYzU~cmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=NJ54brol~ntTkC9mPNVgSjsMhaFaUTTV8Dfi6FtZTFMVnNqEHtDKTVAG2RircEkbZN6brgoGeGU7IzQT4bdyGI9p3qnyEtWU-DTKAlBDtVBtOEe8bBstAkxRr-0YDVE-nPDgdrTeuv68zbbyZpscGbP680ol8daAyyNbPneT0~sU9rG2eZlmOTjeBVV0tS8KE47cF1meRrSUTXAQWBkvRS45MzIP8zT4C3NihVZmikUiLAURR853DeaeL5xROlcsEWePizUVqbDRGxvzZxIvpMck7sPNScafZcR5uZHxWgmTQVql1wphbnuIBVHRQws1hRhpw-2utiYawqu7RLjjwA__&Key-Pair-Id=KVTP0A1DKRTAX

codestylist/combined_code_style_transformer failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

diegozs97/chemprot-seed-0-400k failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/chemprot-seed-0-400k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/chemprot-seed-0-400k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

lyx10290516/model_cntest failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

m3/m3-experiment-albert-base-v2-tweet-eval-hate-word-swapping-embedding-0 failed
Architectures: ['MPNetModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-tweet-eval-hate-word-swapping-embedding-0/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f771f-126a4f125838ef434c48b13c;39d768f8-079b-4398-83ab-63e63c405455)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-tweet-eval-hate-word-swapping-embedding-0/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-albert-base-v2-tweet-eval-hate-word-swapping-embedding-0 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Lvxue/distilled-mt5-small-0.005-0.25 failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

StonyBrookNLP/preasm-large-iirc-gold failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Helsinki-NLP/opus-mt-inc-inc failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

nnn/shangpin-pre-training failed
Architectures: ['NeZhaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

DOOGLAK/05_LWTR_100v2_NER_Model_3Epochs_AUGMENTED failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

AdapterHub/bert-base-uncased-pf-squad_v2 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/AdapterHub/bert-base-uncased-pf-squad_v2/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f773e-001f43f229b45765757a4ca7;6a98832f-6b84-463f-9403-40e21aa28d2d)

Entry Not Found for url: https://huggingface.co/AdapterHub/bert-base-uncased-pf-squad_v2/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: AdapterHub/bert-base-uncased-pf-squad_v2 does not appear to have a file named config.json. Checkout 'https://huggingface.co/AdapterHub/bert-base-uncased-pf-squad_v2/main' for available files.

spneshaei/vira_outputs_f_5_score_classifier_binned_2_categories failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

ArnavL/yelp-pretrained failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

josetapia/HyBertHeuristic-model-19 failed
Architectures: ['RobertaModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'josetapia/HyBertHeuristic-model-19'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'josetapia/HyBertHeuristic-model-19' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

johnpaulbin/cvai-bert-asag failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

OttoZastrow/bart_10k_bva failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/OttoZastrow/bart_10k_bva/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f7754-52aefec038107e7d32008203;96642e05-3094-48f1-a955-38389850a3f3)

Repository Not Found for url: https://huggingface.co/OttoZastrow/bart_10k_bva/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: OttoZastrow/bart_10k_bva is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

amanm27/bert-base-uncased-sports failed
Architectures: ['DistilBertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'amanm27/bert-base-uncased-sports'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'amanm27/bert-base-uncased-sports' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

rahul77/pegasus-large-finetuned-rahulv-summarization-pegasus-model failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/rahul77/pegasus-large-finetuned-rahulv-summarization-pegasus-model/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f77b0-689691121b1baa3f5fa42861;bed5fc68-f745-4f9d-b19b-268c120ed098)

Repository Not Found for url: https://huggingface.co/rahul77/pegasus-large-finetuned-rahulv-summarization-pegasus-model/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: rahul77/pegasus-large-finetuned-rahulv-summarization-pegasus-model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

ghadeermobasher/NCBI-disease-WLT-128-BlueBERT-50 failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/NCBI-disease-WLT-128-BlueBERT-50'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/NCBI-disease-WLT-128-BlueBERT-50' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

it5/mt5-small-informal-to-formal failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

muhtasham/bert-small-finetuned-wnut17-ner-longer10 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

Geotrend/bert-base-en-cased failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

diegozs97/chemprot-seed-3-700k failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/chemprot-seed-3-700k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/chemprot-seed-3-700k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Nithiwat/soda-berta failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Nithiwat/soda-berta'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Nithiwat/soda-berta' is the correct path to a directory containing all relevant files for a CamembertTokenizerFast tokenizer.

Jeevesh8/6ep_bert_ft_cola-3 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

asafaya/bert-base-arabic failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

Jellywibble/Dalio-Synthetic-Pretrain-Finetuned failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Jellywibble/Dalio-Synthetic-Pretrain-Finetuned'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Jellywibble/Dalio-Synthetic-Pretrain-Finetuned' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

mimi/Waynehills_NLP_muti failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 118, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Jeevesh8/corr_init_shuff_clipped_warmed_wd0_pnt_01_seq_len_128_bert-base-uncased_mnli_ft_43 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

danielbispov/t5-small-finetuned-fi-to-en failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

imosnoi/ro_bon_all_v4 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'imosnoi/ro_bon_all_v4'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'imosnoi/ro_bon_all_v4' is the correct path to a directory containing all relevant files for a LayoutLMTokenizerFast tokenizer.

pile-of-law/legalbert-large-1.7M-1 failed
Architectures: None
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

mboth/klassifizierungKaelteVersorgen failed
Architectures: ['RobertaForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/mboth/klassifizierungKaelteVersorgen/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f77fc-37c8fd7e2676e1f73f3a80b0;67515a00-3708-400a-b2f7-27178f5b32a5)

Repository Not Found for url: https://huggingface.co/mboth/klassifizierungKaelteVersorgen/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: mboth/klassifizierungKaelteVersorgen is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Jeevesh8/lecun_feather_berts-93 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

Rocketknight1/test-bert-finetuned-ner failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

spentaur/yelp failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'spentaur/yelp'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'spentaur/yelp' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

DOOGLAK/01_SIS_50v5_NER_Model_3Epochs_AUGMENTED failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

mesolitica/t5-base-bahasa-cased failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


tner/xlm-roberta-base-uncased-fin failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


textattack/albert-base-v2-ag-news failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/albert/tokenization_albert_fast.py", line 148, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
AlbertConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


erst/xlm-roberta-base-finetuned-db07 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


mezes/my_awsome_model_epoch_3 failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'mezes/my_awsome_model_epoch_3'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'mezes/my_awsome_model_epoch_3' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

newsha/PQuAD failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

Blaise-g/longt5_tglobal_large_scitldr failed
Architectures: ['LongT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/longt5/modeling_longt5.py", line 1877, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/longt5/modeling_longt5.py", line 1435, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

google/tapas-base-finetuned-wikisql-supervised failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 96, in <module>
    concrete_args = tokenizer(text, return_tensors="pt")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/tapas/tokenization_tapas.py", line 643, in __call__
    assert isinstance(table, pd.DataFrame), "Table must be of type pd.DataFrame"
AssertionError: Table must be of type pd.DataFrame

ghadeermobasher/BioRED-Chem-WLT-320-SciBERT failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BioRED-Chem-WLT-320-SciBERT'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BioRED-Chem-WLT-320-SciBERT' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-fi-xh failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

l3cube-pune/hi-bert-embed failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

ncduy/bert-base-cased-wikitext2 failed
Architectures: ['DistilBertModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ncduy/bert-base-cased-wikitext2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ncduy/bert-base-cased-wikitext2' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

google/switch-c-2048 failed
Architectures: None
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 1437, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 966, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

diegozs97/finetuned-sciie-seed-0-2000k failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/finetuned-sciie-seed-0-2000k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/finetuned-sciie-seed-0-2000k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

jaimin/T5-Large-ONNX failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


paola-md/recipe-lr8e06-wd0.01-bs32 failed
Architectures: ['GectorbertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'paola-md/recipe-lr8e06-wd0.01-bs32'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'paola-md/recipe-lr8e06-wd0.01-bs32' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

uer/chinese_roberta_L-8_H-128 failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

tner/xlm-roberta-base-uncased-panx-dataset-en failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


RecordedFuture/Swedish-Sentiment-Fear failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

Geotrend/bert-base-en-fr-nl-ru-ar-cased failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

m3/m3-experiment-albert-base-v2-tweet-eval-hate-add-v3-greedy failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-tweet-eval-hate-add-v3-greedy/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f78f5-6a86916c6c952cb146de9455;94cc027f-50d0-41e9-be2a-89b768ab80ec)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-tweet-eval-hate-add-v3-greedy/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-albert-base-v2-tweet-eval-hate-add-v3-greedy is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

m3/m3-experiment-roberta-base-citation-intent-add-v5 failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-roberta-base-citation-intent-add-v5/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f78f5-6b4ff94a67f1b7d61f95c36c;0d6e7c21-7b46-48c9-828b-6cf128a13dfc)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-roberta-base-citation-intent-add-v5/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-roberta-base-citation-intent-add-v5 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

srir4m/setfit-multilabel-dur-test failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/srir4m/setfit-multilabel-dur-test/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f78f8-732a4f3b1111500e28b915f4;1b0f44e6-69ad-45d1-8ec7-0e283556aae9)

Repository Not Found for url: https://huggingface.co/srir4m/setfit-multilabel-dur-test/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: srir4m/setfit-multilabel-dur-test is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

eunsour/en-ko-transliterator failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


DoyyingFace/bert-tweets-semeval-unclean failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

m3/m3-experiment-roberta-base-tweet-eval-irony-word-swapping-embedding-3 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-roberta-base-tweet-eval-irony-word-swapping-embedding-3/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f7915-3026e0042e014f110b7a81e8;aac7c33e-d25c-4e34-a69f-e6438b9154b2)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-roberta-base-tweet-eval-irony-word-swapping-embedding-3/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-roberta-base-tweet-eval-irony-word-swapping-embedding-3 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

DMetaSoul/sbert-chinese-general-v2-distill failed
Architectures: ['BertModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

Jeevesh8/multiberts-seed_0-step_1400k_mnli_ft_24 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

mojtaba767/bert-base-parsbert-uncased-finetuned-imdb-m-test failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

rajistics/informal_formal_style_transfer failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Jeevesh8/lp1e-3_ft_corr_init_shuff_clipped_warmed_wd0_pnt_01_seq_len_128_bert-base-uncased_mnli_ft_16 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

simonschoe/call2vec failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/simonschoe/call2vec/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f7935-1244471d7d78cbba19c62c56;286f59a6-a0a9-4429-9f78-e3c60b4aee26)

Entry Not Found for url: https://huggingface.co/simonschoe/call2vec/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: simonschoe/call2vec does not appear to have a file named config.json. Checkout 'https://huggingface.co/simonschoe/call2vec/main' for available files.

DOOGLAK/05_SIS_50v3_NER_Model_3Epochs_AUGMENTED failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

allenai/macaw-11b failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


GKLMIP/electra-myanmar-base-uncased failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 96, in <module>
    concrete_args = tokenizer(text, return_tensors="pt")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2798, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2904, in _call_one
    return self.encode_plus(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2977, in encode_plus
    return self._encode_plus(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 576, in _encode_plus
    batched_output = self._batch_encode_plus(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 504, in _batch_encode_plus
    encodings = self._tokenizer.encode_batch(
Exception: WordPiece error: Missing [UNK] token from the vocabulary

youzanai/bert-product-comment-chinese failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

sagorsarker/codeswitch-hineng-lid-lince failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

MorrisPark/twc5-bart-pretrain failed
Architectures: ['DistilBertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2056, in _from_pretrained
    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bart/tokenization_bart.py", line 209, in __init__
    with open(vocab_file, encoding="utf-8") as vocab_handle:
TypeError: expected str, bytes or os.PathLike object, not NoneType

jacksonargo/music-mlm failed
Architectures: ['DistilBertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'jacksonargo/music-mlm'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'jacksonargo/music-mlm' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

google/pegasus-newsroom failed
Architectures: ['DistilBertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/tokenization_pegasus_fast.py", line 147, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
PegasusConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


talhaa/distilbert-base-uncased-masking-lang failed
Architectures: ['DistilBertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'talhaa/distilbert-base-uncased-masking-lang'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'talhaa/distilbert-base-uncased-masking-lang' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

bowphs/t5-homer failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/bowphs/t5-homer/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f7964-73c19ca34c73a1f228ccb53c;774c26d7-b858-43d8-8ded-a6d391cd30ef)

Repository Not Found for url: https://huggingface.co/bowphs/t5-homer/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: bowphs/t5-homer is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

miazhao/airberta_ccnet_airbnb_dat failed
Architectures: ['ElectraForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/miazhao/airberta_ccnet_airbnb_dat/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f796a-312804726f6659a93eb7666f;335d02db-788c-4ded-bba8-1868e6c619c4)

Repository Not Found for url: https://huggingface.co/miazhao/airberta_ccnet_airbnb_dat/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: miazhao/airberta_ccnet_airbnb_dat is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

diegozs97/finetuned-chemprot-seed-1-1800k failed
Architectures: ['XLMRobertaForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/finetuned-chemprot-seed-1-1800k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/finetuned-chemprot-seed-1-1800k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

flboehm/youtube-bert_10 failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

Jeevesh8/clipped_warmed_wd0_pnt_01_seq_len_128_bert-base-uncased_mnli_ft_82 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

Livingwithmachines/pea-pol failed
Architectures: ['XLMRobertaForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/Livingwithmachines/pea-pol/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f7982-1b66b1f938bed0da1f0b80f0;6a233e2a-b20c-40c0-967c-8a76b3d95b45)

Repository Not Found for url: https://huggingface.co/Livingwithmachines/pea-pol/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: Livingwithmachines/pea-pol is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

SEBIS/legal_t5_small_trans_en_de failed
Architectures: ['XLMRobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


diegozs97/chemprot-seed-3-2000k failed
Architectures: ['XLMRobertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/chemprot-seed-3-2000k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/chemprot-seed-3-2000k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

novakat/nerkor-hubert failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

gk07/wikineural-multilingual-ner failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

hckhck/buda_learning failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'hckhck/buda_learning'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'hckhck/buda_learning' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

manarea/Demaxco-sst2 failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: DistilBertModel.forward() got an unexpected keyword argument 'token_type_ids'

google/t5-large-ssm-nq failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Helsinki-NLP/opus-mt-rw-es failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

tau/False_large_t5_lm_8_1024_0.15_1 failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


akoksal/bounti failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

Jeevesh8/bert_ft_cola-49 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

BukaByaka/opus-mt-ru-en-finetuned-ru-to-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ghadeermobasher/linnaeus-WLT-256-BlueBERT-Trial-70 failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/linnaeus-WLT-256-BlueBERT-Trial-70'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/linnaeus-WLT-256-BlueBERT-Trial-70' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

ronanki/MiniLM-L12-v2-alias failed
Architectures: ['BertModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

CoffeeAddict93/gpt2-modest-proposal failed
Architectures: ['BertModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'CoffeeAddict93/gpt2-modest-proposal'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'CoffeeAddict93/gpt2-modest-proposal' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

sepiosky/ParsBERT_QA failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

dary/11_k failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/dary/11_k/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f79c4-093cc9922b8e63633ba6acab;5594a419-4bda-4d6e-9bf6-0d39e88f4680)

Entry Not Found for url: https://huggingface.co/dary/11_k/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: dary/11_k does not appear to have a file named config.json. Checkout 'https://huggingface.co/dary/11_k/main' for available files.

hfl/chinese-pert-base-mrc failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

stjiris/bert-large-portuguese-cased-legal-mlm-v0.11-gpl-nli-sts-v0 failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/stjiris/bert-large-portuguese-cased-legal-mlm-v0.11-gpl-nli-sts-v0/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f79c4-135d343f39ae9ca21f76fba2;8269863d-c86c-4df5-8cd7-85421b159d9f)

Repository Not Found for url: https://huggingface.co/stjiris/bert-large-portuguese-cased-legal-mlm-v0.11-gpl-nli-sts-v0/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: stjiris/bert-large-portuguese-cased-legal-mlm-v0.11-gpl-nli-sts-v0 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

dominguesm/pt_core_news_trf failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/dominguesm/pt_core_news_trf/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f79c5-7d610ea958da58a61b82f95b;085740b5-b0cd-48a3-86ac-db7fabb4da37)

Entry Not Found for url: https://huggingface.co/dominguesm/pt_core_news_trf/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: dominguesm/pt_core_news_trf does not appear to have a file named config.json. Checkout 'https://huggingface.co/dominguesm/pt_core_news_trf/main' for available files.

jaimin/bert-large-squad failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

nobuotto/distilbert-base-uncased-finetuned-imdb failed
Architectures: ['XLMRobertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'nobuotto/distilbert-base-uncased-finetuned-imdb'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'nobuotto/distilbert-base-uncased-finetuned-imdb' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

Jeevesh8/6ep_bert_ft_cola-84 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

ViktorDo/SciBERT-POWO_Lifecycle_Finetuned failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

allenai/tk-instruct-11b-def-pos failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


danielhou13/bert-finetuned_papers failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'danielhou13/bert-finetuned_papers'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'danielhou13/bert-finetuned_papers' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

dhtocks/Named-Entity-Recognition failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'dhtocks/Named-Entity-Recognition'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'dhtocks/Named-Entity-Recognition' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

hfl/chinese-lert-small failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

KevinChoi/bert-finetuned-squad-accelerate failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

Jeevesh8/std_0pnt2_bert_ft_cola-27 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

masakhane/afrimt5_en_swa_news failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

rampasek/prot_bert_bfd_rosetta20aa failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

sumitrsch/xlm_R_large_multiconer22_bn failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'sumitrsch/xlm_R_large_multiconer22_bn'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sumitrsch/xlm_R_large_multiconer22_bn' is the correct path to a directory containing all relevant files for a XLMRobertaTokenizerFast tokenizer.

Nhat1904/test failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Nhat1904/test'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Nhat1904/test' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Hoax0930/kyoto_marian_mod_2_2_0 failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

it5/it5-small-question-answering failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

connectivity/cola_6ep_ft-1 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

Jeevesh8/std_pnt_04_feather_berts-66 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

seduerr/pai_infi failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'seduerr/pai_infi'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'seduerr/pai_infi' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

DTAI-KULeuven/mbert-corona-tweets-belgium-curfew-support failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

mobashgr/NCBI-disease-Original-384-BioBERT-10 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

Salvatore/bert-finetuned-tmvar-corpus failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

josetapia/HyGpt-trainer-9 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'josetapia/HyGpt-trainer-9'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'josetapia/HyGpt-trainer-9' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

ghadeermobasher/BioRED-Dis-WLT-320-BlueBERT-80 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BioRED-Dis-WLT-320-BlueBERT-80'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BioRED-Dis-WLT-320-BlueBERT-80' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

CEBaB/roberta-base.CEBaB.sa.2-class.exclusive.seed_66 failed
Architectures: ['ErnieForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'CEBaB/roberta-base.CEBaB.sa.2-class.exclusive.seed_66'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'CEBaB/roberta-base.CEBaB.sa.2-class.exclusive.seed_66' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

Qilex/mbart-large-50-en-me failed
Architectures: ['ErnieForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Qilex/mbart-large-50-en-me'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Qilex/mbart-large-50-en-me' is the correct path to a directory containing all relevant files for a MBart50TokenizerFast tokenizer.

monobyte/byt5-mono-es-v1 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Jeevesh8/multiberts-seed_0-step_1000k_ft_28 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

Jeevesh8/multiberts-seed_0-step_1000k_ft_30 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

limjiayi/bert-hateful-memes-expanded failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

Jeevesh8/std_0pnt2_bert_ft_cola-3 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

csebuetnlp/mT5_m2o_russian_crossSum failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


tartuNLP/est-roberta-hist-ner failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/camembert/tokenization_camembert_fast.py", line 127, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
CamembertConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


SEBIS/code_trans_t5_small_code_documentation_generation_php_transfer_learning_finetune failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Geotrend/bert-base-en-bg-cased failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

masakhane/afribyt5_pcm_en_news failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

zeus0007/test failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

shreyasharma/shreya_sentence_truth_predictor2 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

m3/m3-experiment-roberta-base-amcd-add-v4 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-roberta-base-amcd-add-v4/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f7a8c-7b7e05f8399b80db38fc6f6a;97eddf8b-232b-466f-aa9c-2e7af784bcc5)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-roberta-base-amcd-add-v4/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-roberta-base-amcd-add-v4 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Luciano/bertimbau-large-lener_br failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

SEBIS/legal_t5_small_multitask_en_sv failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Jeevesh8/bert-base-uncased_mnli_ft_64 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

diegozs97/finetuned-sciie-seed-2-1000k failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/finetuned-sciie-seed-2-1000k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/finetuned-sciie-seed-2-1000k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

blanchefort/rubert-base-cased-sentiment-mokoron failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

TransQuest/monotransquest-hter-en_lv-it-smt failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


SEBIS/code_trans_t5_large_code_documentation_generation_python_multitask failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Lvxue/distilled-mt5-small-b0.5 failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

khasrul-alam/banglabert-finetuned-squad failed
Architectures: ['DistilBertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: DistilBertModel.forward() got an unexpected keyword argument 'token_type_ids'

carlosaguayo/pegasus-samsum failed
Architectures: ['PegasusForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1221, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 981, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

CEBaB/lstm.CEBaB.sa.5-class.exclusive.seed_88 failed
Architectures: ['RobertaModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'CEBaB/lstm.CEBaB.sa.5-class.exclusive.seed_88'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'CEBaB/lstm.CEBaB.sa.5-class.exclusive.seed_88' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

ghadeermobasher/BioNLP13CG-Modified-biobert-v1.1_latest failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

paola-md/recipe-lr5e05-wd0.02-bs64 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'paola-md/recipe-lr5e05-wd0.02-bs64'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'paola-md/recipe-lr5e05-wd0.02-bs64' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

Ebtihal/Aurora failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 111, in <module>
    assert check_align(before_trace, after_trace), "Traced model does not match the original model"
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 49, in check_align
    for key in after_trace.keys():
AttributeError: 'tuple' object has no attribute 'keys'

mitra-mir/ALBERT-Persian-Poetry failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/albert/tokenization_albert_fast.py", line 148, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
AlbertConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


m3/m3-experiment-albert-base-v2-rct-sample-eda-3 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-rct-sample-eda-3/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f7b0d-50b218a40db1ca5b4b932777;fb7036a1-5150-4db3-adf7-c041e4a82a6f)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-rct-sample-eda-3/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-albert-base-v2-rct-sample-eda-3 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Deigant/t5-base-finetuned-qg-context-dataset-2 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ShengdingHu/stsb failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

subhasisj/ar-TAPT-MLM-MiniLM failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'subhasisj/ar-TAPT-MLM-MiniLM'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'subhasisj/ar-TAPT-MLM-MiniLM' is the correct path to a directory containing all relevant files for a XLMRobertaTokenizerFast tokenizer.

feiyangDu/bert-base-cased-0210-celential failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: ParagramSPModel.forward() got an unexpected keyword argument 'token_type_ids'

Ayham/xlmroberta_gpt2_summarization_xsum failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Ayham/xlmroberta_gpt2_summarization_xsum'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Ayham/xlmroberta_gpt2_summarization_xsum' is the correct path to a directory containing all relevant files for a XLMRobertaTokenizerFast tokenizer.

CLAck/vi-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

KES/TEC-English failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

jnz/electra-ka-anti-opo failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'jnz/electra-ka-anti-opo'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'jnz/electra-ka-anti-opo' is the correct path to a directory containing all relevant files for a ElectraTokenizerFast tokenizer.

firqaaa/indo-medical-sentence-bert-base failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/firqaaa/indo-medical-sentence-bert-base/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f7b3e-1d3ae3c54e2020233c418e1a;deea4a1b-fdb2-4517-af4e-1fb0d997c8e1)

Repository Not Found for url: https://huggingface.co/firqaaa/indo-medical-sentence-bert-base/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: firqaaa/indo-medical-sentence-bert-base is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

masakhane/mt5_en_zul_news failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ethzanalytics/gpt-j-8bit-daily_dialogues failed
Architectures: ['GPTJForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 109, in <module>
    after_trace = traced_gm(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 662, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 281, in __call__
    raise e
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 271, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "<eval_with_key>.4283", line 59, in forward
    getitem_9 = getattr_8[-1];  getattr_8 = None
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_proxy.py", line 431, in impl
    return tracer.create_proxy('call_function', target, args, kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 444, in create_proxy
    value_unwrapped = self.run_target(kind, target, args_unwrapped, kwargs_unwrapped)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 374, in run_target
    result = run(kind, target, args, kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 347, in run
    return OperatorPatcherContext.patch_run(fn, *args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 291, in patch_run
    assert OperatorPatcherContext.ctx_tracer is not None
AssertionError

hugo/byt5-mono-ja-v1 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ran/c9 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ran/c9'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ran/c9' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Sweet-Hugging/dummy-model failed
Architectures: ['XLMRobertaForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/Sweet-Hugging/dummy-model/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f851b-08dcf50d5ceff0b85cc87266;78e57ad0-e859-49c0-b327-3b3e4d2ad22c)

Repository Not Found for url: https://huggingface.co/Sweet-Hugging/dummy-model/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: Sweet-Hugging/dummy-model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

TestZee/t5-small-finetuned-pytorch-final failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Matthijs/test-gpt2 failed
Architectures: ['LongformerForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Matthijs/test-gpt2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Matthijs/test-gpt2' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

smangrul/Chat-E failed
Architectures: ['BlenderbotForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/blenderbot/modeling_blenderbot.py", line 1156, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/blenderbot/modeling_blenderbot.py", line 927, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ArafatBHossain/debert_base_fine_tuned_sent140 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ArafatBHossain/debert_base_fine_tuned_sent140'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ArafatBHossain/debert_base_fine_tuned_sent140' is the correct path to a directory containing all relevant files for a DebertaTokenizerFast tokenizer.

lmqg/mt5-base-itquad-qg-ae failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

aidan-o-brien/recipe-improver failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'aidan-o-brien/recipe-improver'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'aidan-o-brien/recipe-improver' is the correct path to a directory containing all relevant files for a AlbertTokenizerFast tokenizer.

chisun/mt5-small-finetuned-amazon-en-es-accelerate failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Haakf/allsides_left_text_headline_conc failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/Haakf/allsides_left_text_headline_conc/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f8585-63952ea26450a4a457e4fb6c;862e9cb0-e201-4af5-9b53-195d18974fef)

Repository Not Found for url: https://huggingface.co/Haakf/allsides_left_text_headline_conc/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: Haakf/allsides_left_text_headline_conc is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Alred/t5-base-finetuned-summarization-cnn-ver2 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

DelinteNicolas/SDG_classifier_v0.0.1 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'DelinteNicolas/SDG_classifier_v0.0.1'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'DelinteNicolas/SDG_classifier_v0.0.1' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Jeevesh8/t5-small_re-cogs_9 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

jarvisx17/japanese-sentiment-analysis failed
Architectures: ['DistilBertForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py", line 457, in __init__
    import fugashi
ModuleNotFoundError: No module named 'fugashi'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py", line 192, in __init__
    self.word_tokenizer = MecabTokenizer(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py", line 459, in __init__
    raise error.__class__(
ModuleNotFoundError: You need to install fugashi to use MecabTokenizer. See https://pypi.org/project/fugashi/ for installation.

dracero/autotrain-dracero-fine-tuned-physics-2123168626 failed
Architectures: ['MPNetModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/dracero/autotrain-dracero-fine-tuned-physics-2123168626/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f85b3-088ad654260c751413428d72;6ee5498e-9894-4888-8ec1-0afbe84dbd0f)

Repository Not Found for url: https://huggingface.co/dracero/autotrain-dracero-fine-tuned-physics-2123168626/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: dracero/autotrain-dracero-fine-tuned-physics-2123168626 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

it5/it5-large-wiki-summarization failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-mh-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

XLab/rst-word-sense-disambiguation-11b failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ghadeermobasher/BC5CDR-disease-WLT-512-BioBERT-latest-40 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC5CDR-disease-WLT-512-BioBERT-latest-40'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC5CDR-disease-WLT-512-BioBERT-latest-40' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-guw-fi failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Qilex/mt5-large-en-me failed
Architectures: ['ElectraForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Qilex/mt5-large-en-me'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Qilex/mt5-large-en-me' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

eleldar/marian-finetuned-kde4-en-to-fr failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

SEBIS/code_trans_t5_small_code_documentation_generation_python_multitask_finetune failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


BramVanroy/xlm-roberta-base-hebban-reviews5 failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/BramVanroy/xlm-roberta-base-hebban-reviews5/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f864c-4ec709950dc7b22f762ba9a4;e0ffc80f-6b61-4417-ac44-638efb4d0800)

Repository Not Found for url: https://huggingface.co/BramVanroy/xlm-roberta-base-hebban-reviews5/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: BramVanroy/xlm-roberta-base-hebban-reviews5 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

hawoihgawjlj/STS-Team3 failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'hawoihgawjlj/STS-Team3'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'hawoihgawjlj/STS-Team3' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

vladimir-lomonosov/gpt2-wikitext2 failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'vladimir-lomonosov/gpt2-wikitext2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'vladimir-lomonosov/gpt2-wikitext2' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

LanYiU/distilbert-base-uncased-finetuned-imdb failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'LanYiU/distilbert-base-uncased-finetuned-imdb'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'LanYiU/distilbert-base-uncased-finetuned-imdb' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

diegozs97/finetuned-chemprot-seed-4-1000k failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/finetuned-chemprot-seed-4-1000k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/finetuned-chemprot-seed-4-1000k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

psyche/bloom-1b7-ko failed
Architectures: ['RobertaModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/psyche/bloom-1b7-ko/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f866b-0fdc6f3c4cf497327c1f397a;0e9819ec-8f71-4699-bf37-b9000c02d6d6)

Repository Not Found for url: https://huggingface.co/psyche/bloom-1b7-ko/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: psyche/bloom-1b7-ko is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

ghadeermobasher/BC4CHEMD-WLT-512-BlueBERT-Trial-30 failed
Architectures: ['RobertaModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC4CHEMD-WLT-512-BlueBERT-Trial-30'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC4CHEMD-WLT-512-BlueBERT-Trial-30' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Mathilda/T5-paraphrasing failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

bowphs/ancient-t5-translation failed
Architectures: ['XLMRobertaForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/bowphs/ancient-t5-translation/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f868b-434f67cd36a8652a247e5862;e649dd56-3496-4a1b-abf8-d878690596ed)

Repository Not Found for url: https://huggingface.co/bowphs/ancient-t5-translation/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: bowphs/ancient-t5-translation is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

azaninello/distilgpt2-finetuned-shroomstoy failed
Architectures: ['XLMRobertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'azaninello/distilgpt2-finetuned-shroomstoy'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'azaninello/distilgpt2-finetuned-shroomstoy' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

okamirvs/finetuning-sentiment-model-3000-samples failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/okamirvs/finetuning-sentiment-model-3000-samples/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f8693-6a42b2f968ae84ac2e022f72;36fc75c3-f642-406d-a2ae-38266dd98c84)

Repository Not Found for url: https://huggingface.co/okamirvs/finetuning-sentiment-model-3000-samples/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: okamirvs/finetuning-sentiment-model-3000-samples is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

pedramyamini/ku_t5_base-finetuned-rudaw-ku-1024-128 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

nurielw/finetuned-layoutlm-nurielw failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'nurielw/finetuned-layoutlm-nurielw'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'nurielw/finetuned-layoutlm-nurielw' is the correct path to a directory containing all relevant files for a LayoutLMv2TokenizerFast tokenizer.

stevemobs/deberta-base-combined-squad1-aqa-1epoch-and-newsqa-2epoch failed
Architectures: ['DebertaForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 31, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 974, in new_func
    encoder_outputs = self.encoder(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 470, in new_func
    hidden_states = layer_module(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 383, in new_func
    attention_output = self.attention(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 316, in new_func
    self_output = self.self(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 661, in new_func
    rel_att = self.disentangled_att_bias(query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 710, in new_func
    c2p_att = torch.gather(c2p_att, dim=-1, index=c2p_dynamic_expand(c2p_pos, query_layer, relative_pos))
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
RuntimeError: c2p_dynamic_expand() Expected a value of type 'Tensor (inferred)' for argument 'c2p_pos' but instead found type 'ConcreteProxy'.
Inferred 'c2p_pos' to be of type 'Tensor' because it was not annotated with an explicit type.
Position: 0
Value: ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]]))
Declaration: c2p_dynamic_expand(Tensor c2p_pos, Tensor query_layer, Tensor relative_pos) -> Tensor
Cast error details: Unable to cast ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]])) to Tensor

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 33, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

disenwang1994/bert-ontonotes-split2 failed
Architectures: ['DebertaForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/disenwang1994/bert-ontonotes-split2/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f86ad-5f7684114507384b00559b41;08dd20b8-e93b-4d14-9d01-29fa917da615)

Repository Not Found for url: https://huggingface.co/disenwang1994/bert-ontonotes-split2/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: disenwang1994/bert-ontonotes-split2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Ayham/bert_ernie_summarization_cnn_dailymail failed
Architectures: ['DebertaForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Ayham/bert_ernie_summarization_cnn_dailymail'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Ayham/bert_ernie_summarization_cnn_dailymail' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

TianyuHan/bertbase14 failed
Architectures: ['DebertaForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'TianyuHan/bertbase14'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'TianyuHan/bertbase14' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

bigscience/mt0-small failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

SEBIS/legal_t5_small_multitask_cs_de failed
Architectures: ['RobertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


LeoAngel/MLT failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'LeoAngel/MLT'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'LeoAngel/MLT' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

vovaf709/bert_mlm_positive failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'vovaf709/bert_mlm_positive'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'vovaf709/bert_mlm_positive' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-sg-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

akshara23/Terra-Classification failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'akshara23/Terra-Classification'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'akshara23/Terra-Classification' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

kimy1119/GCU_T5_4 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

dbmdz/t5-base-conll03-english failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

diegozs97/chemprot-seed-4-2000k failed
Architectures: ['DistilBertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/chemprot-seed-4-2000k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/chemprot-seed-4-2000k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

simecek/ZebrafishDNADeberta failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'simecek/ZebrafishDNADeberta'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'simecek/ZebrafishDNADeberta' is the correct path to a directory containing all relevant files for a DebertaTokenizerFast tokenizer.

ghadeermobasher/WLT-BioBERT-Linnaeus failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/WLT-BioBERT-Linnaeus'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/WLT-BioBERT-Linnaeus' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Linbo/dummy-model failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/Linbo/dummy-model/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f8739-182d3872550e031b37253af0;6218cb16-0dc1-4c4f-aaa9-07159a314de3)

Repository Not Found for url: https://huggingface.co/Linbo/dummy-model/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: Linbo/dummy-model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

aistatssubmission/split-1_17_210_id-078 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'aistatssubmission/split-1_17_210_id-078'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'aistatssubmission/split-1_17_210_id-078' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

alirezamsh/small100 failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1181, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 966, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

m3/m3-experiment-roberta-base-citation-intent-word-swapping-synonym-1 failed
Architectures: ['RobertaForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-roberta-base-citation-intent-word-swapping-synonym-1/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f8758-1c364c1a5e2cea2925c1a350;ad2312a4-09e6-4ccf-90f0-d0605fefbb4a)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-roberta-base-citation-intent-word-swapping-synonym-1/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-roberta-base-citation-intent-word-swapping-synonym-1 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

circulus/kobart-trans-ko-en-v1 failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: BartModel.forward() got an unexpected keyword argument 'token_type_ids'

AriakimTaiyo/DialoGPT-medium-Kumiko failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/AriakimTaiyo/DialoGPT-medium-Kumiko/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f875b-073891b062cb2733499ef0cd;c3f4432c-8eba-4d99-9c5b-562e83e9e121)

Entry Not Found for url: https://huggingface.co/AriakimTaiyo/DialoGPT-medium-Kumiko/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 100, in <module>
    config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: AriakimTaiyo/DialoGPT-medium-Kumiko does not appear to have a file named config.json. Checkout 'https://huggingface.co/AriakimTaiyo/DialoGPT-medium-Kumiko/main' for available files.

tau/fewsion_1024_0.3_3900 failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'tau/fewsion_1024_0.3_3900'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'tau/fewsion_1024_0.3_3900' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

spacy/ru_core_news_lg failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/spacy/ru_core_news_lg/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f875b-66fc25fb473f1c2e0733c46a;afa90c5a-93c5-41a0-b1fc-67a274666d15)

Entry Not Found for url: https://huggingface.co/spacy/ru_core_news_lg/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: spacy/ru_core_news_lg does not appear to have a file named config.json. Checkout 'https://huggingface.co/spacy/ru_core_news_lg/main' for available files.

csebuetnlp/mT5_m2o_english_crossSum failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


spacy/hr_core_news_sm failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/spacy/hr_core_news_sm/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f875e-1cfa77345a9ad671137c69f7;a1eb3fdd-c8e9-49e4-9786-d81df9e0caed)

Entry Not Found for url: https://huggingface.co/spacy/hr_core_news_sm/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: spacy/hr_core_news_sm does not appear to have a file named config.json. Checkout 'https://huggingface.co/spacy/hr_core_news_sm/main' for available files.

sshleifer/pegasus-cnn-ft-v2 failed
Architectures: ['XLMRobertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/tokenization_pegasus_fast.py", line 147, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
PegasusConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Peltarion/dnabert-distilbert failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2056, in _from_pretrained
    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/distilbert/tokenization_distilbert.py", line 152, in __init__
    if not os.path.isfile(vocab_file):
  File "/anaconda/envs/amc/lib/python3.10/genericpath.py", line 30, in isfile
    st = os.stat(path)
TypeError: stat: path should be string, bytes, os.PathLike or integer, not NoneType

stevemobs/deberta-base-combined-squad1-aqa-newsqa-and-newsqa failed
Architectures: ['DebertaForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 31, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 974, in new_func
    encoder_outputs = self.encoder(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 470, in new_func
    hidden_states = layer_module(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 383, in new_func
    attention_output = self.attention(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 316, in new_func
    self_output = self.self(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 661, in new_func
    rel_att = self.disentangled_att_bias(query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 710, in new_func
    c2p_att = torch.gather(c2p_att, dim=-1, index=c2p_dynamic_expand(c2p_pos, query_layer, relative_pos))
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
RuntimeError: c2p_dynamic_expand() Expected a value of type 'Tensor (inferred)' for argument 'c2p_pos' but instead found type 'ConcreteProxy'.
Inferred 'c2p_pos' to be of type 'Tensor' because it was not annotated with an explicit type.
Position: 0
Value: ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]]))
Declaration: c2p_dynamic_expand(Tensor c2p_pos, Tensor query_layer, Tensor relative_pos) -> Tensor
Cast error details: Unable to cast ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]])) to Tensor

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 33, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

m3/m3-experiment-albert-base-v2-tweet-eval-hate-word-swapping-random-4 failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-tweet-eval-hate-word-swapping-random-4/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f879a-63d5debf60f280665d516592;578b23b4-da39-4460-998d-fd139eb6bbc6)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-tweet-eval-hate-word-swapping-random-4/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-albert-base-v2-tweet-eval-hate-word-swapping-random-4 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

kbhugging/autonlp-text2sql-18413376 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

DrishtiSharma/LayoutLMv3-Finetuned-CORD_100 failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 96, in <module>
    concrete_args = tokenizer(text, return_tensors="pt")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/layoutlmv3/tokenization_layoutlmv3_fast.py", line 299, in __call__
    raise ValueError(
ValueError: Words must be of type `List[str]` (single pretokenized example), or `List[List[str]]` (batch of pretokenized examples).

m3/m3-experiment-albert-base-v2-tweet-eval-hate-add-v3 failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-tweet-eval-hate-add-v3/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f87ab-1c0f11811314674f584f2013;78ee6e97-c8ab-45bc-8b6b-d529be0002d0)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-tweet-eval-hate-add-v3/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-albert-base-v2-tweet-eval-hate-add-v3 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Helsinki-NLP/opus-mt-efi-sv failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Lvxue/distilled-mt5-small-b50 failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-ilo-sv failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

mesolitica/t5-small-finetuned-noisy-en-ms failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/mesolitica/t5-small-finetuned-noisy-en-ms/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f87cd-5e3d472717a4446674b7059d;fa546597-f3ab-4c5c-af48-c6d1df94126b)

Repository Not Found for url: https://huggingface.co/mesolitica/t5-small-finetuned-noisy-en-ms/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: mesolitica/t5-small-finetuned-noisy-en-ms is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

WindowsRegedit/zuowen failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'WindowsRegedit/zuowen'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'WindowsRegedit/zuowen' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

spacy/ro_core_news_md failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/spacy/ro_core_news_md/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f87ce-2e5bffd56ca7739b23ab5540;7926b8e1-1095-4081-8f9d-83f7bdaba2ec)

Entry Not Found for url: https://huggingface.co/spacy/ro_core_news_md/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: spacy/ro_core_news_md does not appear to have a file named config.json. Checkout 'https://huggingface.co/spacy/ro_core_news_md/main' for available files.

seduerr/t5-small-pytorch failed
Architectures: ['T5WithLMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

adalbertojunior/test-256-uncased failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/adalbertojunior/test-256-uncased/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f87e1-4c8d81df21504c5661b7428f;67911770-935d-4570-ab13-3032330cbbce)

Repository Not Found for url: https://huggingface.co/adalbertojunior/test-256-uncased/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: adalbertojunior/test-256-uncased is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

cl-tohoku/bert-base-japanese-char-whole-word-masking failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py", line 457, in __init__
    import fugashi
ModuleNotFoundError: No module named 'fugashi'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py", line 192, in __init__
    self.word_tokenizer = MecabTokenizer(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py", line 459, in __init__
    raise error.__class__(
ModuleNotFoundError: You need to install fugashi to use MecabTokenizer. See https://pypi.org/project/fugashi/ for installation.

kmfoda/long-t5-tglobal-xxl failed
Architectures: ['LongT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/longt5/modeling_longt5.py", line 1877, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/longt5/modeling_longt5.py", line 1435, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

SEBIS/code_trans_t5_large_code_documentation_generation_java_multitask_finetune failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


JorgeSarry/est5-summarize failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

din0s/t5-base-finetuned-en-to-it failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

SEBIS/code_trans_t5_large_program_synthese_multitask_finetune failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


rahul77/t5-small-finetuned-rahul-rough failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

patrickvonplaten/bert-glue-mrpc-test failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'patrickvonplaten/bert-glue-mrpc-test'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'patrickvonplaten/bert-glue-mrpc-test' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

m3/m3-experiment-albert-base-v2-rct-sample-word-swapping-synonym-3 failed
Architectures: ['DistilBertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-rct-sample-word-swapping-synonym-3/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f8910-130d57912f3ca43904645f72;71682d53-8190-42b6-9ebb-6814d18b7f9d)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-rct-sample-word-swapping-synonym-3/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-albert-base-v2-rct-sample-word-swapping-synonym-3 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Helsinki-NLP/opus-mt-es-tll failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

cuongtran/RobertaTextSummarization failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'cuongtran/RobertaTextSummarization'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'cuongtran/RobertaTextSummarization' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-es-hr failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

FritzOS/TEdetection_distiBERT_NER_final_8e failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'FritzOS/TEdetection_distiBERT_NER_final_8e'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'FritzOS/TEdetection_distiBERT_NER_final_8e' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

shivaniNK8/t5-small-finetuned-cnn-news failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Chemsseddine/bert2gpt2SUMM-finetuned-mlsum failed
Architectures: ['EncoderDecoderModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 101, in <module>
    model = AutoModel.from_config(config, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 443, in from_config
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig'> for this kind of AutoModel: AutoModel.
Model type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, LlamaConfig, CodeGenConfig, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MistralConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SamConfig, SeamlessM4TConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VitDetConfig, VitsConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig.

ufal/byt5-small-multilexnorm2021-nl failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

mattymchen/nli-synthesizer-t5-base failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

m3/m3-experiment-albert-base-v2-citation-intent-word-swapping-synonym-0 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-citation-intent-word-swapping-synonym-0/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f8971-369166ef45f5995b19807312;4df86f38-6e9a-4052-ab5c-a861efcc91bb)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-citation-intent-word-swapping-synonym-0/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-albert-base-v2-citation-intent-word-swapping-synonym-0 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

ucinlp/compas-t5-large failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


haritzpuerto/xtremedistil-l6-h256-uncased-squad_1.1 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'haritzpuerto/xtremedistil-l6-h256-uncased-squad_1.1'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'haritzpuerto/xtremedistil-l6-h256-uncased-squad_1.1' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

rajeshradhakrishnan/contradictory-watson-multi-kaggle failed
Architectures: ['AlbertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'rajeshradhakrishnan/contradictory-watson-multi-kaggle'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'rajeshradhakrishnan/contradictory-watson-multi-kaggle' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

google/t5-efficient-small-dm128 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

edwardgowsmith/roberta-base-unigram-prime failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'edwardgowsmith/roberta-base-unigram-prime'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'edwardgowsmith/roberta-base-unigram-prime' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

Mimita6654/AI4Code-01 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Mimita6654/AI4Code-01'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Mimita6654/AI4Code-01' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Lvxue/distilled-mt5-small-test2 failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

josetapia/HyBertHeuristic-model-2 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'josetapia/HyBertHeuristic-model-2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'josetapia/HyBertHeuristic-model-2' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

Graphcore/lxmert-gqa-uncased failed
Architectures: ['PoptorchPipelinedLxmertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/lxmert/modeling_lxmert.py", line 934, in forward
    raise ValueError("`visual_feats` cannot be `None`")
ValueError: `visual_feats` cannot be `None`

m3/m3-experiment-roberta-base-amcd-back-translation failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-roberta-base-amcd-back-translation/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f89e9-4786f7da7f7beff46146581c;8d428890-ccc8-4d76-8b32-72af68ff5a72)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-roberta-base-amcd-back-translation/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-roberta-base-amcd-back-translation is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

MickyMike/111-GPT2SP-talenddataquality-aptanastudio failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'MickyMike/111-GPT2SP-talenddataquality-aptanastudio'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'MickyMike/111-GPT2SP-talenddataquality-aptanastudio' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

Helsinki-NLP/opus-mt-st-fr failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

zeineb/LearningQ-t5 failed
Architectures: ['LongformerForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/zeineb/LearningQ-t5/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f89ff-5c733bae1b5e251d2edd2607;ec612426-4512-4db2-b776-e535dc55f546)

Repository Not Found for url: https://huggingface.co/zeineb/LearningQ-t5/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: zeineb/LearningQ-t5 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

arnaudstiegler/long-layoutlm failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'arnaudstiegler/long-layoutlm'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'arnaudstiegler/long-layoutlm' is the correct path to a directory containing all relevant files for a LayoutLMTokenizerFast tokenizer.

stevems1/distilroberta-base-SmithsModel failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'stevems1/distilroberta-base-SmithsModel'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'stevems1/distilroberta-base-SmithsModel' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

eslamxm/MBART-finetuned-Spanish failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mbart50/tokenization_mbart50_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
MBart50Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Chikashi/t5-small-finetuned-wikihow_3epoch failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

k4black/edos-2023-baseline-bert-base-uncased-label_sexist failed
Architectures: ['DistilBertForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/k4black/edos-2023-baseline-bert-base-uncased-label_sexist/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f8a23-41225320046b21152e654493;c11f5d8a-e4db-497a-a06c-73f18d370899)

Repository Not Found for url: https://huggingface.co/k4black/edos-2023-baseline-bert-base-uncased-label_sexist/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: k4black/edos-2023-baseline-bert-base-uncased-label_sexist is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Helsinki-NLP/opus-mt-en-zle failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

masakhane/afrimt5_twi_en_news failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

varunlpai/t5-base-cbs failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

explosion/fr_udv25_frenchsequoia_trf failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/explosion/fr_udv25_frenchsequoia_trf/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f8a3b-01a5d1d305bedb2d0baab9bc;c90e4b1a-9e4c-40e1-9e33-a64e52e2982b)

Entry Not Found for url: https://huggingface.co/explosion/fr_udv25_frenchsequoia_trf/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: explosion/fr_udv25_frenchsequoia_trf does not appear to have a file named config.json. Checkout 'https://huggingface.co/explosion/fr_udv25_frenchsequoia_trf/main' for available files.

Stancld/long-t5-tglobal-base failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Stancld/long-t5-tglobal-base'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Stancld/long-t5-tglobal-base' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

ghadeermobasher/BC4CHEMD-WLT-256-PubMedBERT-Trial-latest-70 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC4CHEMD-WLT-256-PubMedBERT-Trial-latest-70'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC4CHEMD-WLT-256-PubMedBERT-Trial-latest-70' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

m3/m3-experiment-albert-base-v2-tweet-eval-emotion-eda-1 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-tweet-eval-emotion-eda-1/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f8a3d-624fb1921c3cc14272aa0002;84ca0acf-c5fa-444e-86d5-59af717899b5)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-tweet-eval-emotion-eda-1/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-albert-base-v2-tweet-eval-emotion-eda-1 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

easyh/de_fnhd_nerdh failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/easyh/de_fnhd_nerdh/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f8a3d-33c557bd6ac80c865973eb88;e6d8f3f7-5a64-4ef8-96ef-6a29b58ca57f)

Entry Not Found for url: https://huggingface.co/easyh/de_fnhd_nerdh/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: easyh/de_fnhd_nerdh does not appear to have a file named config.json. Checkout 'https://huggingface.co/easyh/de_fnhd_nerdh/main' for available files.

dennishe97/longformer-code-mlm failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/dennishe97/longformer-code-mlm/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f8a6a-57ca6a9c5ac3b5f26d6033c9;6c6034b8-7a6d-498b-9372-514754ccd45e)

Repository Not Found for url: https://huggingface.co/dennishe97/longformer-code-mlm/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: dennishe97/longformer-code-mlm is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

haji2438/test_Com_bertweet_fine_tuned failed
Architectures: ['DistilBertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'haji2438/test_Com_bertweet_fine_tuned'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'haji2438/test_Com_bertweet_fine_tuned' is the correct path to a directory containing all relevant files for a BertweetTokenizer tokenizer.

vidhur2k/mBERT-GermanicLang failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'vidhur2k/mBERT-GermanicLang'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'vidhur2k/mBERT-GermanicLang' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

NDugar/ZSD-microsoft-v2xxlmnli failed
Architectures: ['DebertaV2ForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py", line 133, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
DebertaV2Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


StonyBrookNLP/teabreac-nt5-small failed
Architectures: ['DebertaV2ForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Helsinki-NLP/opus-mt-sv-bg failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

miesnerjacob/marian-finetuned-kde4-en-to-fr failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

vovaf709/bert_mlm_negative failed
Architectures: ['EsmForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'vovaf709/bert_mlm_negative'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'vovaf709/bert_mlm_negative' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

jenspt/bert_classification failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'jenspt/bert_classification'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'jenspt/bert_classification' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

amrezo/en_pipeline failed
Architectures: ['GPTNeoForCausalLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/amrezo/en_pipeline/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f8ac2-219500de2ac3b8324dc5693e;c1905769-14b8-4b90-bf5c-ba90239c5502)

Entry Not Found for url: https://huggingface.co/amrezo/en_pipeline/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: amrezo/en_pipeline does not appear to have a file named config.json. Checkout 'https://huggingface.co/amrezo/en_pipeline/main' for available files.

ouiame/autotrain-Robertatogpt2-995132944 failed
Architectures: ['EncoderDecoderModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 101, in <module>
    model = AutoModel.from_config(config, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 443, in from_config
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig'> for this kind of AutoModel: AutoModel.
Model type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, LlamaConfig, CodeGenConfig, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MistralConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SamConfig, SeamlessM4TConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VitDetConfig, VitsConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig.

anas-awadalla/splinter-large-few-shot-k-16-finetuned-squad-seed-4 failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/splinter/tokenization_splinter_fast.py", line 118, in __init__
    super().__init__(
TypeError: transformers.tokenization_utils_fast.PreTrainedTokenizerFast.__init__() got multiple values for keyword argument 'additional_special_tokens'

ushikado/yuyuyui-chatbot failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2056, in _from_pretrained
    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/gpt2/tokenization_gpt2.py", line 181, in __init__
    with open(vocab_file, encoding="utf-8") as vocab_handle:
TypeError: expected str, bytes or os.PathLike object, not NoneType

Benito/LayoutXLM-CHRU-5 failed
Architectures: ['ElectraForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Benito/LayoutXLM-CHRU-5'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Benito/LayoutXLM-CHRU-5' is the correct path to a directory containing all relevant files for a XLMRobertaTokenizerFast tokenizer.

Declan/Breitbart_model_v1 failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Declan/Breitbart_model_v1'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Declan/Breitbart_model_v1' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

allenai/unifiedqa-v2-t5-small-1251000 failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


SEBIS/legal_t5_small_trans_de_en failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


textattack/xlnet-base-cased-MRPC failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlnet/tokenization_xlnet_fast.py", line 150, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLNetConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


AnnasBlackHat/layoutlmv3-finetuned-cord_100 failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 96, in <module>
    concrete_args = tokenizer(text, return_tensors="pt")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/layoutlmv3/tokenization_layoutlmv3_fast.py", line 299, in __call__
    raise ValueError(
ValueError: Words must be of type `List[str]` (single pretokenized example), or `List[List[str]]` (batch of pretokenized examples).

jenniferjane/test_trainer failed
Architectures: ['ElectraForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'jenniferjane/test_trainer'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'jenniferjane/test_trainer' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

Narsil/pretrained2 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1073, in from_pretrained
    raise ValueError(
ValueError: Unrecognized model in Narsil/pretrained2. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chinese_clip, clap, clip, clipseg, code_llama, codegen, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, data2vec-audio, data2vec-text, data2vec-vision, deberta, deberta-v2, decision_transformer, deformable_detr, deit, deta, detr, dinat, dinov2, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, git, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, graphormer, groupvit, hubert, ibert, idefics, imagegpt, informer, instructblip, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, longformer, longt5, luke, lxmert, m2m_100, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mistral, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, mpnet, mpt, mra, mt5, musicgen, mvp, nat, nezha, nllb-moe, nougat, nystromformer, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, pegasus, pegasus_x, perceiver, persimmon, pix2struct, plbart, poolformer, pop2piano, prophetnet, pvt, qdqbert, rag, realm, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rwkv, sam, seamless_m4t, segformer, sew, sew-d, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, umt5, unispeech, unispeech-sat, upernet, van, videomae, vilt, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vits, vivit, wav2vec2, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso

allenai/hvila-block-layoutlm-finetuned-grotoap2 failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1064, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 761, in __getitem__
    raise KeyError(key)
KeyError: 'hierarchical_model'

m3/m3-experiment-albert-base-v2-amcd-vanilla failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-amcd-vanilla/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f8b0e-777ced0e4995b3254b1b3464;13ba8ff4-058a-40a7-a268-b0cd8f6e7338)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-amcd-vanilla/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-albert-base-v2-amcd-vanilla is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

uaritm/ukrt5-base failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


mimi/Waynehills-NLP-doogie-AIHub-paper-summary failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


sshleifer/student_marian_en_ro_6_1 failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-tc-big-es-zle failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ruriko/bacqua failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/ruriko/bacqua/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f8b44-43b4f1102590ffce3f2791dd;a5971c20-af5b-42dc-92c5-526b84aac8f7)

Entry Not Found for url: https://huggingface.co/ruriko/bacqua/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: ruriko/bacqua does not appear to have a file named config.json. Checkout 'https://huggingface.co/ruriko/bacqua/main' for available files.

razent/SciFive-large-Pubmed_PMC-MedNLI failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

rtoguchi/t5-small-finetuned-en-to-ro-fp16_off failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

hackathon-pln-es/gpt2-small-spanish-disco-poetry failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2056, in _from_pretrained
    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/gpt2/tokenization_gpt2.py", line 187, in __init__
    with open(merges_file, encoding="utf-8") as merges_handle:
TypeError: expected str, bytes or os.PathLike object, not NoneType

OrfeasTsk/bert-base-uncased-finetuned-newsqa failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'OrfeasTsk/bert-base-uncased-finetuned-newsqa'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'OrfeasTsk/bert-base-uncased-finetuned-newsqa' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Gunulhona/tbecmodel failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Gunulhona/tbecmodel'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Gunulhona/tbecmodel' is the correct path to a directory containing all relevant files for a PreTrainedTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-pap-es failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

lmqg/t5-base-tweetqa-qa failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

adalbertojunior/test-128-uncased failed
Architectures: ['OPTForCausalLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/adalbertojunior/test-128-uncased/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f8cad-32eb280074f458672a198551;49e43d5a-e177-4998-bc24-214bf5426e7d)

Repository Not Found for url: https://huggingface.co/adalbertojunior/test-128-uncased/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: adalbertojunior/test-128-uncased is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Wizounovziki/t5-base-devices-sum-ver1 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

BatuhanYilmaz/dummy-model failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'BatuhanYilmaz/dummy-model'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'BatuhanYilmaz/dummy-model' is the correct path to a directory containing all relevant files for a CamembertTokenizerFast tokenizer.

clam004/emerg-intent-gpt2-v2 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'clam004/emerg-intent-gpt2-v2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'clam004/emerg-intent-gpt2-v2' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

masakhane/m2m100_418M-EN-NEWS failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1181, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 966, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

sshleifer/distill-pegasus-xsum-12-12 failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/tokenization_pegasus_fast.py", line 147, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
PegasusConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


kravchenko/uk-mt5-base-gec failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


kevinbror/whynotwork failed
Architectures: ['XLMRobertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'kevinbror/whynotwork'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'kevinbror/whynotwork' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

elopezlopez/xlnet-base-cased_fold_3_binary failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlnet/tokenization_xlnet_fast.py", line 150, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLNetConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


hf-internal-testing/tiny-random-CodeGenForCausalLM failed
Architectures: ['CodeGenForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 109, in <module>
    after_trace = traced_gm(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 662, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 281, in __call__
    raise e
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 271, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "<eval_with_key>.581", line 77, in forward
    getitem_17 = to_1[unsqueeze];  to_1 = None
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_proxy.py", line 431, in impl
    return tracer.create_proxy('call_function', target, args, kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 444, in create_proxy
    value_unwrapped = self.run_target(kind, target, args_unwrapped, kwargs_unwrapped)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 374, in run_target
    result = run(kind, target, args, kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 347, in run
    return OperatorPatcherContext.patch_run(fn, *args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 291, in patch_run
    assert OperatorPatcherContext.ctx_tracer is not None
AssertionError

retextly/t5-small-finetuned-xsum failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ShengdingHu/qqp failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

m3/m3-experiment-roberta-base-tweet-eval-emotion-word-swapping-random-3 failed
Architectures: ['DistilBertForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-roberta-base-tweet-eval-emotion-word-swapping-random-3/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f8ceb-2d83d437728fa19708c801f1;1f49d9f3-52ff-48b1-abcb-750c32850aeb)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-roberta-base-tweet-eval-emotion-word-swapping-random-3/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-roberta-base-tweet-eval-emotion-word-swapping-random-3 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

liamliang/hate_speech_content failed
Architectures: ['DistilBertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'liamliang/hate_speech_content'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'liamliang/hate_speech_content' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

google/t5-efficient-base-nl2 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

annahaz/xlm-roberta-base-misogyny-sexism-decay0.05-fr-outdomain failed
Architectures: ['MBartForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/annahaz/xlm-roberta-base-misogyny-sexism-decay0.05-fr-outdomain/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f8d09-5dda263e24ee4972334c74de;4879ac03-f6c5-4df0-87d0-4e03ba540df2)

Repository Not Found for url: https://huggingface.co/annahaz/xlm-roberta-base-misogyny-sexism-decay0.05-fr-outdomain/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: annahaz/xlm-roberta-base-misogyny-sexism-decay0.05-fr-outdomain is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

laituan245/molt5-base failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

sshleifer/student_marian_en_ro_1_1 failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 789, in from_pretrained
    return tokenizer_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'sshleifer/student_marian_en_ro_1_1'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/student_marian_en_ro_1_1' is the correct path to a directory containing all relevant files for a MarianTokenizer tokenizer.

BBHKR/DialoGPT-small-jacksparrow failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/BBHKR/DialoGPT-small-jacksparrow/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-656f8d1d-01aaf9cf5354937e4feab50d;ca7fe3bf-5b80-4883-a7ba-740cfbc12bb3)

Cannot access gated repo for url https://huggingface.co/BBHKR/DialoGPT-small-jacksparrow/resolve/main/tokenizer_config.json.
Repo model BBHKR/DialoGPT-small-jacksparrow is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 445, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/BBHKR/DialoGPT-small-jacksparrow and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.

prashanth/IndicBART-ibart-en-to-hi failed
Architectures: ['MBartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: MBartModel.forward() got an unexpected keyword argument 'token_type_ids'

ajsmith201/t5-base-finetuned-bias-99c3c657 failed
Architectures: ['MBartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Helsinki-NLP/opus-tatoeba-fr-it failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Ife/FR-BM failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Sancha/t5-small-finetuned-fi-to-en failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ghadeermobasher/BC4CHEMD-WLT-384-PubMedBERT-Trial failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC4CHEMD-WLT-384-PubMedBERT-Trial'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC4CHEMD-WLT-384-PubMedBERT-Trial' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-lus-fi failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

stanfordnlp/stanza-cop failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/stanfordnlp/stanza-cop/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f8d45-198071af05dbe4025b5b4a85;f15409ac-75eb-465d-8cfa-aa5bbd765f34)

Entry Not Found for url: https://huggingface.co/stanfordnlp/stanza-cop/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: stanfordnlp/stanza-cop does not appear to have a file named config.json. Checkout 'https://huggingface.co/stanfordnlp/stanza-cop/main' for available files.

stevemobs/deberta-base-combined-squad1-aqa-1epoch-and-newsqa-1epoch failed
Architectures: ['DebertaForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 31, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 974, in new_func
    encoder_outputs = self.encoder(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 470, in new_func
    hidden_states = layer_module(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 383, in new_func
    attention_output = self.attention(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 316, in new_func
    self_output = self.self(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 661, in new_func
    rel_att = self.disentangled_att_bias(query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 710, in new_func
    c2p_att = torch.gather(c2p_att, dim=-1, index=c2p_dynamic_expand(c2p_pos, query_layer, relative_pos))
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
RuntimeError: c2p_dynamic_expand() Expected a value of type 'Tensor (inferred)' for argument 'c2p_pos' but instead found type 'ConcreteProxy'.
Inferred 'c2p_pos' to be of type 'Tensor' because it was not annotated with an explicit type.
Position: 0
Value: ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]]))
Declaration: c2p_dynamic_expand(Tensor c2p_pos, Tensor query_layer, Tensor relative_pos) -> Tensor
Cast error details: Unable to cast ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]])) to Tensor

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 33, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

chenshuangcufe/Bert-greenjob failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/chenshuangcufe/Bert-greenjob/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-656f8d5f-1c715fe55afcc58f7250458b;90beec77-d2b1-48ca-9ff1-207c62a941b7)

Cannot access gated repo for url https://huggingface.co/chenshuangcufe/Bert-greenjob/resolve/main/tokenizer_config.json.
Repo model chenshuangcufe/Bert-greenjob is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 445, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/chenshuangcufe/Bert-greenjob and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.

zhifei/autotrain-chinese-title-summarization-1-1084539138 failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

mustapha/distilgpt2-finetuned-wikitext2 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'mustapha/distilgpt2-finetuned-wikitext2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'mustapha/distilgpt2-finetuned-wikitext2' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

asafaya/albert-xlarge-arabic failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/albert/tokenization_albert_fast.py", line 148, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
AlbertConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


jorge-henao/spanish-t5-small-disco-poetry failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'jorge-henao/spanish-t5-small-disco-poetry'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'jorge-henao/spanish-t5-small-disco-poetry' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

prodm93/GPT2Dynamic_title_model_v1 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'prodm93/GPT2Dynamic_title_model_v1'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'prodm93/GPT2Dynamic_title_model_v1' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

Shamus/nllb-200-distilled-600M-finetuned-pan_Guru-to-eng_Latn failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1181, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 966, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-es-lua failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

PontifexMaximus/opus-mt-en-ro-finetuned-en-to-ro failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

valhalla/t5-base-squad failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


ydshieh/tiny-random-LiltForSequenceClassification failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 96, in <module>
    concrete_args = tokenizer(text, return_tensors="pt")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/layoutlmv3/tokenization_layoutlmv3_fast.py", line 299, in __call__
    raise ValueError(
ValueError: Words must be of type `List[str]` (single pretokenized example), or `List[List[str]]` (batch of pretokenized examples).

lemon234071/ct5-small failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Helsinki-NLP/opus-mt-ROMANCE-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

irenepap/t5-small-asqa-cb failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'irenepap/t5-small-asqa-cb'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'irenepap/t5-small-asqa-cb' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

cahya/bart-large failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/cahya/bart-large/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f8ddc-6b08f4a01d4c249a006ab9e0;1cc05ac1-af77-4cc4-9192-a7a66a63916e)

Repository Not Found for url: https://huggingface.co/cahya/bart-large/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: cahya/bart-large is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

tlkh/code-byt5-large failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

iamholmes/cuad_test failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'iamholmes/cuad_test'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'iamholmes/cuad_test' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-tc-big-en-el failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

cuongtran/BARTTextSummarization failed
Architectures: ['MBartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: MBartModel.forward() got an unexpected keyword argument 'token_type_ids'

NSUniversity/finelytuned-emotions failed
Architectures: ['MBartForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/NSUniversity/finelytuned-emotions/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-656f8e04-2f8e3ec0332af926648fe75d;e3db8dff-ff68-4a8f-904c-eedfc07449b8)

Cannot access gated repo for url https://huggingface.co/NSUniversity/finelytuned-emotions/resolve/main/tokenizer_config.json.
Repo model NSUniversity/finelytuned-emotions is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 445, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/NSUniversity/finelytuned-emotions and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.

DioLiu/distilroberta-base-OnlyShakeMask failed
Architectures: ['MBartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'DioLiu/distilroberta-base-OnlyShakeMask'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'DioLiu/distilroberta-base-OnlyShakeMask' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

SreyanG-NVIDIA/distilgpt2-finetuned-wikitext2 failed
Architectures: ['MBartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'SreyanG-NVIDIA/distilgpt2-finetuned-wikitext2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'SreyanG-NVIDIA/distilgpt2-finetuned-wikitext2' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

Harsit/mt5-small-finetuned-multilingual-xlsum-new failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Formzu/bart-large-japanese failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 755, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/home/yileiyang/.cache/huggingface/modules/transformers_modules/Formzu/bart-large-japanese/cd9cd265b015e9b635f71f7af89524ad96c38ddc/tokenization_bart_japanese.py", line 103, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils.py", line 367, in __init__
    self._add_tokens(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils.py", line 467, in _add_tokens
    current_vocab = self.get_vocab().copy()
  File "/home/yileiyang/.cache/huggingface/modules/transformers_modules/Formzu/bart-large-japanese/cd9cd265b015e9b635f71f7af89524ad96c38ddc/tokenization_bart_japanese.py", line 262, in get_vocab
    vocab = {self.convert_ids_to_tokens(i): i for i in range(self.vocab_size)}
  File "/home/yileiyang/.cache/huggingface/modules/transformers_modules/Formzu/bart-large-japanese/cd9cd265b015e9b635f71f7af89524ad96c38ddc/tokenization_bart_japanese.py", line 178, in vocab_size
    return len(self.sp_model) + self.fairseq_offset + 1  # Plus 1 for the mask token
AttributeError: 'BartJapaneseTokenizer' object has no attribute 'sp_model'

aytugkaya/python-gpt2-large-issues-128 failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py", line 1006, in forward
    embedding_output = self.embeddings(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py", line 232, in forward
    inputs_embeds = self.word_embeddings(input_ids)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self

spacy/sv_core_news_lg failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/spacy/sv_core_news_lg/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f8e2c-5659a76c15f66aca0879780c;1798782b-e6b9-4c16-b339-82e93e2bdb60)

Entry Not Found for url: https://huggingface.co/spacy/sv_core_news_lg/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: spacy/sv_core_news_lg does not appear to have a file named config.json. Checkout 'https://huggingface.co/spacy/sv_core_news_lg/main' for available files.

lmqg/t5-base-tweetqa-qag failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

CEBaB/lstm.CEBaB.absa.inclusive.seed_42 failed
Architectures: ['GPTNeoXForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'CEBaB/lstm.CEBaB.absa.inclusive.seed_42'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'CEBaB/lstm.CEBaB.absa.inclusive.seed_42' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py", line 133, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
DebertaV2Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


ghadeermobasher/BC5CDR-chem-WLT-320-BioBERT-latest-40 failed
Architectures: ['DistilBertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC5CDR-chem-WLT-320-BioBERT-latest-40'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC5CDR-chem-WLT-320-BioBERT-latest-40' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

luffycodes/roberta-large-md-conllpp-v2 failed
Architectures: ['DistilBertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/luffycodes/roberta-large-md-conllpp-v2/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f8f04-4ef241a67c7e4b7a228da1b7;968f16c7-c87b-4c2d-af2f-026898da5d7c)

Repository Not Found for url: https://huggingface.co/luffycodes/roberta-large-md-conllpp-v2/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: luffycodes/roberta-large-md-conllpp-v2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

elopezlopez/xlnet-base-cased_fold_5_binary_v1 failed
Architectures: ['Data2VecTextForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlnet/tokenization_xlnet_fast.py", line 150, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLNetConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Helsinki-NLP/opus-mt-fi-tn failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

SupritiVijay/fake-news-detector failed
Architectures: ['Data2VecTextForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'SupritiVijay/fake-news-detector'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'SupritiVijay/fake-news-detector' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

cammy/pegasus-cnn_dailymail-1000-lit-evalMA-ga failed
Architectures: ['PegasusForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1221, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 981, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

joe5campbell/BERT_Tweet_Sentiment_50k_5eps failed
Architectures: ['BertModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'joe5campbell/BERT_Tweet_Sentiment_50k_5eps'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'joe5campbell/BERT_Tweet_Sentiment_50k_5eps' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

jakka/t5_small_NCC_lm-finetuned-sv-frp-classifier-3 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Miranda/t5-small-train failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

patrickramos/bert-base-japanese-v2-wrime-fine-tune failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py", line 457, in __init__
    import fugashi
ModuleNotFoundError: No module named 'fugashi'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py", line 192, in __init__
    self.word_tokenizer = MecabTokenizer(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py", line 459, in __init__
    raise error.__class__(
ModuleNotFoundError: You need to install fugashi to use MecabTokenizer. See https://pypi.org/project/fugashi/ for installation.

spacy/zh_core_web_lg failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/spacy/zh_core_web_lg/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f8f4a-2fa8cb935e664a10434afba9;8e1ed882-d9b3-4afa-9168-74318a18f5f9)

Entry Not Found for url: https://huggingface.co/spacy/zh_core_web_lg/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: spacy/zh_core_web_lg does not appear to have a file named config.json. Checkout 'https://huggingface.co/spacy/zh_core_web_lg/main' for available files.

pserna/bert2bert-spanish-paraphraser failed
Architectures: ['EncoderDecoderModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 101, in <module>
    model = AutoModel.from_config(config, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 443, in from_config
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig'> for this kind of AutoModel: AutoModel.
Model type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, LlamaConfig, CodeGenConfig, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MistralConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SamConfig, SeamlessM4TConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VitDetConfig, VitsConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig.

smeoni/nbme-deberta-V3-large failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py", line 133, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
DebertaV2Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


hebafl/nlp failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2056, in _from_pretrained
    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert.py", line 199, in __init__
    if not os.path.isfile(vocab_file):
  File "/anaconda/envs/amc/lib/python3.10/genericpath.py", line 30, in isfile
    st = os.stat(path)
TypeError: stat: path should be string, bytes, os.PathLike or integer, not NoneType

Davlan/xlm-roberta-base-ner-hrl failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


kit-nlp/transformers-ud-japanese-electra-base-discriminator-cyberbullying failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 765, in from_pretrained
    raise ValueError(
ValueError: Tokenizer class sudachitra.tokenization_electra_sudachipy.ElectraSudachipyTokenizer does not exist or is not currently imported.

Hoax0930/kyoto_marian_mod_2_2 failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

spacy/de_core_news_lg failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/spacy/de_core_news_lg/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f8f93-71efa2574cccd40d4655d947;fb4b236b-800c-4fb8-ae7f-51b371e2fd64)

Entry Not Found for url: https://huggingface.co/spacy/de_core_news_lg/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: spacy/de_core_news_lg does not appear to have a file named config.json. Checkout 'https://huggingface.co/spacy/de_core_news_lg/main' for available files.

Team-PIXEL/pixel-base-finetuned-masakhaner-wol failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1064, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 761, in __getitem__
    raise KeyError(key)
KeyError: 'pixel'

Helsinki-NLP/opus-mt-tc-base-uk-ces_slk failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ablam/distilgpt2_fine_tuned_gcode failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ablam/distilgpt2_fine_tuned_gcode'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ablam/distilgpt2_fine_tuned_gcode' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

Helsinki-NLP/opus-mt-es-NORWAY failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

wandemberg-eld/opus-mt-en-de-finetuned-en-to-de failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-ts-fr failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ghadeermobasher/BC5CDR-disease-WLT-256-BlueBERT-latest-53 failed
Architectures: ['XLMRobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC5CDR-disease-WLT-256-BlueBERT-latest-53'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC5CDR-disease-WLT-256-BlueBERT-latest-53' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Narrativa/mT5-base-finetuned-tydiQA-question-generation failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-nl-no failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

prajjwal1/ctrl_discovery_10 failed
Architectures: ['CTRLLMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_proxy.py", line 79, in __getattr__
    return ConcreteAttrProxy(self, k)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_proxy.py", line 290, in __init__
    self.value = _orig_getattr(root.value, attr)
AttributeError: 'int' object has no attribute 'sqrt'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 31, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/ctrl/modeling_ctrl.py", line 473, in new_func
    outputs = h(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/ctrl/modeling_ctrl.py", line 188, in new_func
    attn_outputs = self.multi_head_attention(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/ctrl/modeling_ctrl.py", line 155, in new_func
    output = scaled_dot_product_attention(q, k, v, mask, attention_mask, head_mask)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/ctrl/modeling_ctrl.py", line 67, in new_func
    scaled_attention_logits = matmul_qk / np.sqrt(dk)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
TypeError: loop of ufunc does not support argument 0 of type ConcreteProxy which has no callable sqrt method

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 33, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

thammarat-th/distilbert-base-uncased-finetuned-imdb failed
Architectures: ['CTRLLMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'thammarat-th/distilbert-base-uncased-finetuned-imdb'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'thammarat-th/distilbert-base-uncased-finetuned-imdb' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

hf-internal-testing/tiny-random-XLMForSequenceClassification failed
Architectures: ['GPT2ForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm/tokenization_xlm.py", line 617, in __init__
    import sacremoses
ModuleNotFoundError: No module named 'sacremoses'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm/tokenization_xlm.py", line 619, in __init__
    raise ImportError(
ImportError: You need to install sacremoses to use XLMTokenizer. See https://pypi.org/project/sacremoses/ for installation.

Helsinki-NLP/opus-mt-sv-pon failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

maelfabien/marcel_customer_service_xlarge_masked failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/camembert/tokenization_camembert_fast.py", line 127, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
CamembertConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


donmaclean/dfm_cosql failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

nbroad/mrasp2-6e6d-no-mono failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/fsmt/tokenization_fsmt.py", line 201, in __init__
    import sacremoses
ModuleNotFoundError: No module named 'sacremoses'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 789, in from_pretrained
    return tokenizer_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/fsmt/tokenization_fsmt.py", line 203, in __init__
    raise ImportError(
ImportError: You need to install sacremoses to use XLMTokenizer. See https://pypi.org/project/sacremoses/ for installation.

pere/north failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'pere/north'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'pere/north' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

neulab/omnitab-large-128shot-finetuned-wtq-128shot failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 96, in <module>
    concrete_args = tokenizer(text, return_tensors="pt")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deprecated/tapex/tokenization_tapex.py", line 551, in __call__
    return self.source_call_func(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deprecated/tapex/tokenization_tapex.py", line 632, in source_call_func
    raise ValueError(
ValueError: table input must of type `pd.DataFrame` (single example), `List[pd.DataFrame]` (batch of examples). 

stevems1/distilroberta-base-SmithsModel2 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'stevems1/distilroberta-base-SmithsModel2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'stevems1/distilroberta-base-SmithsModel2' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

sumitrsch/muril_large_multiconer22_bn failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'sumitrsch/muril_large_multiconer22_bn'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sumitrsch/muril_large_multiconer22_bn' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Team-PIXEL/pixel-base-finetuned-pos-ud-korean-gsd failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1064, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 761, in __getitem__
    raise KeyError(key)
KeyError: 'pixel'

valhalla/s2t_librispeech_medium failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1064, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 761, in __getitem__
    raise KeyError(key)
KeyError: 'speech_to_text_transformer'

jwuthri/distilbert-base-uncased-finetuned-imdb failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'jwuthri/distilbert-base-uncased-finetuned-imdb'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'jwuthri/distilbert-base-uncased-finetuned-imdb' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

google/t5-efficient-large-dl12 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

pinxi/bloom-560m-bloom failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'pinxi/bloom-560m-bloom'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'pinxi/bloom-560m-bloom' is the correct path to a directory containing all relevant files for a BloomTokenizerFast tokenizer.

Lvxue/distilled-mt5-small-0.07-0.5 failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

tartuNLP/mtee-crisis failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/tartuNLP/mtee-crisis/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f905e-69cbde4b6b1ff6095f5a8cb9;da743f85-a77e-4c24-bafb-e08360d7f795)

Entry Not Found for url: https://huggingface.co/tartuNLP/mtee-crisis/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: tartuNLP/mtee-crisis does not appear to have a file named config.json. Checkout 'https://huggingface.co/tartuNLP/mtee-crisis/main' for available files.

IDEA-CCNL/Randeng-Pegasus-523M-Summary-Chinese failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2056, in _from_pretrained
    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/tokenization_pegasus.py", line 149, in __init__
    self.sp_model.Load(vocab_file)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/sentencepiece/__init__.py", line 905, in Load
    return self.LoadFromFile(model_file)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/sentencepiece/__init__.py", line 310, in LoadFromFile
    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg)
TypeError: not a string

donggyu/mnmt failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Helsinki-NLP/opus-mt-es-ro failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

tau/t5_lm_1024_0.3_epoch1_v2 failed
Architectures: ['ElectraForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'tau/t5_lm_1024_0.3_epoch1_v2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'tau/t5_lm_1024_0.3_epoch1_v2' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

joshanashakya/codebert_sourcecode_nmt_ja2pn_50E_2e-05LR_16B failed
Architectures: ['EncoderDecoderModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 101, in <module>
    model = AutoModel.from_config(config, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 443, in from_config
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig'> for this kind of AutoModel: AutoModel.
Model type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, LlamaConfig, CodeGenConfig, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MistralConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SamConfig, SeamlessM4TConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VitDetConfig, VitsConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig.

DHBaek/xlm-roberta-large-korquad-mask failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Jellywibble/dummy_question_mark_model failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Jellywibble/dummy_question_mark_model'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Jellywibble/dummy_question_mark_model' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

chuaziheng/metaphor-id-xlm-roberta failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'chuaziheng/metaphor-id-xlm-roberta'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'chuaziheng/metaphor-id-xlm-roberta' is the correct path to a directory containing all relevant files for a XLMRobertaTokenizerFast tokenizer.

hisaoka/pegasus-pubmed_dataset_radiology_20220912.tsv failed
Architectures: ['PegasusForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1221, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 981, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

disenwang1994/bert-base-1e-5 failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/disenwang1994/bert-base-1e-5/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f90b6-4390d9fe0349d6ba70f2f930;2e4457ee-03d5-46e7-a201-f4c7cf50bc56)

Repository Not Found for url: https://huggingface.co/disenwang1994/bert-base-1e-5/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: disenwang1994/bert-base-1e-5 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

sanbohork/t5 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

abhinavkulkarni/bigbird-roberta-base-finetuned-squad failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/abhinavkulkarni/bigbird-roberta-base-finetuned-squad/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f90ce-1347db3902df29df1731c895;d2933d6c-6169-451d-a284-e780ed13104f)

Repository Not Found for url: https://huggingface.co/abhinavkulkarni/bigbird-roberta-base-finetuned-squad/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: abhinavkulkarni/bigbird-roberta-base-finetuned-squad is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Helsinki-NLP/opus-mt-de-ig failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

IlyaGusev/sber_rut5_filler failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

leokai/distilroberta-base-finetuned-wikitextepoch_230 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'leokai/distilroberta-base-finetuned-wikitextepoch_230'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'leokai/distilroberta-base-finetuned-wikitextepoch_230' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

younggns/mf_distilbert failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'younggns/mf_distilbert'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'younggns/mf_distilbert' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

josetapia/HyBertHeuristic-model-18 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'josetapia/HyBertHeuristic-model-18'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'josetapia/HyBertHeuristic-model-18' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

strikertweny/t5-base-medium-title-generation failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

flairbook/flairmodel failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/flairbook/flairmodel/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f9122-240bba816f64373e37d585c4;ab7330da-1b80-4d33-bab5-2095a6585b3c)

Entry Not Found for url: https://huggingface.co/flairbook/flairmodel/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: flairbook/flairmodel does not appear to have a file named config.json. Checkout 'https://huggingface.co/flairbook/flairmodel/main' for available files.

Helsinki-NLP/opus-mt-it-bg failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

sdi/greek-text-summarization failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

StivenLancheros/xlm-roberta-base-finetuned-ner-false-finetuned-ner-2002-1 failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/StivenLancheros/xlm-roberta-base-finetuned-ner-false-finetuned-ner-2002-1/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f912c-392f2eeb46f2735b29f666b7;451b0968-f0d6-4339-bbc8-8a0ca2ad58e3)

Repository Not Found for url: https://huggingface.co/StivenLancheros/xlm-roberta-base-finetuned-ner-false-finetuned-ner-2002-1/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: StivenLancheros/xlm-roberta-base-finetuned-ner-false-finetuned-ner-2002-1 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Helsinki-NLP/opus-mt-vi-fr failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

domenicrosati/deberta-v3-large-finetuned-synthetic-translated-only failed
Architectures: ['ElectraForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py", line 133, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
DebertaV2Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


icon-it-tdtu/mt-en-vi-optimum failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

marblyso/DialoGPT-medium-kel failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/marblyso/DialoGPT-medium-kel/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f914a-63e898d30a035af428191377;0ec3fbeb-ade0-4eac-833d-78a01fb9f372)

Repository Not Found for url: https://huggingface.co/marblyso/DialoGPT-medium-kel/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: marblyso/DialoGPT-medium-kel is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

diegozs97/chemprot-seed-1-1800k failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/chemprot-seed-1-1800k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/chemprot-seed-1-1800k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

sonoisa/t5-base-japanese-title-generation failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Helsinki-NLP/opus-mt-guw-fr failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

hanseokhyeon/bert-badword-large failed
Architectures: ['XLMRobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'hanseokhyeon/bert-badword-large'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'hanseokhyeon/bert-badword-large' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Tritkoman/GermantoHunsrik failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-uk-es failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

HJOK/free-bart-v5 failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/HJOK/free-bart-v5/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f91a6-3d4aea7a1a6ae8f52e975f16;af088f0f-9dc4-4f9d-80cb-108b66be121a)

Repository Not Found for url: https://huggingface.co/HJOK/free-bart-v5/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: HJOK/free-bart-v5 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

google/t5-efficient-base-nl32 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

abusiddik/autotrain-NMT-778623908 failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

akreal/mbart-large-50-finetuned-slue failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mbart50/tokenization_mbart50_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
MBart50Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


it5/it5-efficient-small-el32-formal-to-informal failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

rycont/ryconic-bart-ko-en-code-v1 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/rycont/ryconic-bart-ko-en-code-v1/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f91d4-088eb6d06b9099846e3b8708;44c8f422-47de-48e9-94fb-2d53a2b2cf39)

Repository Not Found for url: https://huggingface.co/rycont/ryconic-bart-ko-en-code-v1/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: rycont/ryconic-bart-ko-en-code-v1 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

fenffef/t5-content-moderation failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/fenffef/t5-content-moderation/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f91d9-67be0c8612ec4f5f248f4fcc;8bd1a9ea-1504-490c-9d3c-bf47714a630f)

Repository Not Found for url: https://huggingface.co/fenffef/t5-content-moderation/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: fenffef/t5-content-moderation is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

masakhane/m2m100_418M_en_swa_rel_news failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1181, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 966, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-tc-big-en-ro failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

m3/m3-experiment-albert-base-v2-amcd-add-v3-greedy failed
Architectures: ['GPT2ForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-amcd-add-v3-greedy/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f91fa-72122b0e6c2892fd6288b0a2;0b74d2c8-2069-4b4d-9464-8cfd3ea0da45)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-amcd-add-v3-greedy/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-albert-base-v2-amcd-add-v3-greedy is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

woctordho/lojban-translation failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

masakhane/m2m100_418M_hau_en_rel_ft failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1181, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 966, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

tareknaous/bert2bert-empathetic-dialogues failed
Architectures: ['EncoderDecoderModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 101, in <module>
    model = AutoModel.from_config(config, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 443, in from_config
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig'> for this kind of AutoModel: AutoModel.
Model type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, LlamaConfig, CodeGenConfig, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MistralConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SamConfig, SeamlessM4TConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VitDetConfig, VitsConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig.

ghadeermobasher/BioRED-Chem-WLT-320-SciBERT failed
Architectures: ['EncoderDecoderModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BioRED-Chem-WLT-320-SciBERT'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BioRED-Chem-WLT-320-SciBERT' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

ashu1318/lilt-en-funsd failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 96, in <module>
    concrete_args = tokenizer(text, return_tensors="pt")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/layoutlmv3/tokenization_layoutlmv3_fast.py", line 299, in __call__
    raise ValueError(
ValueError: Words must be of type `List[str]` (single pretokenized example), or `List[List[str]]` (batch of pretokenized examples).

zhuqing/bert-large-whole-uncased-exp3-parent-nointersection failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'zhuqing/bert-large-whole-uncased-exp3-parent-nointersection'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'zhuqing/bert-large-whole-uncased-exp3-parent-nointersection' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

lilouuch/t5-base-finetuned-xsum_epoch_1 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/lilouuch/t5-base-finetuned-xsum_epoch_1/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f9257-6ddca9b0707c344b713aa875;33f14531-5be8-48ec-9fe5-2f9fb6462bef)

Repository Not Found for url: https://huggingface.co/lilouuch/t5-base-finetuned-xsum_epoch_1/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: lilouuch/t5-base-finetuned-xsum_epoch_1 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

negfir/bert_uncased_L-6_H-768_A-12 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'negfir/bert_uncased_L-6_H-768_A-12'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'negfir/bert_uncased_L-6_H-768_A-12' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

vparytskyy/lucy-small failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Tonjk/OCR_wangchanberta-base-att-spm-uncased failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/Tonjk/OCR_wangchanberta-base-att-spm-uncased/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f926b-2a052f940dc1d8dc10519e46;3ad93ae2-fd05-4b40-99a9-469e1e51b146)

Repository Not Found for url: https://huggingface.co/Tonjk/OCR_wangchanberta-base-att-spm-uncased/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: Tonjk/OCR_wangchanberta-base-att-spm-uncased is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

allenai/System4_classify_FigLang2022 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'allenai/System4_classify_FigLang2022'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'allenai/System4_classify_FigLang2022' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

lmqg/t5-small-subjqa-movies-qg failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-sv-st failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

rwante/t5-small-finetuned-mlsum-tr failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

MickyMike/7-GPT2SP-jirasoftware failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'MickyMike/7-GPT2SP-jirasoftware'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'MickyMike/7-GPT2SP-jirasoftware' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

Julietheg/checkpoint-1000 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

frizwankhan/tokenization_model failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'frizwankhan/tokenization_model'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'frizwankhan/tokenization_model' is the correct path to a directory containing all relevant files for a LayoutLMv2TokenizerFast tokenizer.

negfir/bert_uncased_L-10_H-768_A-12wiki103 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'negfir/bert_uncased_L-10_H-768_A-12wiki103'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'negfir/bert_uncased_L-10_H-768_A-12wiki103' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

WillHeld/t5-base-pointer-top_v2 failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-es-xh failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Tritkoman/EnglishtoAncientGreek failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

rajkumarrrk/gpt-2-fine-tuned-on-cnn-dm failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'rajkumarrrk/gpt-2-fine-tuned-on-cnn-dm'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'rajkumarrrk/gpt-2-fine-tuned-on-cnn-dm' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

monsoon-nlp/byt5-dv failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

harish/t5-e2e-10epochs-lr1e4-alpha0-1PLUSalpha0-9-e30 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

mohsin-riad/bert-finetuned-ner failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/mohsin-riad/bert-finetuned-ner/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f92f3-0ae42c2f065f53365bf5e123;995953be-45ea-4238-b865-ec676b4b4191)

Repository Not Found for url: https://huggingface.co/mohsin-riad/bert-finetuned-ner/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: mohsin-riad/bert-finetuned-ner is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

diegozs97/finetuned-chemprot-seed-3-2000k failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/finetuned-chemprot-seed-3-2000k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/finetuned-chemprot-seed-3-2000k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

jdminor/autotrain-t5-base-samsum-2333773650 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

laxya007/gpt2_bd2_systemanalysis failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/laxya007/gpt2_bd2_systemanalysis/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f92fc-39049e3865ba28bc34b801a1;04eb527b-0234-4000-ba10-dca91b79aa27)

Repository Not Found for url: https://huggingface.co/laxya007/gpt2_bd2_systemanalysis/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: laxya007/gpt2_bd2_systemanalysis is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

brjezierski/german-gpt2-easy failed
Architectures: ['MBartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'brjezierski/german-gpt2-easy'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'brjezierski/german-gpt2-easy' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

datasciencemmw/current-best failed
Architectures: ['MBartForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/datasciencemmw/current-best/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f9322-4a4b6b7b0f9e646a6c9ddf5b;5727e544-427e-495c-99a2-ae94bf172876)

Repository Not Found for url: https://huggingface.co/datasciencemmw/current-best/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: datasciencemmw/current-best is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

SetFit/deberta-v3-large__sst2__train-16-4 failed
Architectures: ['MBartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py", line 133, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
DebertaV2Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


t5-11b failed
Architectures: ['T5WithLMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ml6team/byt5-base-dutch-ocr-correction failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ChaiML/dalio_book_e3 failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/ChaiML/dalio_book_e3/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f93de-446381d31db2ca321676317a;2b972a22-a804-4c6e-bc90-89a0597b1a2d)

Repository Not Found for url: https://huggingface.co/ChaiML/dalio_book_e3/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: ChaiML/dalio_book_e3 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

mrm8488/t5-small-finetuned-AESLC-summarization failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


KeLiu/QETRA_JavaScript failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


PhilSad/GPT-J6B-Guided-SCP failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'PhilSad/GPT-J6B-Guided-SCP'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'PhilSad/GPT-J6B-Guided-SCP' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

MickyMike/77-GPT2SP-usergrid-mesos failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'MickyMike/77-GPT2SP-usergrid-mesos'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'MickyMike/77-GPT2SP-usergrid-mesos' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

prodm93/gpt2-sum-abstract-model-v1 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'prodm93/gpt2-sum-abstract-model-v1'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'prodm93/gpt2-sum-abstract-model-v1' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

cstorm125/marianmt-th-zh_cn failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-sm-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-iso-fi failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

VanessaSchenkel/pt-unicamp-handcrafted failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

liat-nakayama/roberta_base_ja_20190121_m10000_v24000_u125000 failed
Architectures: ['MBartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'liat-nakayama/roberta_base_ja_20190121_m10000_v24000_u125000'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'liat-nakayama/roberta_base_ja_20190121_m10000_v24000_u125000' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

vumichien/sequence-classification-bigbird-roberta-base failed
Architectures: ['MBartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/big_bird/tokenization_big_bird_fast.py", line 139, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
BigBirdConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


google/t5-efficient-base-el4 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

CEBaB/roberta-base.CEBaB.absa.inclusive.seed_77 failed
Architectures: ['BertModelWithHeads']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'CEBaB/roberta-base.CEBaB.absa.inclusive.seed_77'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'CEBaB/roberta-base.CEBaB.absa.inclusive.seed_77' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

masakhane/afrimt5_en_pcm_news failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

victorialslocum/en_reciparse_model failed
Architectures: ['SplinterForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/victorialslocum/en_reciparse_model/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f9458-66fef38f72e9d85d44ea086e;ebfd518f-4994-4f34-86c7-d5b0239b28c5)

Entry Not Found for url: https://huggingface.co/victorialslocum/en_reciparse_model/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: victorialslocum/en_reciparse_model does not appear to have a file named config.json. Checkout 'https://huggingface.co/victorialslocum/en_reciparse_model/main' for available files.

Abdullah17/distill-bert-uncased-clickbait failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 571, in get_tokenizer_config
    result = json.load(reader)
  File "/anaconda/envs/amc/lib/python3.10/json/__init__.py", line 293, in load
    return loads(fp.read(),
  File "/anaconda/envs/amc/lib/python3.10/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
  File "/anaconda/envs/amc/lib/python3.10/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/anaconda/envs/amc/lib/python3.10/json/decoder.py", line 355, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

m3/m3-experiment-albert-base-v2-tweet-eval-hate-word-swapping-embedding-0 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-tweet-eval-hate-word-swapping-embedding-0/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f9497-6e8fc10312db414d3686ab70;9895b745-97c5-47e1-b707-d0264429dd12)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-tweet-eval-hate-word-swapping-embedding-0/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-albert-base-v2-tweet-eval-hate-word-swapping-embedding-0 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

merve/deberta-small-mrpc failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/merve/deberta-small-mrpc/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f949d-351a999e7697c38c6da005d1;c4812676-de0c-476e-8212-c7e1a7cdb39d)

Repository Not Found for url: https://huggingface.co/merve/deberta-small-mrpc/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: merve/deberta-small-mrpc is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

circulus/kobart-trans-gangwon-v1 failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: BartModel.forward() got an unexpected keyword argument 'token_type_ids'

StonyBrookNLP/t5-large-iirc-retrieved failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


m3/m3-experiment-albert-base-v2-tweet-eval-irony-add-v5 failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-tweet-eval-irony-add-v5/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f94a4-7f68ea97450cc5cb6bb8990e;2e55a664-7634-4271-89c9-a0b04b86e231)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-tweet-eval-irony-add-v5/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-albert-base-v2-tweet-eval-irony-add-v5 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Helsinki-NLP/opus-mt-de-pis failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

botisan-ai/mt5-translate-zh-yue failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Wizounovziki/t5-small-devices-sum-ver3 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

DeskDown/MarianMix_en-ja-10 failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

anonsubms/msrp_length_sb failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ad6398/gupshup_e2e_t5 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


diegozs97/finetuned-sciie-seed-4-1500k failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/finetuned-sciie-seed-4-1500k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/finetuned-sciie-seed-4-1500k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

MoeZilla/Chatbot failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2056, in _from_pretrained
    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/gpt2/tokenization_gpt2.py", line 181, in __init__
    with open(vocab_file, encoding="utf-8") as vocab_handle:
TypeError: expected str, bytes or os.PathLike object, not NoneType

chou/distilbert-base-uncased-finetuned-imdb failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'chou/distilbert-base-uncased-finetuned-imdb'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'chou/distilbert-base-uncased-finetuned-imdb' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

anthonymirand/haha_2019_adaptation_task failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'anthonymirand/haha_2019_adaptation_task'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'anthonymirand/haha_2019_adaptation_task' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

yliu337/t5_token_nonfilter_bothcontext failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


SimonZvara/Memes-CS_1.0 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'SimonZvara/Memes-CS_1.0'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'SimonZvara/Memes-CS_1.0' is the correct path to a directory containing all relevant files for a XLMRobertaTokenizerFast tokenizer.

tezign/BERT-LSTM-based-ABSA failed
Architectures: ['BertLSTMForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 101, in <module>
    model = AutoModel.from_config(config, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 443, in from_config
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers_modules.tezign.BERT-LSTM-based-ABSA.30bf7f71427501ca7a5300825589f2a708843566.configuration.BertABSAConfig'> for this kind of AutoModel: AutoModel.
Model type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, LlamaConfig, CodeGenConfig, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MistralConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SamConfig, SeamlessM4TConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VitDetConfig, VitsConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig.

jmdatasci/MiniMBart failed
Architectures: ['BertLSTMForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'jmdatasci/MiniMBart'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'jmdatasci/MiniMBart' is the correct path to a directory containing all relevant files for a MBart50TokenizerFast tokenizer.

rycont/biblify failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: BartModel.forward() got an unexpected keyword argument 'token_type_ids'

mariopeng/phoneT5seg failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Jiexing/cosql_add_coref_t5_3b-1280 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-tc-big-lv-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Cloudy/DialoGPT-CJ-large failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 571, in get_tokenizer_config
    result = json.load(reader)
  File "/anaconda/envs/amc/lib/python3.10/json/__init__.py", line 293, in load
    return loads(fp.read(),
  File "/anaconda/envs/amc/lib/python3.10/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
  File "/anaconda/envs/amc/lib/python3.10/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/anaconda/envs/amc/lib/python3.10/json/decoder.py", line 355, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

Istiaque190515/Sherlock failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/Istiaque190515/Sherlock/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f9554-3b8a04c73a0efa7e245a75c8;cc223c2f-fc31-4eb4-8b46-690c7db24d29)

Entry Not Found for url: https://huggingface.co/Istiaque190515/Sherlock/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: Istiaque190515/Sherlock does not appear to have a file named config.json. Checkout 'https://huggingface.co/Istiaque190515/Sherlock/main' for available files.

princeton-nlp/CoFi-SST2-s95 failed
Architectures: ['NewBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 31, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py", line 1013, in new_func
    encoder_outputs = self.encoder(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py", line 607, in new_func
    layer_outputs = layer_module(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py", line 497, in new_func
    self_attention_outputs = self.attention(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py", line 436, in new_func
    attention_output = self.output(self_outputs[0], hidden_states)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py", line 386, in new_func
    hidden_states = self.dense(hidden_states)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 855, in module_call_wrapper
    ret_val = _orig_module_call(mod, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1383, in func_wrapper
    return tracer.create_proxy('call_function', to_func, args, kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 444, in create_proxy
    value_unwrapped = self.run_target(kind, target, args_unwrapped, kwargs_unwrapped)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 374, in run_target
    result = run(kind, target, args, kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 347, in run
    return OperatorPatcherContext.patch_run(fn, *args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 33, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

yihsuan/best_model_0426_base failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

sumitrsch/muril_base_multiconer22_bn failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'sumitrsch/muril_base_multiconer22_bn'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sumitrsch/muril_base_multiconer22_bn' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

negfir/bert_uncased_L-6_H-512_A-8 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'negfir/bert_uncased_L-6_H-512_A-8'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'negfir/bert_uncased_L-6_H-512_A-8' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

lhoestq/distilbert-base-uncased-finetuned-absa-as failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'lhoestq/distilbert-base-uncased-finetuned-absa-as'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'lhoestq/distilbert-base-uncased-finetuned-absa-as' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

AmitBHuji/mt5-small-finetuned-mt5-simplification-1epoch failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

MickyMike/66-GPT2SP-appceleratorstudio-aptanastudio failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'MickyMike/66-GPT2SP-appceleratorstudio-aptanastudio'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'MickyMike/66-GPT2SP-appceleratorstudio-aptanastudio' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

mobashgr/linnaeus-WLT-320-BioBERT-ISNS-10 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/mobashgr/linnaeus-WLT-320-BioBERT-ISNS-10/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f9595-5f3624d4472b4b6c5c5c330c;324c70d9-84ea-4e90-b5ed-8073f64b6217)

Repository Not Found for url: https://huggingface.co/mobashgr/linnaeus-WLT-320-BioBERT-ISNS-10/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: mobashgr/linnaeus-WLT-320-BioBERT-ISNS-10 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

sonoisa/byt5-small-japanese failed
Architectures: ['MT5ForConditionGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

SEBIS/legal_t5_small_multitask_de_it failed
Architectures: ['MT5ForConditionGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


MorrisPark/twc10-bart-r3f failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: BartModel.forward() got an unexpected keyword argument 'token_type_ids'

Gunulhona/tbstmodel_v4 failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: BartModel.forward() got an unexpected keyword argument 'token_type_ids'

bochaowei/t5-small-finetuned-cnn-wei0 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

hisaoka/pegasus-samsum failed
Architectures: ['PegasusForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1221, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 981, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

roa7n/DNABert_K6_G_quad failed
Architectures: ['PegasusForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'roa7n/DNABert_K6_G_quad'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'roa7n/DNABert_K6_G_quad' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

rtoguchi/t5-small-finetuned-en-to-ro-weight_decay_0.001 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

meongracun/nmt-mpst-id-en-lr_1e-05-ep_20-seq_128_bs-32 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Artifact-AI/flan-t5-xxl-sharded-fp16 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ydshieh/bert2bert-cnn_dailymail-fp16 failed
Architectures: ['EncoderDecoderModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 101, in <module>
    model = AutoModel.from_config(config, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 443, in from_config
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig'> for this kind of AutoModel: AutoModel.
Model type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, LlamaConfig, CodeGenConfig, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MistralConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SamConfig, SeamlessM4TConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VitDetConfig, VitsConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig.

lmqg/t5-small-squadshifts-reddit-qg failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

SEBIS/legal_t5_small_trans_es_it_small_finetuned failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


SEBIS/code_trans_t5_large_code_documentation_generation_javascript_transfer_learning_finetune failed
Architectures: ['RobertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Helsinki-NLP/opus-mt-ilo-fi failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

masakhane/m2m100_418M_en_nya_rel failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1181, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 966, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

FelipeAD/mt5-small-SENTENCE_COMPRESSION failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-gmw-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

masakhane/mt5_fr_bbj_news failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

SetFit/deberta-v3-large__sst2__train-8-1 failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py", line 133, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
DebertaV2Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


din0s/t5-base-finetuned-en-to-it-hrs failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

spacy/es_core_news_lg failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/spacy/es_core_news_lg/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f96e3-3cdf39e4118e04a368a50efc;f3e80c32-cbbc-44bb-ac0a-18001575e8b3)

Entry Not Found for url: https://huggingface.co/spacy/es_core_news_lg/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: spacy/es_core_news_lg does not appear to have a file named config.json. Checkout 'https://huggingface.co/spacy/es_core_news_lg/main' for available files.

joshanashakya/codebert_sourcecode_nmt_ja2pn_100E_2e-05LR_16B_12E_12D failed
Architectures: ['EncoderDecoderModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 101, in <module>
    model = AutoModel.from_config(config, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 443, in from_config
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig'> for this kind of AutoModel: AutoModel.
Model type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, LlamaConfig, CodeGenConfig, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MistralConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SamConfig, SeamlessM4TConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VitDetConfig, VitsConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig.

StonyBrookNLP/teabreac-nt5-small-drop failed
Architectures: ['EncoderDecoderModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


SivilTaram/tapex-t5-large-lm-adapt failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

rossanez/t5-small-finetuned-de-en-256-epochs2 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ffrmns/t5-small_XSum-finetuned failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Helsinki-NLP/opus-mt-yo-fr failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

furyhawk/t5-base-finetuned-bbc-headline failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

MrBananaHuman/klue_ner failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'MrBananaHuman/klue_ner'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'MrBananaHuman/klue_ner' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

bakrianoo/t5-arabic-base failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


stanfordnlp/stanza-be failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/stanfordnlp/stanza-be/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f9744-392868360c88fa5e7422a68e;2c7d29ee-eb41-4400-aff7-674e86e38d68)

Entry Not Found for url: https://huggingface.co/stanfordnlp/stanza-be/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: stanfordnlp/stanza-be does not appear to have a file named config.json. Checkout 'https://huggingface.co/stanfordnlp/stanza-be/main' for available files.

Helsinki-NLP/opus-mt-fr-af failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-no-da failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ghadeermobasher/BioRED-Chem-WLT-512-BioBERT failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BioRED-Chem-WLT-512-BioBERT'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BioRED-Chem-WLT-512-BioBERT' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Yaxin/xlm-roberta-base-yelp-mlm failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Helsinki-NLP/opus-mt-kg-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

allenai/System3_DREAM_FLUTE_emotion_FigLang2022 failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'allenai/System3_DREAM_FLUTE_emotion_FigLang2022'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'allenai/System3_DREAM_FLUTE_emotion_FigLang2022' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

davidcechak/DNADeberta_finehuman_nontata_promoters failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'davidcechak/DNADeberta_finehuman_nontata_promoters'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'davidcechak/DNADeberta_finehuman_nontata_promoters' is the correct path to a directory containing all relevant files for a DebertaTokenizerFast tokenizer.

peterhsu/mt5-small-finetuned-amazon-en-es failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

alicekwak/TN_distilbert-base-uncased failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'alicekwak/TN_distilbert-base-uncased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'alicekwak/TN_distilbert-base-uncased' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

anahitapld/electra-small-dbd failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'anahitapld/electra-small-dbd'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'anahitapld/electra-small-dbd' is the correct path to a directory containing all relevant files for a ElectraTokenizerFast tokenizer.

m3/m3-experiment-roberta-base-amcd-add-v5 failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-roberta-base-amcd-add-v5/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f97c0-5814123d799d7b887b1c3439;92105984-dd06-4338-93db-0aff0223c26f)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-roberta-base-amcd-add-v5/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-roberta-base-amcd-add-v5 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

ghadeermobasher/BC4CHEMD-WLT-320-PubMedBERT-latest failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC4CHEMD-WLT-320-PubMedBERT-latest'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC4CHEMD-WLT-320-PubMedBERT-latest' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

bragovo/flux-xlsum failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/bragovo/flux-xlsum/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f97c0-58c25de418c9ed053eded3b0;b89d1a74-e1b9-42d2-b90a-e46786a588a7)

Repository Not Found for url: https://huggingface.co/bragovo/flux-xlsum/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: bragovo/flux-xlsum is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

pszemraj/long-t5-tglobal-large-pubmed-3k-booksum-16384-WIP14 failed
Architectures: ['LongT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/longt5/modeling_longt5.py", line 1877, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/longt5/modeling_longt5.py", line 1435, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

AnonymousSub/SciFive_pubmedqa_question_generation failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

juancavallotti/t5-base-gec failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

u1537782/it_tei2go failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/u1537782/it_tei2go/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f9803-75f775a80c9d45fa48646e5f;df71a8f5-5dd8-4cc9-9687-d76214525522)

Repository Not Found for url: https://huggingface.co/u1537782/it_tei2go/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: u1537782/it_tei2go is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

MarianaLC/mt5-finetuned-amazon-en-accelerate failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

alanakbik/test-push-public failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/alanakbik/test-push-public/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f980f-2f75f8c74b34da5c2fbeaa28;173faf1a-f9f8-466c-abfd-204f1ef4c962)

Entry Not Found for url: https://huggingface.co/alanakbik/test-push-public/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: alanakbik/test-push-public does not appear to have a file named config.json. Checkout 'https://huggingface.co/alanakbik/test-push-public/main' for available files.

ytlin/31r11ahz_2 failed
Architectures: ['XLMRobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mbart/tokenization_mbart_fast.py", line 123, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
MBartConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


lmqg/mt5-small-jaquad-qg-ae failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

mrm8488/deberta-v3-small-finetuned-squadv2 failed
Architectures: ['GectorbertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py", line 133, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
DebertaV2Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Rumesh/mbart-tr-simp failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mbart/tokenization_mbart_fast.py", line 123, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
MBartConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


masakhane/mt5_lug_en_news failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Copninich/distilbert-base-uncased-finetuned-imdb failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Copninich/distilbert-base-uncased-finetuned-imdb'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Copninich/distilbert-base-uncased-finetuned-imdb' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

t5-base failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

masakhane/m2m100_418M_yor_en_rel_news_ft failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1181, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 966, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

aiknowyou/mt5-base-it-paraphraser failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


dropout05/t5-tiny failed
Architectures: ['DistilBertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'dropout05/t5-tiny'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'dropout05/t5-tiny' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

research-backup/t5-large-squad-qg-default failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

google/pegasus-reddit_tifu failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/tokenization_pegasus_fast.py", line 147, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
PegasusConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


assayw119/koelectra-wellnesee-text-classification.pth failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'assayw119/koelectra-wellnesee-text-classification.pth'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'assayw119/koelectra-wellnesee-text-classification.pth' is the correct path to a directory containing all relevant files for a ElectraTokenizerFast tokenizer.

allenai/hvila-row-layoutlm-finetuned-grotoap2 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1064, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 761, in __getitem__
    raise KeyError(key)
KeyError: 'hierarchical_model'

AbidHasan95/movieHunt2 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'AbidHasan95/movieHunt2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'AbidHasan95/movieHunt2' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

diegozs97/chemprot-seed-3-200k failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/chemprot-seed-3-200k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/chemprot-seed-3-200k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

research-backup/t5-large-subjqa-vanilla-restaurants-qg failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

kit-nlp/bert-base-japanese-sentiment-irony failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py", line 457, in __init__
    import fugashi
ModuleNotFoundError: No module named 'fugashi'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py", line 192, in __init__
    self.word_tokenizer = MecabTokenizer(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py", line 459, in __init__
    raise error.__class__(
ModuleNotFoundError: You need to install fugashi to use MecabTokenizer. See https://pypi.org/project/fugashi/ for installation.

masakhane/m2m100_418M_amh_en_rel failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1181, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 966, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

BumBelDumBel/ZORK_AI_SCIFI failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'BumBelDumBel/ZORK_AI_SCIFI'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'BumBelDumBel/ZORK_AI_SCIFI' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

MaryaAI/opus-mt-ar-en-finetunedTanzil-v6-ar-to-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

reneknaebel/de_dep_hdt_dist failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/reneknaebel/de_dep_hdt_dist/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f9906-3a727cc34a3353f6537894f4;0167d717-e9a6-4166-b494-0feca546dc1b)

Entry Not Found for url: https://huggingface.co/reneknaebel/de_dep_hdt_dist/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: reneknaebel/de_dep_hdt_dist does not appear to have a file named config.json. Checkout 'https://huggingface.co/reneknaebel/de_dep_hdt_dist/main' for available files.

QuoQA-NLP/KE-T5-Ko2En-Base failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ChaiML/dalio_training_epoch_8_lr_9e-7 failed
Architectures: ['GPTNeoForCausalLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/ChaiML/dalio_training_epoch_8_lr_9e-7/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f9936-08355f5c4bc68ddc065d4541;79c2f77e-3e1b-4ca5-a10d-e4d6d4b64707)

Repository Not Found for url: https://huggingface.co/ChaiML/dalio_training_epoch_8_lr_9e-7/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: ChaiML/dalio_training_epoch_8_lr_9e-7 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

MaryaAI/opus-mt-ar-en-finetunedQAdata-v1-ar-to-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ghadeermobasher/BC4CHEMD-WLT-320-SciBERT-latest failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC4CHEMD-WLT-320-SciBERT-latest'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC4CHEMD-WLT-320-SciBERT-latest' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

SEBIS/code_trans_t5_small_source_code_summarization_sql failed
Architectures: ['XLMRobertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


google/t5-efficient-tiny-nl16 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Jingya/t5-large-finetuned-xsum failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

SteveC/sdc_bot_two_step failed
Architectures: ['DistilBertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'SteveC/sdc_bot_two_step'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'SteveC/sdc_bot_two_step' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

mohadz19/sadarjasa failed
Architectures: ['DistilBertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/mohadz19/sadarjasa/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-656f9997-1ba3a1d639bbaf0948aabbd0;94b86dea-4a77-4670-9e05-bab1c0d2f91b)

Cannot access gated repo for url https://huggingface.co/mohadz19/sadarjasa/resolve/main/tokenizer_config.json.
Repo model mohadz19/sadarjasa is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 445, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/mohadz19/sadarjasa and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.

abdulmatinomotoso/multi_news_headline_generator_testing_2 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/tokenization_pegasus_fast.py", line 147, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
PegasusConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


paola-md/recipesberta failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'paola-md/recipesberta'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'paola-md/recipesberta' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

vikram15/t5-small-finetuned-newsSummary failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

suksun1412/wangchanberta-ner-2 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/camembert/tokenization_camembert_fast.py", line 127, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
CamembertConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


piotr-rybak/poleval2021-task4-herbert-large-encoder failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/herbert/tokenization_herbert.py", line 338, in __init__
    import sacremoses
ModuleNotFoundError: No module named 'sacremoses'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/herbert/tokenization_herbert_fast.py", line 77, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 117, in __init__
    slow_tokenizer = self.slow_tokenizer_class(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/herbert/tokenization_herbert.py", line 340, in __init__
    raise ImportError(
ImportError: You need to install sacremoses to use HerbertTokenizer. See https://pypi.org/project/sacremoses/ for installation.

SJ-Ray/Re-Punctuate failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


malmarjeh/gpt2 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'malmarjeh/gpt2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'malmarjeh/gpt2' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

boychaboy/MNLI_bert-base-cased_4 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/boychaboy/MNLI_bert-base-cased_4/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f99d7-4b59e37c1bcca7af63a52151;01023456-0972-4b66-9c81-0578e8e07c1d)

Repository Not Found for url: https://huggingface.co/boychaboy/MNLI_bert-base-cased_4/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: boychaboy/MNLI_bert-base-cased_4 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

google/tapas-small-finetuned-sqa failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 96, in <module>
    concrete_args = tokenizer(text, return_tensors="pt")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/tapas/tokenization_tapas.py", line 643, in __call__
    assert isinstance(table, pd.DataFrame), "Table must be of type pd.DataFrame"
AssertionError: Table must be of type pd.DataFrame

masakhane/m2m100_418M_fr_bbj_news failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1181, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 966, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Qilex/mbart-large-50-en-me failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Qilex/mbart-large-50-en-me'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Qilex/mbart-large-50-en-me' is the correct path to a directory containing all relevant files for a MBart50TokenizerFast tokenizer.

diegozs97/chemprot-seed-2-60k failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/chemprot-seed-2-60k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/chemprot-seed-2-60k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Teepika/t5-small-finetuned-xsum-proplus failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

nlp-waseda/roberta-base-japanese-with-auto-jumanpp failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py", line 649, in __init__
    import rhoknp
ModuleNotFoundError: No module named 'rhoknp'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py", line 200, in __init__
    self.word_tokenizer = JumanppTokenizer(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py", line 651, in __init__
    raise ImportError(
ImportError: You need to install rhoknp to use JumanppTokenizer. See https://github.com/ku-nlp/rhoknp for installation.

ashwinperti/newSentiment_1Oct22 failed
Architectures: ['OPTForCausalLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/ashwinperti/newSentiment_1Oct22/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-656f9a32-23412efb6a3b64925ab34f36;e5c52f44-bc49-499b-8cb9-3753de05a971)

Cannot access gated repo for url https://huggingface.co/ashwinperti/newSentiment_1Oct22/resolve/main/tokenizer_config.json.
Repo model ashwinperti/newSentiment_1Oct22 is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 445, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/ashwinperti/newSentiment_1Oct22 and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.

Helsinki-NLP/opus-mt-hu-sv failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-en-el failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

simecek/DNADebertaK6_Zebrafish failed
Architectures: ['RobertaForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'simecek/DNADebertaK6_Zebrafish'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'simecek/DNADebertaK6_Zebrafish' is the correct path to a directory containing all relevant files for a DebertaTokenizerFast tokenizer.

stuartmesham/deberta-large_basetags_5k_2_p3 failed
Architectures: ['GectorbertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 31, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 974, in new_func
    encoder_outputs = self.encoder(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 470, in new_func
    hidden_states = layer_module(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 383, in new_func
    attention_output = self.attention(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 316, in new_func
    self_output = self.self(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 661, in new_func
    rel_att = self.disentangled_att_bias(query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 710, in new_func
    c2p_att = torch.gather(c2p_att, dim=-1, index=c2p_dynamic_expand(c2p_pos, query_layer, relative_pos))
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
RuntimeError: c2p_dynamic_expand() Expected a value of type 'Tensor (inferred)' for argument 'c2p_pos' but instead found type 'ConcreteProxy'.
Inferred 'c2p_pos' to be of type 'Tensor' because it was not annotated with an explicit type.
Position: 0
Value: ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]]))
Declaration: c2p_dynamic_expand(Tensor c2p_pos, Tensor query_layer, Tensor relative_pos) -> Tensor
Cast error details: Unable to cast ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]])) to Tensor

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 33, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

Helsinki-NLP/opus-mt-niu-fi failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

mphamsioo/lol failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'mphamsioo/lol'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'mphamsioo/lol' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

flaubert/flaubert_small_cased failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/flaubert/tokenization_flaubert.py", line 262, in __init__
    import sacremoses
ModuleNotFoundError: No module named 'sacremoses'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 789, in from_pretrained
    return tokenizer_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/flaubert/tokenization_flaubert.py", line 264, in __init__
    raise ImportError(
ImportError: You need to install sacremoses to use FlaubertTokenizer. See https://pypi.org/project/sacremoses/ for installation.

Vasanth/t5-news-summarization failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


eliotm/t5-small-finetuned-en-to-ro-LR_1e-3 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

oftshsl/t5_ua_gec failed
Architectures: ['XLMRobertaForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


josetapia/hybertheuristic-trainer failed
Architectures: ['XLMRobertaForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'josetapia/hybertheuristic-trainer'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'josetapia/hybertheuristic-trainer' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

DioLiu/distilroberta-base-Ctrl2 failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'DioLiu/distilroberta-base-Ctrl2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'DioLiu/distilroberta-base-Ctrl2' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-ve-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

abdusah/ft-tatoeba-ar-en failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1181, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 966, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

PSW/t5-base-mediasum-seed17 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

OttoZastrow/t5-small_dummy-AGdvF89L263nk65gZBuo5E failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/OttoZastrow/t5-small_dummy-AGdvF89L263nk65gZBuo5E/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f9ae5-045be52361806eda1680f879;082307ae-65ab-43cd-9c0b-80ec1516d9b0)

Repository Not Found for url: https://huggingface.co/OttoZastrow/t5-small_dummy-AGdvF89L263nk65gZBuo5E/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: OttoZastrow/t5-small_dummy-AGdvF89L263nk65gZBuo5E is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Helsinki-NLP/opus-mt-en-pis failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

m3/m3-experiment-roberta-base-chemprot-word-swapping-synonym-1 failed
Architectures: ['RobertaForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-roberta-base-chemprot-word-swapping-synonym-1/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f9afc-1a6d519a5d364384258e0462;44348167-cbb5-448a-81d7-c449bc4e2336)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-roberta-base-chemprot-word-swapping-synonym-1/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-roberta-base-chemprot-word-swapping-synonym-1 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

ramsrigouthamg/t5-large-paraphraser-diverse-high-quality failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


it5/mt5-base-headline-generation failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

juierror/thai-news-summarization failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


luke-thorburn/suggest-objections-full-finetune failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'luke-thorburn/suggest-objections-full-finetune'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'luke-thorburn/suggest-objections-full-finetune' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

gayanin/t5-small-med-term-mlm failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Declan/ChicagoTribune_model_v8 failed
Architectures: ['BertModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Declan/ChicagoTribune_model_v8'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Declan/ChicagoTribune_model_v8' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Qilex/mbart-large-cc25-en-me failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Qilex/mbart-large-cc25-en-me'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Qilex/mbart-large-cc25-en-me' is the correct path to a directory containing all relevant files for a MBartTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-wal-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

IlyaGusev/rubert_telegram_headlines failed
Architectures: ['EncoderDecoderModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 101, in <module>
    model = AutoModel.from_config(config, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 443, in from_config
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig'> for this kind of AutoModel: AutoModel.
Model type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, LlamaConfig, CodeGenConfig, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MistralConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SamConfig, SeamlessM4TConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VitDetConfig, VitsConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig.

aopstudio/my-summary failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'aopstudio/my-summary'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'aopstudio/my-summary' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

Helsinki-NLP/opus-mt-en-mfe failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

explosion/ru_udv25_russiangsd_trf failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/explosion/ru_udv25_russiangsd_trf/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f9bb4-2c08b61e5d7978632f84943b;fa1536a5-2d75-4d1b-a40e-989e4077734e)

Entry Not Found for url: https://huggingface.co/explosion/ru_udv25_russiangsd_trf/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: explosion/ru_udv25_russiangsd_trf does not appear to have a file named config.json. Checkout 'https://huggingface.co/explosion/ru_udv25_russiangsd_trf/main' for available files.

birgermoell/t5-base-swedish failed
Architectures: ['T5Model']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

SEBIS/code_trans_t5_base_code_documentation_generation_python failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


google/bigbird-roberta-large failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/big_bird/tokenization_big_bird_fast.py", line 139, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
BigBirdConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


diegozs97/finetuned-chemprot-seed-1-0k failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/finetuned-chemprot-seed-1-0k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/finetuned-chemprot-seed-1-0k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

tau/False_large_t5_lm_8_1024_0.15_1 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


huranokuma/es failed
Architectures: ['DistilBertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


prajjwal1/roberta-large-mnli failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2056, in _from_pretrained
    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/roberta/tokenization_roberta.py", line 227, in __init__
    with open(merges_file, encoding="utf-8") as merges_handle:
TypeError: expected str, bytes or os.PathLike object, not NoneType

camilodefelipe/t5_squad_v1_es failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


formermagic/codet5-small failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

asifhugs/Bittensor-gpt-neo-2.7B failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/asifhugs/Bittensor-gpt-neo-2.7B/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-656f9c08-41f7847e16cc53752ac5c1fe;edc3b220-3fb3-42d2-a0fe-b749b1d7f769)

Cannot access gated repo for url https://huggingface.co/asifhugs/Bittensor-gpt-neo-2.7B/resolve/main/tokenizer_config.json.
Repo model asifhugs/Bittensor-gpt-neo-2.7B is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 445, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/asifhugs/Bittensor-gpt-neo-2.7B and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.

hululuzhu/chinese-couplet-t5-mengzi-finetune failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


MickyMike/222-GPT2SP-mulestudio-titanium failed
Architectures: ['ElectraForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'MickyMike/222-GPT2SP-mulestudio-titanium'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'MickyMike/222-GPT2SP-mulestudio-titanium' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

chiendvhust/distilbert-base-uncased-finetuned-QA failed
Architectures: ['ElectraForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/chiendvhust/distilbert-base-uncased-finetuned-QA/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f9c14-34b5ceb567501dbd70ca45a4;169d98d0-5cc4-489e-8ca6-bd013bf2708a)

Repository Not Found for url: https://huggingface.co/chiendvhust/distilbert-base-uncased-finetuned-QA/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: chiendvhust/distilbert-base-uncased-finetuned-QA is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Sebabrata/lmv2-g-w9-2018-148-doc-07-07_1 failed
Architectures: ['ElectraForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 96, in <module>
    concrete_args = tokenizer(text, return_tensors="pt")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py", line 253, in __call__
    raise ValueError(
ValueError: Words must be of type `List[str]` (single pretokenized example), or `List[List[str]]` (batch of pretokenized examples).

diegozs97/chemprot-seed-0-700k failed
Architectures: ['BertModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/chemprot-seed-0-700k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/chemprot-seed-0-700k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

pertschuk/albert-base-squad-classifier-ms failed
Architectures: ['BertModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'pertschuk/albert-base-squad-classifier-ms'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'pertschuk/albert-base-squad-classifier-ms' is the correct path to a directory containing all relevant files for a AlbertTokenizerFast tokenizer.

moyix/codegen-350M-multi-gptj failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'moyix/codegen-350M-multi-gptj'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'moyix/codegen-350M-multi-gptj' is the correct path to a directory containing all relevant files for a CodeGenTokenizerFast tokenizer.

fxmarty/t5-small-onnx failed
Architectures: ['T5WithLMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

mesolitica/finetune-translation-austronesian-t5-base-standard-bahasa-cased failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/mesolitica/finetune-translation-austronesian-t5-base-standard-bahasa-cased/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f9c64-38d3aaa27e6ce7816c1bfb02;65bdeea8-438e-442e-8528-193c94a2df8c)

Repository Not Found for url: https://huggingface.co/mesolitica/finetune-translation-austronesian-t5-base-standard-bahasa-cased/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: mesolitica/finetune-translation-austronesian-t5-base-standard-bahasa-cased is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Qilex/t5-small-en-me failed
Architectures: ['MBartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Qilex/t5-small-en-me'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Qilex/t5-small-en-me' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

bhumikak/resultse failed
Architectures: ['PegasusForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1221, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 981, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-fr-st failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

NilsDamAi/nils-nl-to-rx-pt-v6 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

abhi1nandy2/EManuals_BERT failed
Architectures: ['EsmForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert_fast.py", line 221, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 111, in __init__
    fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)
Exception: No such file or directory (os error 2)

achieveordie/DistilBERT-SmallFiNER failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/achieveordie/DistilBERT-SmallFiNER/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-656f9cb4-78891e7d7728ec2f1545c82f;9b18b926-1638-4250-8c2e-3409acc7f5f3)

Cannot access gated repo for url https://huggingface.co/achieveordie/DistilBERT-SmallFiNER/resolve/main/tokenizer_config.json.
Repo model achieveordie/DistilBERT-SmallFiNER is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 445, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/achieveordie/DistilBERT-SmallFiNER and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.

DylanJHJ/mt5-large-mmarco-v2-temp failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


insop/xlm-roberta-base-finetuned-panx-it failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/insop/xlm-roberta-base-finetuned-panx-it/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f9cb7-26a22b5f201565320ec159e2;9068b8bc-50c9-4288-8720-8dfa921152ce)

Repository Not Found for url: https://huggingface.co/insop/xlm-roberta-base-finetuned-panx-it/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: insop/xlm-roberta-base-finetuned-panx-it is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

ai4bharat/MultiIndicQuestionGenerationSS failed
Architectures: ['RobertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/albert/tokenization_albert_fast.py", line 148, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
AlbertConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Salesforce/codegen-6B-multi failed
Architectures: ['CodeGenForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 109, in <module>
    after_trace = traced_gm(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 662, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 281, in __call__
    raise e
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 271, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "<eval_with_key>.1857", line 77, in forward
    getitem_17 = to_1[unsqueeze];  to_1 = None
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_proxy.py", line 431, in impl
    return tracer.create_proxy('call_function', target, args, kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 444, in create_proxy
    value_unwrapped = self.run_target(kind, target, args_unwrapped, kwargs_unwrapped)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 374, in run_target
    result = run(kind, target, args, kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 347, in run
    return OperatorPatcherContext.patch_run(fn, *args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 291, in patch_run
    assert OperatorPatcherContext.ctx_tracer is not None
AssertionError

Jeevesh8/t5-small-cogs_18 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ThomasNLG/t5-qg_webnlg_synth-en failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


masakhane/m2m100_418M_mos_fr_rel failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1181, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 966, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-run-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Akhror/autotrain-text-classification-kunuz-1630257501 failed
Architectures: ['DebertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 31, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 974, in new_func
    encoder_outputs = self.encoder(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 470, in new_func
    hidden_states = layer_module(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 383, in new_func
    attention_output = self.attention(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 316, in new_func
    self_output = self.self(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 661, in new_func
    rel_att = self.disentangled_att_bias(query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 710, in new_func
    c2p_att = torch.gather(c2p_att, dim=-1, index=c2p_dynamic_expand(c2p_pos, query_layer, relative_pos))
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
RuntimeError: c2p_dynamic_expand() Expected a value of type 'Tensor (inferred)' for argument 'c2p_pos' but instead found type 'ConcreteProxy'.
Inferred 'c2p_pos' to be of type 'Tensor' because it was not annotated with an explicit type.
Position: 0
Value: ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]]))
Declaration: c2p_dynamic_expand(Tensor c2p_pos, Tensor query_layer, Tensor relative_pos) -> Tensor
Cast error details: Unable to cast ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]])) to Tensor

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 33, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

philschmid/pt-test failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

BlackKakapo/t5-small-grammar-ro-v2 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/BlackKakapo/t5-small-grammar-ro-v2/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f9d59-4f08def51e3f26992d60321a;247eae6c-8e24-40c9-b8ce-68fee9e18f75)

Repository Not Found for url: https://huggingface.co/BlackKakapo/t5-small-grammar-ro-v2/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: BlackKakapo/t5-small-grammar-ro-v2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

diegozs97/chemprot-seed-3-100k failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/chemprot-seed-3-100k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/chemprot-seed-3-100k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

HJOK/free-bart-v3 failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: BartModel.forward() got an unexpected keyword argument 'token_type_ids'

tgummadi/t5-11785-t5-20-reinforce-bertscore failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'tgummadi/t5-11785-t5-20-reinforce-bertscore'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'tgummadi/t5-11785-t5-20-reinforce-bertscore' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

somesh212/Harry_Potter_botDialoGPT_Som2 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/somesh212/Harry_Potter_botDialoGPT_Som2/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-656f9d8e-4c6a9831604f08fb2f1256f2;bd582e88-7628-43ce-8f17-74394eef5582)

Cannot access gated repo for url https://huggingface.co/somesh212/Harry_Potter_botDialoGPT_Som2/resolve/main/tokenizer_config.json.
Repo model somesh212/Harry_Potter_botDialoGPT_Som2 is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 445, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/somesh212/Harry_Potter_botDialoGPT_Som2 and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.

alireza7/ARMAN-MSR-persian-base-perkey-summary failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/tokenization_pegasus_fast.py", line 147, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
PegasusConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


mtreviso/ct5-small-en-wiki failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

MoyAI/ProfNet failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/MoyAI/ProfNet/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f9daa-5ceacf6d21af8b1d44f2f7bf;3f5830af-be19-45d7-a710-4912931b5117)

Repository Not Found for url: https://huggingface.co/MoyAI/ProfNet/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: MoyAI/ProfNet is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Shamus/NLLB-600m-swh_Latn-to-eng_Latn failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1181, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 966, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

hugo/byt5-mono-fi-v1 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

htermotto/distilbert-base-uncased-finetuned-sngp-for-qa-squad-seed-9990 failed
Architectures: ['SNGPDistilbertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: DistilBertModel.forward() got an unexpected keyword argument 'token_type_ids'

ilhami/Tr_En_AcademicTranslation failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

kookyklavicle/sean-diaz-bot failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2056, in _from_pretrained
    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/gpt2/tokenization_gpt2.py", line 181, in __init__
    with open(vocab_file, encoding="utf-8") as vocab_handle:
TypeError: expected str, bytes or os.PathLike object, not NoneType

Cedille/fr-boris failed
Architectures: ['GPTJForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 109, in <module>
    after_trace = traced_gm(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 662, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 281, in __call__
    raise e
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 271, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "<eval_with_key>.1917", line 59, in forward
    getitem_9 = getattr_8[-1];  getattr_8 = None
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_proxy.py", line 431, in impl
    return tracer.create_proxy('call_function', target, args, kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 444, in create_proxy
    value_unwrapped = self.run_target(kind, target, args_unwrapped, kwargs_unwrapped)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 374, in run_target
    result = run(kind, target, args, kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 347, in run
    return OperatorPatcherContext.patch_run(fn, *args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 291, in patch_run
    assert OperatorPatcherContext.ctx_tracer is not None
AssertionError

Helsinki-NLP/opus-mt-fr-niu failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

laituan245/molt5-base-smiles2caption failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

miazhao/airberta_airbnb_dat_lang8_xml_roberta_base_single_instance failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/miazhao/airberta_airbnb_dat_lang8_xml_roberta_base_single_instance/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f9e34-6452513574300ce64fda9f3e;dcce7b4a-e2c4-4a35-87d3-e2b273c1a920)

Repository Not Found for url: https://huggingface.co/miazhao/airberta_airbnb_dat_lang8_xml_roberta_base_single_instance/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: miazhao/airberta_airbnb_dat_lang8_xml_roberta_base_single_instance is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

masakhane/byt5_fr_mos_news failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

gustavecortal/T0_3B-8bit failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'gustavecortal/T0_3B-8bit'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'gustavecortal/T0_3B-8bit' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

huranokuma/es2 failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


jackstanley/HAT-base-4096-modified failed
Architectures: ['DistilBertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/.cache/huggingface/modules/transformers_modules/jackstanley/HAT-base-4096-modified/aaa8c8dac4b712e2ab99d2e1b7c33b97f0e2ac94/tokenization_hat.py", line 19, in <module>
    from nltk import sent_tokenize
ModuleNotFoundError: No module named 'nltk'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 751, in from_pretrained
    tokenizer_class = get_class_from_dynamic_module(class_ref, pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/dynamic_module_utils.py", line 499, in get_class_from_dynamic_module
    return get_class_in_module(class_name, final_module.replace(".py", ""))
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/dynamic_module_utils.py", line 199, in get_class_in_module
    module = importlib.import_module(module_path)
  File "/anaconda/envs/amc/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/yileiyang/.cache/huggingface/modules/transformers_modules/jackstanley/HAT-base-4096-modified/aaa8c8dac4b712e2ab99d2e1b7c33b97f0e2ac94/tokenization_hat.py", line 21, in <module>
    raise Exception('NLTK is not installed! Install it with `pip install nltk`...')
Exception: NLTK is not installed! Install it with `pip install nltk`...

alicekwak/TN-final-bert-base-uncased failed
Architectures: ['DistilBertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'alicekwak/TN-final-bert-base-uncased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'alicekwak/TN-final-bert-base-uncased' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

eslamxm/mt5-base-finetuned-persian failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

textattack/albert-base-v2-WNLI failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/albert/tokenization_albert_fast.py", line 148, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
AlbertConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


nloc2578/QAG_Pegasus_2ep failed
Architectures: ['CustomPegasusForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1221, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 981, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-fi-efi failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

kiri-ai/t5-base-qa-summary-emotion failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Capstone/autotrain-healthcare_summarization_uta-2207670804 failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/Capstone/autotrain-healthcare_summarization_uta-2207670804/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-656f9e91-1931bd0b5e3d9f5f1939fc84;dd8b73f0-3183-4332-97ba-ae5f577152c0)

Cannot access gated repo for url https://huggingface.co/Capstone/autotrain-healthcare_summarization_uta-2207670804/resolve/main/tokenizer_config.json.
Repo model Capstone/autotrain-healthcare_summarization_uta-2207670804 is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 445, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/Capstone/autotrain-healthcare_summarization_uta-2207670804 and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.

Helsinki-NLP/opus-mt-uk-hu failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

stanfordnlp/stanza-nn failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/stanfordnlp/stanza-nn/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f9e9c-31e2a8883a90e0ec635caecb;1a7bf217-c5f8-4001-b8e3-899f8ba49d3f)

Entry Not Found for url: https://huggingface.co/stanfordnlp/stanza-nn/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: stanfordnlp/stanza-nn does not appear to have a file named config.json. Checkout 'https://huggingface.co/stanfordnlp/stanza-nn/main' for available files.

mirakle/marian-finetuned-kde4-en-to-fr_2 failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

gsarti/opus-mt-tc-base-en-ru failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

nzwii/model_11244029 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


dyyyyyyyy/MVR_squad_XLM-RoBERTa-large failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2056, in _from_pretrained
    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/roberta/tokenization_roberta.py", line 221, in __init__
    with open(vocab_file, encoding="utf-8") as vocab_handle:
TypeError: expected str, bytes or os.PathLike object, not NoneType

imosnoi/md_bon_2 failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'imosnoi/md_bon_2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'imosnoi/md_bon_2' is the correct path to a directory containing all relevant files for a LayoutLMTokenizerFast tokenizer.

stanfordnlp/stanza-my failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/stanfordnlp/stanza-my/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656f9ed8-5c326e9415dd0c3f653425eb;000b2cbe-7672-4e78-bc61-518693b3791a)

Entry Not Found for url: https://huggingface.co/stanfordnlp/stanza-my/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: stanfordnlp/stanza-my does not appear to have a file named config.json. Checkout 'https://huggingface.co/stanfordnlp/stanza-my/main' for available files.

FritzOS/TEdetection_distiBERT_mLM_V3 failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'FritzOS/TEdetection_distiBERT_mLM_V3'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'FritzOS/TEdetection_distiBERT_mLM_V3' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

alireza7/TRANSFORMER-persian-base-tebyan failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/tokenization_pegasus_fast.py", line 147, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
PegasusConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


jatinshah/distilbert-base-uncased-finetuned-imdb failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'jatinshah/distilbert-base-uncased-finetuned-imdb'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'jatinshah/distilbert-base-uncased-finetuned-imdb' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

diegozs97/finetuned-chemprot-seed-2-1500k failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/finetuned-chemprot-seed-2-1500k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/finetuned-chemprot-seed-2-1500k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

minu/koelectra-nsmc-discriminator failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'minu/koelectra-nsmc-discriminator'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'minu/koelectra-nsmc-discriminator' is the correct path to a directory containing all relevant files for a ElectraTokenizerFast tokenizer.

igorktech/rubert-tiny-bender-rodriguez-emotion failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/igorktech/rubert-tiny-bender-rodriguez-emotion/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f9ef0-1a25d2db2b8cceb5039f1a7f;ecb5c13d-0a02-4d7f-95a8-84d10f5dab98)

Repository Not Found for url: https://huggingface.co/igorktech/rubert-tiny-bender-rodriguez-emotion/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: igorktech/rubert-tiny-bender-rodriguez-emotion is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

sileod/distractors_prediction failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/sileod/distractors_prediction/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f9ef9-638611d30134f7e96bd70ce2;8904c462-92f5-4894-91ee-89254aacf1b1)

Repository Not Found for url: https://huggingface.co/sileod/distractors_prediction/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: sileod/distractors_prediction is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

mesolitica/finetune-mnli-t5-super-tiny-standard-bahasa-cased failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


huak95/mt-align-finetuned-LST-en-to-th failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

mimi/ke-t5-base-ko-AIHub-paper-summary failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


hf-internal-testing/tiny-random-CanineForQuestionAnswering failed
Architectures: ['CanineForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 31, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/canine/modeling_canine.py", line 1212, in new_func
    sequence_output = self.projection(concat)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/canine/modeling_canine.py", line 383, in new_func
    result = self.conv(pad(inputs))
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 841, in module_call_wrapper
    module_qualified_name = self.path_of_module(mod)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 580, in path_of_module
    sub_path = _orig_type(mod).__name__ + mod.extra_repr()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/padding.py", line 28, in extra_repr
    return 'padding={}, value={}'.format(self.padding, self.value)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 820, in module_getattribute_wrapper
    if self.the_path_of_middle_class[id(mod)] == '':
KeyError: 140098680764736

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 33, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

Rohitvadhya06/distilbert-base-uncased failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Rohitvadhya06/distilbert-base-uncased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Rohitvadhya06/distilbert-base-uncased' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

ctoraman/RoBERTa-TR-medium-char failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ctoraman/RoBERTa-TR-medium-char'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ctoraman/RoBERTa-TR-medium-char' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

roscazo/BNE-conv-v1 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'roscazo/BNE-conv-v1'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'roscazo/BNE-conv-v1' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

it5/it5-large-question-generation failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

warrormac/autotrain-my-train-2209070896 failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

AhmedSSoliman/MarianCG-CoNaLa failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

fgua/bert-base-uncased-wikitext2 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'fgua/bert-base-uncased-wikitext2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'fgua/bert-base-uncased-wikitext2' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-uk-no failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-es-vi failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

google/t5-11b-ssm-wq failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


m3/m3-experiment-roberta-base-tweet-eval-emotion-eda-0 failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-roberta-base-tweet-eval-emotion-eda-0/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656f9fa1-67781131533733f14c357dc9;d2ca6e76-ee03-4e4c-9649-6748d0b5e5bd)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-roberta-base-tweet-eval-emotion-eda-0/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-roberta-base-tweet-eval-emotion-eda-0 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

luffycodes/roberta-large-ner-conllpp-v3 failed
Architectures: ['OPTForCausalLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/luffycodes/roberta-large-ner-conllpp-v3/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656fa02b-1bf8fe5c6892ff82101666f9;35a132f4-ebd8-491c-9c71-5b125f9f9a0d)

Repository Not Found for url: https://huggingface.co/luffycodes/roberta-large-ner-conllpp-v3/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: luffycodes/roberta-large-ner-conllpp-v3 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

rajistics/layoutlmv2-finetuned-cord_100 failed
Architectures: ['OPTForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'rajistics/layoutlmv2-finetuned-cord_100'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'rajistics/layoutlmv2-finetuned-cord_100' is the correct path to a directory containing all relevant files for a LayoutLMv2TokenizerFast tokenizer.

MickyMike/2-GPT2SP-moodle failed
Architectures: ['OPTForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'MickyMike/2-GPT2SP-moodle'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'MickyMike/2-GPT2SP-moodle' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

beomi/exKcBERT-paws-extonly failed
Architectures: ['OPTForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1064, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 761, in __getitem__
    raise KeyError(key)
KeyError: 'exbert'

chrishuber/roberta-kaggledev-testing failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'chrishuber/roberta-kaggledev-testing'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'chrishuber/roberta-kaggledev-testing' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

ml6team/keyphrase-generation-t5-small-inspec failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

google/switch-base-8 failed
Architectures: ['SwitchTransformersForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 1437, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 966, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

google/t5-efficient-tiny-dl6 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

xiaj/test failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/xiaj/test/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656fa0d6-143b7ec466860a220e78aecc;ea5afc38-dc23-4c15-b7fb-adca6868ef3d)

Entry Not Found for url: https://huggingface.co/xiaj/test/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: xiaj/test does not appear to have a file named config.json. Checkout 'https://huggingface.co/xiaj/test/main' for available files.

zhuqing/roberta-base-uncased-exp2-parent failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'zhuqing/roberta-base-uncased-exp2-parent'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'zhuqing/roberta-base-uncased-exp2-parent' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

haritzpuerto/xtremedistil-squad failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'haritzpuerto/xtremedistil-squad'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'haritzpuerto/xtremedistil-squad' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

ylh1013/fintune-ja-chatbot failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


kk4real/t5-small-finetuned-eli5 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

SiriRRR/mt5-small-finetuned-test failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

spacy/da_core_news_md failed
Architectures: ['LEDForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/spacy/da_core_news_md/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656fa10f-0db70b8f338c1549536e03d8;f9f66335-0308-4193-b95e-18ccac674af0)

Entry Not Found for url: https://huggingface.co/spacy/da_core_news_md/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: spacy/da_core_news_md does not appear to have a file named config.json. Checkout 'https://huggingface.co/spacy/da_core_news_md/main' for available files.

AdapterHub/roberta-base-pf-conll2003_pos failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/AdapterHub/roberta-base-pf-conll2003_pos/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656fa116-2869a3967b01bca65d867a0e;f6a62fca-7f0d-412e-a400-8f25ba7bddf6)

Entry Not Found for url: https://huggingface.co/AdapterHub/roberta-base-pf-conll2003_pos/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: AdapterHub/roberta-base-pf-conll2003_pos does not appear to have a file named config.json. Checkout 'https://huggingface.co/AdapterHub/roberta-base-pf-conll2003_pos/main' for available files.

luke-thorburn/suggest-intermediary-claims-soft failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'luke-thorburn/suggest-intermediary-claims-soft'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'luke-thorburn/suggest-intermediary-claims-soft' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

hf-internal-testing/tiny-random-FlaubertWithLMHeadModel failed
Architectures: ['CamembertForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/flaubert/tokenization_flaubert.py", line 262, in __init__
    import sacremoses
ModuleNotFoundError: No module named 'sacremoses'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/flaubert/tokenization_flaubert.py", line 264, in __init__
    raise ImportError(
ImportError: You need to install sacremoses to use FlaubertTokenizer. See https://pypi.org/project/sacremoses/ for installation.

diegozs97/sciie-seed-3-0k failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/sciie-seed-3-0k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/sciie-seed-3-0k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

research-backup/t5-small-subjqa-vanilla-books-qg failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

google/t5-xxl-ssm failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


anas-awadalla/t5-small-few-shot-k-256-finetuned-squad-seed-4 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

baophuc27/tbwt_grammar failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Maelstrom77/rtevib failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Maelstrom77/rtevib'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Maelstrom77/rtevib' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

nbalepur/gpt2_cs_history_lg failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/nbalepur/gpt2_cs_history_lg/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656fa14c-1215f3eb264007ad3a8ceb79;83c255d4-5959-4ccf-b0bc-1df55f7bda3f)

Repository Not Found for url: https://huggingface.co/nbalepur/gpt2_cs_history_lg/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: nbalepur/gpt2_cs_history_lg is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

ryo0634/luke-base-full-20181220 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/ryo0634/luke-base-full-20181220/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656fa14c-3f71debf34c9551b5ea45ffc;2d63bce7-3005-41d3-9180-e70b4fc1ee25)

Repository Not Found for url: https://huggingface.co/ryo0634/luke-base-full-20181220/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: ryo0634/luke-base-full-20181220 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

pritoms/opt-350m-opt-350m-pretrained failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'pritoms/opt-350m-opt-350m-pretrained'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'pritoms/opt-350m-opt-350m-pretrained' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

m3/m3-experiment-albert-base-v2-chemprot-word-swapping-embedding-4 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-chemprot-word-swapping-embedding-4/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656fa16f-351334137ce8786764bd2c33;08613575-cdd5-4bd5-8ed5-d4b32183b78c)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-albert-base-v2-chemprot-word-swapping-embedding-4/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-albert-base-v2-chemprot-word-swapping-embedding-4 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

research-backup/t5-large-subjqa-vanilla-movies-qg failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

baptiste/deberta-finetuned-ner-connll-late-stop failed
Architectures: ['DebertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 31, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 974, in new_func
    encoder_outputs = self.encoder(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 470, in new_func
    hidden_states = layer_module(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 383, in new_func
    attention_output = self.attention(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 316, in new_func
    self_output = self.self(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 661, in new_func
    rel_att = self.disentangled_att_bias(query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 710, in new_func
    c2p_att = torch.gather(c2p_att, dim=-1, index=c2p_dynamic_expand(c2p_pos, query_layer, relative_pos))
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
RuntimeError: c2p_dynamic_expand() Expected a value of type 'Tensor (inferred)' for argument 'c2p_pos' but instead found type 'ConcreteProxy'.
Inferred 'c2p_pos' to be of type 'Tensor' because it was not annotated with an explicit type.
Position: 0
Value: ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]]))
Declaration: c2p_dynamic_expand(Tensor c2p_pos, Tensor query_layer, Tensor relative_pos) -> Tensor
Cast error details: Unable to cast ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]])) to Tensor

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 33, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

allenai/System3_DREAM_FLUTE_motivation_FigLang2022 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'allenai/System3_DREAM_FLUTE_motivation_FigLang2022'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'allenai/System3_DREAM_FLUTE_motivation_FigLang2022' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

Rocketknight1/test_callback_upload failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Rocketknight1/test_callback_upload'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Rocketknight1/test_callback_upload' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

irudnyts/dummy-model failed
Architectures: ['RobertaForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'irudnyts/dummy-model'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'irudnyts/dummy-model' is the correct path to a directory containing all relevant files for a CamembertTokenizerFast tokenizer.

adamlin/distilbert-base-cased-sgd_qa-step5000 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/adamlin/distilbert-base-cased-sgd_qa-step5000/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656fa1a0-16374e3f08e7106b47ed2705;37c50db3-1e1c-495f-a3c3-c828453759a6)

Repository Not Found for url: https://huggingface.co/adamlin/distilbert-base-cased-sgd_qa-step5000/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: adamlin/distilbert-base-cased-sgd_qa-step5000 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

maesneako/ES_corlec_DeepESP-gpt2-spanish failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'maesneako/ES_corlec_DeepESP-gpt2-spanish'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'maesneako/ES_corlec_DeepESP-gpt2-spanish' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

stanfordnlp/stanza-sd failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/stanfordnlp/stanza-sd/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656fa1b0-785ac85616d94c35129967b4;cc180bae-5720-4d83-a085-08844751617b)

Entry Not Found for url: https://huggingface.co/stanfordnlp/stanza-sd/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: stanfordnlp/stanza-sd does not appear to have a file named config.json. Checkout 'https://huggingface.co/stanfordnlp/stanza-sd/main' for available files.

SEBIS/legal_t5_small_finetuned_summ_es failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


SkolkovoInstitute/ruT5-base-detox failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Teeto/test_trainer failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Teeto/test_trainer'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Teeto/test_trainer' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

research-backup/t5-small-squadshifts-vanilla-amazon-qg failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

MartinoMensio/racism-models-raw-label-epoch-3 failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'MartinoMensio/racism-models-raw-label-epoch-3'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'MartinoMensio/racism-models-raw-label-epoch-3' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Jatin-WIAI/tamil_relevance_clf failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Jatin-WIAI/tamil_relevance_clf'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Jatin-WIAI/tamil_relevance_clf' is the correct path to a directory containing all relevant files for a XLMRobertaTokenizerFast tokenizer.

AdapterHub/roberta-base-pf-wic failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/AdapterHub/roberta-base-pf-wic/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656fa1d2-480328696373a851184292e6;aa9b1ea3-b294-4775-87a7-bc261c992b30)

Entry Not Found for url: https://huggingface.co/AdapterHub/roberta-base-pf-wic/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: AdapterHub/roberta-base-pf-wic does not appear to have a file named config.json. Checkout 'https://huggingface.co/AdapterHub/roberta-base-pf-wic/main' for available files.

Sevil/t5-small-finetuned-wikihow_3epoch_v2 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Wikidepia/IndoT5-base failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


mrm8488/t5-base-finetuned-turk-text-simplification failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

charsiu/g2p_multilingual_byT5_tiny_12_layers failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'charsiu/g2p_multilingual_byT5_tiny_12_layers'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'charsiu/g2p_multilingual_byT5_tiny_12_layers' is the correct path to a directory containing all relevant files for a ByT5Tokenizer tokenizer.

ChiefTheLord/imdb-distilbert-base-cased failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/ChiefTheLord/imdb-distilbert-base-cased/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656fa20f-44372dcc04df5f472793b486;bc5aa6d4-42c0-46ef-a984-f6490f4f0a1d)

Repository Not Found for url: https://huggingface.co/ChiefTheLord/imdb-distilbert-base-cased/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: ChiefTheLord/imdb-distilbert-base-cased is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

sirui/bert-base-chinese-finetuned-car_corpus failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'sirui/bert-base-chinese-finetuned-car_corpus'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sirui/bert-base-chinese-finetuned-car_corpus' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

google/t5-efficient-large-dl32 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Jardenna/opus-mt-en-nl-finetuned-en-to-af failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

flexudy/cheapity3 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

anshr/distilgpt2_trained_policy_model_01 failed
Architectures: ['RobertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'anshr/distilgpt2_trained_policy_model_01'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'anshr/distilgpt2_trained_policy_model_01' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

Matthijs/ane-distilbert-test failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 101, in <module>
    model = AutoModel.from_config(config, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 443, in from_config
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers_modules.Matthijs.ane-distilbert-test.e4c33a637122614bcd26939f1236fe87f853faa2.configuration_distilbert_ane.DistilBertConfig'> for this kind of AutoModel: AutoModel.
Model type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, LlamaConfig, CodeGenConfig, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MistralConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SamConfig, SeamlessM4TConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VitDetConfig, VitsConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig.

Jeevesh8/t5-small_re-cogs_14 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

aristotletan/t5-small-finetuned-xsum failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Helsinki-NLP/opus-mt-nyk-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ibraheemmoosa/xlmindic-base-multiscript-soham failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/albert/tokenization_albert_fast.py", line 148, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
AlbertConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Langame/convai-gpt-j-6B-8bit failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Langame/convai-gpt-j-6B-8bit'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Langame/convai-gpt-j-6B-8bit' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

ghadeermobasher/BC4CHEMD-WLT-512-BlueBERT-Trial failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC4CHEMD-WLT-512-BlueBERT-Trial'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC4CHEMD-WLT-512-BlueBERT-Trial' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

VietAI/gpt-j-6B-vietnamese-news failed
Architectures: ['GPTJForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 109, in <module>
    after_trace = traced_gm(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 662, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 281, in __call__
    raise e
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 271, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "<eval_with_key>.2279", line 59, in forward
    getitem_9 = getattr_8[-1];  getattr_8 = None
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_proxy.py", line 431, in impl
    return tracer.create_proxy('call_function', target, args, kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 444, in create_proxy
    value_unwrapped = self.run_target(kind, target, args_unwrapped, kwargs_unwrapped)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 374, in run_target
    result = run(kind, target, args, kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 347, in run
    return OperatorPatcherContext.patch_run(fn, *args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 291, in patch_run
    assert OperatorPatcherContext.ctx_tracer is not None
AssertionError

Chetan1997/layoutlmv2-finetuned-funsd-test failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Chetan1997/layoutlmv2-finetuned-funsd-test'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Chetan1997/layoutlmv2-finetuned-funsd-test' is the correct path to a directory containing all relevant files for a LayoutLMv2TokenizerFast tokenizer.

smartiros/BERT_for_sentiment_5k_2pcs_sampled_airlines_tweets failed
Architectures: ['MBartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'smartiros/BERT_for_sentiment_5k_2pcs_sampled_airlines_tweets'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'smartiros/BERT_for_sentiment_5k_2pcs_sampled_airlines_tweets' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

plum/distilbert-base-cased failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/plum/distilbert-base-cased/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656fa2e4-057592924ce836e87174ec1c;77fbcd1f-3e3e-454c-a0a8-994f6b171d4d)

Repository Not Found for url: https://huggingface.co/plum/distilbert-base-cased/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: plum/distilbert-base-cased is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Declan/CNN_model_v6 failed
Architectures: ['DistilBertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Declan/CNN_model_v6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Declan/CNN_model_v6' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

SEBIS/legal_t5_small_summ_multitask_it failed
Architectures: ['OPTForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


naitian/bert-cola-finetune failed
Architectures: ['OPTForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'naitian/bert-cola-finetune'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'naitian/bert-cola-finetune' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

hadxu/xlm-roberta-base-finetuned-panx-de failed
Architectures: ['OPTForCausalLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/hadxu/xlm-roberta-base-finetuned-panx-de/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656fa34f-73d684003c71a0487aa3e281;d0fa24c7-6e9b-4b7c-b306-0a277a14a882)

Repository Not Found for url: https://huggingface.co/hadxu/xlm-roberta-base-finetuned-panx-de/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: hadxu/xlm-roberta-base-finetuned-panx-de is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

ghadeermobasher/BC4CHEMD-WLT-256-SciBERT-Trial-latest-90 failed
Architectures: ['OPTForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC4CHEMD-WLT-256-SciBERT-Trial-latest-90'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC4CHEMD-WLT-256-SciBERT-Trial-latest-90' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

atatavana/layoutlm_csvmodel_perioli2 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 96, in <module>
    concrete_args = tokenizer(text, return_tensors="pt")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/layoutlmv3/tokenization_layoutlmv3_fast.py", line 299, in __call__
    raise ValueError(
ValueError: Words must be of type `List[str]` (single pretokenized example), or `List[List[str]]` (batch of pretokenized examples).

noah-ai/mt5-base-question-generation-vi failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


ghadeermobasher/BioRED-Chem-WLT-128-BlueBERT-100 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BioRED-Chem-WLT-128-BlueBERT-100'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BioRED-Chem-WLT-128-BlueBERT-100' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

PSW/t5-base-dialogsum-seed36 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

allenai/unifiedqa-t5-3b failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


mbateman/distialbert-base-uncased-finetuned-squad-d5716d28 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'mbateman/distialbert-base-uncased-finetuned-squad-d5716d28'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'mbateman/distialbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

TuhinColumbia/spanishpoetrymany failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mbart/tokenization_mbart_fast.py", line 123, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
MBartConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


elopezlopez/xlnet-base-cased_fold_3_binary_v1 failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlnet/tokenization_xlnet_fast.py", line 150, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLNetConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


sshleifer/distill-mbart-en-ro-12-4 failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mbart/tokenization_mbart_fast.py", line 123, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
MBartConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


lakshaywadhwa1993/mt5-small-finetuned-hindi-mt5 failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

sgugger/test-upload1 failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'sgugger/test-upload1'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sgugger/test-upload1' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

BaptisteDoyen/camembert-base-xnli failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/BaptisteDoyen/camembert-base-xnli/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656fa3b9-62a786c557fa8ff57bc2582b;283eecc9-4476-4842-a1c8-e3bb61b55356)

Repository Not Found for url: https://huggingface.co/BaptisteDoyen/camembert-base-xnli/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: BaptisteDoyen/camembert-base-xnli is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Helsinki-NLP/opus-mt-tc-big-fi-zls failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

monobyte/byt5-mono-nonsense-v1 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

vblagoje/pipeline-generator-v2 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/vblagoje/pipeline-generator-v2/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656fa3ce-1ad438843ddaa5296c0af494;c16aedf8-7220-4fe8-a7d9-ff4ad0fe254c)

Repository Not Found for url: https://huggingface.co/vblagoje/pipeline-generator-v2/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: vblagoje/pipeline-generator-v2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

paola-md/distilr2-lr2e05-wd0.05-bs64 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'paola-md/distilr2-lr2e05-wd0.05-bs64'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'paola-md/distilr2-lr2e05-wd0.05-bs64' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

ArafatBHossain/robert_base_fine_tuned_emotion_dataset failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ArafatBHossain/robert_base_fine_tuned_emotion_dataset'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ArafatBHossain/robert_base_fine_tuned_emotion_dataset' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

MickyMike/11-GPT2SP-mule-mulestudio failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'MickyMike/11-GPT2SP-mule-mulestudio'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'MickyMike/11-GPT2SP-mule-mulestudio' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

spacy/mk_core_news_lg failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/spacy/mk_core_news_lg/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656fa3f9-1d469f0c1a8ea14062c7ed9e;96470065-0f1b-4d31-9164-d9fc5885637c)

Entry Not Found for url: https://huggingface.co/spacy/mk_core_news_lg/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: spacy/mk_core_news_lg does not appear to have a file named config.json. Checkout 'https://huggingface.co/spacy/mk_core_news_lg/main' for available files.

devansh71/ai5_sum_model failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'devansh71/ai5_sum_model'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'devansh71/ai5_sum_model' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.

regel-corpus/hunflair-enhancer failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/regel-corpus/hunflair-enhancer/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656fa402-76e3770509d5246c434099fd;f3f59e1e-6825-4e0a-9e8a-3dc5da19dc4a)

Entry Not Found for url: https://huggingface.co/regel-corpus/hunflair-enhancer/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: regel-corpus/hunflair-enhancer does not appear to have a file named config.json. Checkout 'https://huggingface.co/regel-corpus/hunflair-enhancer/main' for available files.

diegozs97/sciie-seed-1-400k failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/sciie-seed-1-400k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/sciie-seed-1-400k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

mwp/whole-word-absolute-lm-stage1 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/mwp/whole-word-absolute-lm-stage1/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656fa40a-23b4046a748aaf61702a8c6a;9bcca548-35e3-4318-9c24-928852e8533e)

Repository Not Found for url: https://huggingface.co/mwp/whole-word-absolute-lm-stage1/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: mwp/whole-word-absolute-lm-stage1 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

SEBIS/code_trans_t5_small_api_generation_transfer_learning_finetune failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


AnyaSchen/rugpt3_blok failed
Architectures: ['GPT2ForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'AnyaSchen/rugpt3_blok'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'AnyaSchen/rugpt3_blok' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

CennetOguz/distilbert-base-uncased-finetuned-recipe-1 failed
Architectures: ['XLMRobertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'CennetOguz/distilbert-base-uncased-finetuned-recipe-1'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'CennetOguz/distilbert-base-uncased-finetuned-recipe-1' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

MorrisPark/twc-bart-r3f failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: BartModel.forward() got an unexpected keyword argument 'token_type_ids'

limsc/reqbert-tapt-epoch29 failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'limsc/reqbert-tapt-epoch29'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'limsc/reqbert-tapt-epoch29' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

meetyildiz/M-TurQA-bert-base-turkish-128k-cased-finetuned-toqad failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 95, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'meetyildiz/M-TurQA-bert-base-turkish-128k-cased-finetuned-toqad'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'meetyildiz/M-TurQA-bert-base-turkish-128k-cased-finetuned-toqad' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

matheusvolpon/WE4LKD_AML_distilbert_1921_2000 failed
Architectures: ['DistilBertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: DistilBertModel.forward() got an unexpected keyword argument 'token_type_ids'

cjvt/t5-sl-small failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 104, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

google/t5-efficient-base-ff12000 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 119, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

AravindKumarRajendran/t5-small-xsum failed
Architectures: ['MPNetModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/AravindKumarRajendran/t5-small-xsum/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656ff024-03603af251f9b21134779527;be66e48e-ce57-4403-98ed-a2013f6c9b10)

Repository Not Found for url: https://huggingface.co/AravindKumarRajendran/t5-small-xsum/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 108, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: AravindKumarRajendran/t5-small-xsum is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

tner/xlm-roberta-large-uncased-bc5cdr failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 108, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


NbAiLab/nb-gpt-j-6B failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/NbAiLab/nb-gpt-j-6B/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-656ff03a-4b3e781266fe7f474f17ca03;f6743c49-30b3-466b-a45e-861f3a65e291)

Cannot access gated repo for url https://huggingface.co/NbAiLab/nb-gpt-j-6B/resolve/main/tokenizer_config.json.
Repo model NbAiLab/nb-gpt-j-6B is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 108, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 445, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/NbAiLab/nb-gpt-j-6B and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.

tartuNLP/mtee-crisis failed
Architectures: ['BartForConditionalGeneration']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/tartuNLP/mtee-crisis/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656ff04f-3ee293ca5cba462a0370f125;25cd6aff-8248-47c4-8c91-2ecef71de4c5)

Entry Not Found for url: https://huggingface.co/tartuNLP/mtee-crisis/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 108, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: tartuNLP/mtee-crisis does not appear to have a file named config.json. Checkout 'https://huggingface.co/tartuNLP/mtee-crisis/main' for available files.

Helsinki-NLP/opus-mt-es-ca failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 119, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

CEBaB/roberta-base.CEBaB.sa.3-class.exclusive.seed_42 failed
Architectures: ['XLMRobertaForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 108, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'CEBaB/roberta-base.CEBaB.sa.3-class.exclusive.seed_42'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'CEBaB/roberta-base.CEBaB.sa.3-class.exclusive.seed_42' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

parvezmrobin/bugsplainer-t5 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 119, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ghadeermobasher/BioRED-Dis-WLT-128-BlueBERT-40 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 108, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BioRED-Dis-WLT-128-BlueBERT-40'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BioRED-Dis-WLT-128-BlueBERT-40' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-yo-sv failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 119, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

SEBIS/legal_t5_small_trans_sv_it failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 108, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


stanfordnlp/stanza-ro failed
Architectures: ['BertForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/stanfordnlp/stanza-ro/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656ff096-117ec12d7c35142d648cf522;26a9cdf7-30a4-4795-a867-8168060146c9)

Entry Not Found for url: https://huggingface.co/stanfordnlp/stanza-ro/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 108, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: stanfordnlp/stanza-ro does not appear to have a file named config.json. Checkout 'https://huggingface.co/stanfordnlp/stanza-ro/main' for available files.

Vilnius-Lithuania-iGEM/Albumin failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 108, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Vilnius-Lithuania-iGEM/Albumin'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Vilnius-Lithuania-iGEM/Albumin' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

ikerHerrero/Basque_Dialects_Classification failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 108, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


valhalla/t5-base-qg-hl failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 108, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


google/pegasus-big_patent failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 108, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/pegasus/tokenization_pegasus_fast.py", line 147, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
PegasusConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


tbochens/dummy-model failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 108, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'tbochens/dummy-model'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'tbochens/dummy-model' is the correct path to a directory containing all relevant files for a CamembertTokenizerFast tokenizer.

thivy/flan-t5-base-finetuned-en-to-no-test failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 119, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

redwoodresearch/classifier_12aug_50k_labels failed
Architectures: ['DebertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 38, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 974, in new_func
    encoder_outputs = self.encoder(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 470, in new_func
    hidden_states = layer_module(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 383, in new_func
    attention_output = self.attention(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 316, in new_func
    self_output = self.self(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 661, in new_func
    rel_att = self.disentangled_att_bias(query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 710, in new_func
    c2p_att = torch.gather(c2p_att, dim=-1, index=c2p_dynamic_expand(c2p_pos, query_layer, relative_pos))
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
RuntimeError: c2p_dynamic_expand() Expected a value of type 'Tensor (inferred)' for argument 'c2p_pos' but instead found type 'ConcreteProxy'.
Inferred 'c2p_pos' to be of type 'Tensor' because it was not annotated with an explicit type.
Position: 0
Value: ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]]))
Declaration: c2p_dynamic_expand(Tensor c2p_pos, Tensor query_layer, Tensor relative_pos) -> Tensor
Cast error details: Unable to cast ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]])) to Tensor

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 120, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 40, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

iis2009002/distilbert-base-uncased-finetuned-emotion failed
Architectures: ['DebertaForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'iis2009002/distilbert-base-uncased-finetuned-emotion'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'iis2009002/distilbert-base-uncased-finetuned-emotion' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

michaelrglass/albert-base-rci-wtq-row failed
Architectures: ['CamembertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2056, in _from_pretrained
    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/albert/tokenization_albert.py", line 168, in __init__
    self.sp_model.Load(vocab_file)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/sentencepiece/__init__.py", line 905, in Load
    return self.LoadFromFile(model_file)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/sentencepiece/__init__.py", line 310, in LoadFromFile
    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg)
TypeError: not a string

ghadeermobasher/BC5CDR-chem-WLT-384-SciBERT failed
Architectures: ['CamembertForMaskedLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC5CDR-chem-WLT-384-SciBERT'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC5CDR-chem-WLT-384-SciBERT' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

rovai/chatbotone failed
Architectures: ['RobertaForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/rovai/chatbotone/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656ff126-1c6518c04c6844d4137af2da;b255d11e-dfcb-4fb0-b3e1-6a08c5c0da02)

Entry Not Found for url: https://huggingface.co/rovai/chatbotone/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: rovai/chatbotone does not appear to have a file named config.json. Checkout 'https://huggingface.co/rovai/chatbotone/main' for available files.

SEBIS/legal_t5_small_finetuned_summ_fr failed
Architectures: ['BertForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


cm-mueller/BACnet-Klassifizierung-Gewerke-wd20-ah2-hl2-do04 failed
Architectures: ['GPT2LMHeadModel']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/cm-mueller/BACnet-Klassifizierung-Gewerke-wd20-ah2-hl2-do04/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656ff140-1856f6b06fb5b44b476ad48d;5f883cae-e786-419f-8d6d-89154aa21c2d)

Repository Not Found for url: https://huggingface.co/cm-mueller/BACnet-Klassifizierung-Gewerke-wd20-ah2-hl2-do04/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: cm-mueller/BACnet-Klassifizierung-Gewerke-wd20-ah2-hl2-do04 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

aditya2029/gpt-neo-genre-storygenerator failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'aditya2029/gpt-neo-genre-storygenerator'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'aditya2029/gpt-neo-genre-storygenerator' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

prikarsartam/Chatelet failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/prikarsartam/Chatelet/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-656ff144-597a04cf653c473219c1a712;bc8cc010-a9e3-4892-889b-acff41b944c4)

Cannot access gated repo for url https://huggingface.co/prikarsartam/Chatelet/resolve/main/tokenizer_config.json.
Repo model prikarsartam/Chatelet is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 445, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/prikarsartam/Chatelet and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.

jaimin/Active_to_passive failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 117, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

tner/xlm-roberta-large-panx-dataset-es failed
Architectures: ['BloomForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Helsinki-NLP/opus-mt-pl-es failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 117, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

stevemobs/deberta-base-combined-squad1-aqa-1epoch-and-newsqa-2epoch failed
Architectures: ['DebertaForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 38, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 974, in new_func
    encoder_outputs = self.encoder(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 470, in new_func
    hidden_states = layer_module(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 383, in new_func
    attention_output = self.attention(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 316, in new_func
    self_output = self.self(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 661, in new_func
    rel_att = self.disentangled_att_bias(query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 710, in new_func
    c2p_att = torch.gather(c2p_att, dim=-1, index=c2p_dynamic_expand(c2p_pos, query_layer, relative_pos))
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
RuntimeError: c2p_dynamic_expand() Expected a value of type 'Tensor (inferred)' for argument 'c2p_pos' but instead found type 'ConcreteProxy'.
Inferred 'c2p_pos' to be of type 'Tensor' because it was not annotated with an explicit type.
Position: 0
Value: ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]]))
Declaration: c2p_dynamic_expand(Tensor c2p_pos, Tensor query_layer, Tensor relative_pos) -> Tensor
Cast error details: Unable to cast ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]])) to Tensor

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 120, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 40, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

Kyungill/marian-finetuned-kde4-en-to-fr failed
Architectures: ['RobertaForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/Kyungill/marian-finetuned-kde4-en-to-fr/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656ff16c-1d2f71255bb7b6ce736e6dfa;7951076a-6cb1-4d1f-9f08-287efeca03ef)

Repository Not Found for url: https://huggingface.co/Kyungill/marian-finetuned-kde4-en-to-fr/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: Kyungill/marian-finetuned-kde4-en-to-fr is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

DeskDown/MarianMixFT_en-ms failed
Architectures: ['GPT2ForQuestionAnswering']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 789, in from_pretrained
    return tokenizer_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'DeskDown/MarianMixFT_en-ms'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'DeskDown/MarianMixFT_en-ms' is the correct path to a directory containing all relevant files for a MarianTokenizer tokenizer.

lilouuch/t5-base-finetuned-xsum_epoch4 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/lilouuch/t5-base-finetuned-xsum_epoch4/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656ff189-681aed3577e3773966d29146;8cd0c87d-9df0-467b-a064-1e20ffff4389)

Repository Not Found for url: https://huggingface.co/lilouuch/t5-base-finetuned-xsum_epoch4/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: lilouuch/t5-base-finetuned-xsum_epoch4 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

Ammar-alhaj-ali/LayoutLMv3-Fine-Tuning-FUNSD failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 107, in <module>
    concrete_args = tokenizer(text, return_tensors="pt")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/layoutlmv3/tokenization_layoutlmv3_fast.py", line 299, in __call__
    raise ValueError(
ValueError: Words must be of type `List[str]` (single pretokenized example), or `List[List[str]]` (batch of pretokenized examples).

liux3790/autotrain-journals-covid-990032813 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/liux3790/autotrain-journals-covid-990032813/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-656ff18a-0619d9ec350fba89594e4a5f;7ffc0fa8-a051-462c-91af-a7523b49b4a3)

Cannot access gated repo for url https://huggingface.co/liux3790/autotrain-journals-covid-990032813/resolve/main/tokenizer_config.json.
Repo model liux3790/autotrain-journals-covid-990032813 is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 445, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/liux3790/autotrain-journals-covid-990032813 and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.

Alireza1044/MobileBERT_Theseus-qqp failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Alireza1044/MobileBERT_Theseus-qqp'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Alireza1044/MobileBERT_Theseus-qqp' is the correct path to a directory containing all relevant files for a MobileBertTokenizerFast tokenizer.

dbsamu/deberta-base-finetuned-ner failed
Architectures: ['DebertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 38, in concrete_trace_wrap
    traced_gm = to_fx_graph(model, concrete_args)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/converter.py", line 55, in to_fx_graph
    traced_model = concrete_trace(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1708, in concrete_trace
    graph = tracer.trace(root,
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 1141, in trace
    results = OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 974, in new_func
    encoder_outputs = self.encoder(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 470, in new_func
    hidden_states = layer_module(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 383, in new_func
    attention_output = self.attention(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 316, in new_func
    self_output = self.self(
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 661, in new_func
    rel_att = self.disentangled_att_bias(query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 710, in new_func
    c2p_att = torch.gather(c2p_att, dim=-1, index=c2p_dynamic_expand(c2p_pos, query_layer, relative_pos))
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 295, in patch_run
    return new_func(*args, **kwargs)
RuntimeError: c2p_dynamic_expand() Expected a value of type 'Tensor (inferred)' for argument 'c2p_pos' but instead found type 'ConcreteProxy'.
Inferred 'c2p_pos' to be of type 'Tensor' because it was not annotated with an explicit type.
Position: 0
Value: ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]]))
Declaration: c2p_dynamic_expand(Tensor c2p_pos, Tensor query_layer, Tensor relative_pos) -> Tensor
Cast error details: Unable to cast ConcreteProxy(clamp, tensor([[[[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],
          [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],
          [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],
          [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],
          [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],
          [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],
          [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],
          [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],
          [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],
          [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
          [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]]]])) to Tensor

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 120, in <module>
    traced_gm = concrete_trace_wrap(model, concrete_args)
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 40, in concrete_trace_wrap
    raise Exception("Failed to trace with gpu")
Exception: Failed to trace with gpu

Jiexing/relation_t5_small failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 117, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Danastos/newsqa_bert_el_4 failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/Danastos/newsqa_bert_el_4/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656ff1a1-2efb79c626b568775a13bf05;2f2640a7-e14a-4b7c-9042-105612cac318)

Repository Not Found for url: https://huggingface.co/Danastos/newsqa_bert_el_4/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: Danastos/newsqa_bert_el_4 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

p-christ/text2text_12345 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 117, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

taoroalin/classifier_12aug_50k_labels failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'taoroalin/classifier_12aug_50k_labels'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'taoroalin/classifier_12aug_50k_labels' is the correct path to a directory containing all relevant files for a DebertaTokenizerFast tokenizer.

Helsinki-NLP/opus-mt-wa-en failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 117, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

masakhane/afribyt5_en_twi_news failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 117, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

mattymchen/nli-synthesizer-t5-base failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 117, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

eslamxm/AraT5-base-title-generation-finetune-ar-xlsum failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 117, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Declan/FoxNews_model_v3 failed
Architectures: ['DistilBertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Declan/FoxNews_model_v3'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Declan/FoxNews_model_v3' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

kevinbror/distilbertbaseuncasedz failed
Architectures: ['XLMRobertaForTokenClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'kevinbror/distilbertbaseuncasedz'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'kevinbror/distilbertbaseuncasedz' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

IIC/mt5-base-lfqa-es failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 117, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Barkavi/totto-t5-base-bleurt-121K failed
Architectures: ['DistilBertForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/Barkavi/totto-t5-base-bleurt-121K/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656ff25f-53f3a8771e4d69d54f4c4606;d1dc8bfb-52f4-4a18-92d5-dd736cf5bc4c)

Repository Not Found for url: https://huggingface.co/Barkavi/totto-t5-base-bleurt-121K/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: Barkavi/totto-t5-base-bleurt-121K is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

research-backup/t5-base-subjqa-vanilla-movies-qg failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 117, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

anas-awadalla/t5-base-few-shot-k-32-finetuned-squad-infilling-seed-2 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 117, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

danyaljj/gpt-j-6B-step-378500 failed
Architectures: ['GPTJForCausalLM']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 124, in <module>
    after_trace = traced_gm(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 662, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 281, in __call__
    raise e
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/fx/graph_module.py", line 271, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "<eval_with_key>.101", line 59, in forward
    getitem_9 = getattr_8[-1];  getattr_8 = None
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_proxy.py", line 431, in impl
    return tracer.create_proxy('call_function', target, args, kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 444, in create_proxy
    value_unwrapped = self.run_target(kind, target, args_unwrapped, kwargs_unwrapped)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 374, in run_target
    result = run(kind, target, args, kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/concrete_tracer.py", line 347, in run
    return OperatorPatcherContext.patch_run(fn, *args, **kwargs)
  File "/home/yileiyang/workspace/MagicCube/cube/graph/parser/fx/concrete_trace_utils/operator_patcher.py", line 291, in patch_run
    assert OperatorPatcherContext.ctx_tracer is not None
AssertionError

efederici/sentence-BERTino-3-64 failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/efederici/sentence-BERTino-3-64/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656ff333-183de2153a70ade20c44db66;9b7a5c2c-10ed-4157-895d-5da947b2ec7a)

Entry Not Found for url: https://huggingface.co/efederici/sentence-BERTino-3-64/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: efederici/sentence-BERTino-3-64 does not appear to have a file named config.json. Checkout 'https://huggingface.co/efederici/sentence-BERTino-3-64/main' for available files.

Taramiko/DialoGPT-small-hoshiyo_kojima failed
Architectures: ['BertForMaskedLM']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/Taramiko/DialoGPT-small-hoshiyo_kojima/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656ff333-5c72d61c67e0dc6f7a33674a;f77f99cf-7e70-4616-95a8-07e9808e8758)

Entry Not Found for url: https://huggingface.co/Taramiko/DialoGPT-small-hoshiyo_kojima/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: Taramiko/DialoGPT-small-hoshiyo_kojima does not appear to have a file named config.json. Checkout 'https://huggingface.co/Taramiko/DialoGPT-small-hoshiyo_kojima/main' for available files.

nestoralvaro/mt5-base-finetuned-xsum-RAW_data_prep_2021_12_26___t22027_162754.csv__google_mt5_base failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 117, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

yihsuan/mt5_chinese_small failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 117, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 1515, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py", line 984, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ghadeermobasher/BioRED-Dis-WLT-128-BlueBERT-100 failed
Architectures: ['MT5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BioRED-Dis-WLT-128-BlueBERT-100'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BioRED-Dis-WLT-128-BlueBERT-100' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Meowren/DialoGPT-small-Rick-Bot failed
Architectures: ['ElectraForQuestionAnswering']
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/Meowren/DialoGPT-small-Rick-Bot/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 280, in hf_raise_for_status
    raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-656ff389-3319fec87f2681bc7b3b7b99;adaccff7-d6bb-4e15-9469-45b4d9b9b510)

Entry Not Found for url: https://huggingface.co/Meowren/DialoGPT-small-Rick-Bot/resolve/main/config.json.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_file
    raise EnvironmentError(
OSError: Meowren/DialoGPT-small-Rick-Bot does not appear to have a file named config.json. Checkout 'https://huggingface.co/Meowren/DialoGPT-small-Rick-Bot/main' for available files.

JUNEYEOB/FT_adafactor_lr6_lcs_lr6 failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'JUNEYEOB/FT_adafactor_lr6_lcs_lr6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'JUNEYEOB/FT_adafactor_lr6_lcs_lr6' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

Jean-Baptiste/camembert-ner failed
Architectures: ['BertForSequenceClassification']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/camembert/tokenization_camembert_fast.py", line 127, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
CamembertConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Helsinki-NLP/opus-mt-fiu-fiu failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 117, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

mrm8488/deberta-v3-base-goemotions failed
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py", line 133, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
DebertaV2Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


CarperAI/FIM-NeoX-1.3B failed
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 449, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 444, in _make_request
    httplib_response = conn.getresponse()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 1374, in getresponse
    response.begin()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 318, in begin
    version, status, reason = self._read_status()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 279, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/anaconda/envs/amc/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1274, in recv_into
    return self.read(nbytes, buffer)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1130, in read
    return self._sslobj.read(len, buffer)
TimeoutError: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/util/retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/packages/six.py", line 770, in reraise
    raise value
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 451, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 340, in _raise_timeout
    raise ReadTimeoutError(
urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 425, in _request_wrapper
    response = get_session().request(method=method, url=url, **params)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 63, in send
    return super().send(request, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 532, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: (ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 28f71854-7204-443b-90ce-281f8182c005)')

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1377, in hf_hub_download
    raise LocalEntryNotFoundError(
huggingface_hub.utils._errors.LocalEntryNotFoundError: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 470, in cached_file
    raise EnvironmentError(
OSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like CarperAI/FIM-NeoX-1.3B is not the path to a directory containing a file named config.json.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.

ByungjunKim/distilbert-base-uncased-finetuned-imdb failed
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ByungjunKim/distilbert-base-uncased-finetuned-imdb'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ByungjunKim/distilbert-base-uncased-finetuned-imdb' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

Gozdi/t5-efficient-small-nl16-samsum-exp1 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 117, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Mascariddu8/posText_targetParagraphs_clickbait_model failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Mascariddu8/posText_targetParagraphs_clickbait_model'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Mascariddu8/posText_targetParagraphs_clickbait_model' is the correct path to a directory containing all relevant files for a BigBirdTokenizerFast tokenizer.

yliu337/noun_chunker failed
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py", line 155, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
XLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


paola-md/recipe-lr2e05-wd0.1-bs8 failed
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'paola-md/recipe-lr2e05-wd0.1-bs8'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'paola-md/recipe-lr2e05-wd0.1-bs8' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

Suva/uptag-email-model-v2 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 117, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Jeevesh8/bert-base-uncased_mnli_ft_61 failed
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 449, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 444, in _make_request
    httplib_response = conn.getresponse()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 1374, in getresponse
    response.begin()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 318, in begin
    version, status, reason = self._read_status()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 279, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/anaconda/envs/amc/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1274, in recv_into
    return self.read(nbytes, buffer)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1130, in read
    return self._sslobj.read(len, buffer)
TimeoutError: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/util/retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/packages/six.py", line 770, in reraise
    raise value
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 451, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 340, in _raise_timeout
    raise ReadTimeoutError(
urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 1983, in from_pretrained
    resolved_vocab_files[file_id] = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1461, in hf_hub_download
    http_get(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 468, in http_get
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 425, in _request_wrapper
    response = get_session().request(method=method, url=url, **params)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 63, in send
    return super().send(request, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 532, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: (ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: a184b376-8735-4c3e-a148-7f1c758e6031)')

m3/m3-experiment-roberta-base-rct-sample-add-v3 failed
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-roberta-base-rct-sample-add-v3/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656ff604-58ce721b73946cef780d2256;e3637272-16f1-4f24-b041-00c69ddf33cc)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-roberta-base-rct-sample-add-v3/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-roberta-base-rct-sample-add-v3 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

huggingtweets/yarbsalocin failed
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 449, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 444, in _make_request
    httplib_response = conn.getresponse()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 1374, in getresponse
    response.begin()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 318, in begin
    version, status, reason = self._read_status()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 279, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/anaconda/envs/amc/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1274, in recv_into
    return self.read(nbytes, buffer)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1130, in read
    return self._sslobj.read(len, buffer)
TimeoutError: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/util/retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/packages/six.py", line 770, in reraise
    raise value
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 451, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 340, in _raise_timeout
    raise ReadTimeoutError(
urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 1983, in from_pretrained
    resolved_vocab_files[file_id] = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1461, in hf_hub_download
    http_get(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 468, in http_get
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 425, in _request_wrapper
    response = get_session().request(method=method, url=url, **params)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 63, in send
    return super().send(request, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 532, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: (ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 95340ebe-f623-4597-a128-021f5642e78c)')

imosnoi/rofactura_4 failed
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'imosnoi/rofactura_4'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'imosnoi/rofactura_4' is the correct path to a directory containing all relevant files for a LayoutLMTokenizerFast tokenizer.

AndrewZeng/S2KG-base failed
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py", line 135, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
T5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


xma/gptj-small-train-test failed
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'xma/gptj-small-train-test'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'xma/gptj-small-train-test' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.

mrm8488/roberta-large-finetuned-wsc failed
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2056, in _from_pretrained
    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/roberta/tokenization_roberta.py", line 227, in __init__
    with open(merges_file, encoding="utf-8") as merges_handle:
TypeError: expected str, bytes or os.PathLike object, not NoneType

Declan/FoxNews_model_v1 failed
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'Declan/FoxNews_model_v1'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Declan/FoxNews_model_v1' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

moyix/codegen-2B-multi-gptj failed
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'moyix/codegen-2B-multi-gptj'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'moyix/codegen-2B-multi-gptj' is the correct path to a directory containing all relevant files for a CodeGenTokenizerFast tokenizer.

negfir/bert_uncased_L-2_H-128_A-2wiki103 failed
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'negfir/bert_uncased_L-2_H-128_A-2wiki103'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'negfir/bert_uncased_L-2_H-128_A-2wiki103' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

projecte-aina/m2m100-418M-ft-de-ca failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 117, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1181, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 966, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

ricardo-filho/bert-base-portuguese-cased-finetuned-ner failed
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/ricardo-filho/bert-base-portuguese-cased-finetuned-ner/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656ff707-43e33e425c4ee949556437e1;36a812e1-d18c-4107-9e8d-23a78b4b0a44)

Repository Not Found for url: https://huggingface.co/ricardo-filho/bert-base-portuguese-cased-finetuned-ner/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: ricardo-filho/bert-base-portuguese-cased-finetuned-ner is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

PascalNotin/Tranception_Large failed
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'PascalNotin/Tranception_Large'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'PascalNotin/Tranception_Large' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

alirezamsh/small100 failed
Architectures: ['M2M100ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 117, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1181, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 966, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

mideind/yfirlestur-icelandic-correction-byt5 failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 117, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

Xuan-Rui/pet-100-p1 failed
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 449, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 444, in _make_request
    httplib_response = conn.getresponse()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 1374, in getresponse
    response.begin()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 318, in begin
    version, status, reason = self._read_status()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 279, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/anaconda/envs/amc/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1274, in recv_into
    return self.read(nbytes, buffer)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1130, in read
    return self._sslobj.read(len, buffer)
TimeoutError: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/util/retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/packages/six.py", line 770, in reraise
    raise value
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 451, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 340, in _raise_timeout
    raise ReadTimeoutError(
urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 1983, in from_pretrained
    resolved_vocab_files[file_id] = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1461, in hf_hub_download
    http_get(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 468, in http_get
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 425, in _request_wrapper
    response = get_session().request(method=method, url=url, **params)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 63, in send
    return super().send(request, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 532, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: (ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: e8d200cd-ca44-490d-a3ec-a7532b1c0cae)')

aXhyra/hate_trained_final failed
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 449, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 444, in _make_request
    httplib_response = conn.getresponse()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 1374, in getresponse
    response.begin()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 318, in begin
    version, status, reason = self._read_status()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 279, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/anaconda/envs/amc/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1274, in recv_into
    return self.read(nbytes, buffer)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1130, in read
    return self._sslobj.read(len, buffer)
TimeoutError: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/util/retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/packages/six.py", line 770, in reraise
    raise value
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 451, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 340, in _raise_timeout
    raise ReadTimeoutError(
urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 1983, in from_pretrained
    resolved_vocab_files[file_id] = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1461, in hf_hub_download
    http_get(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 468, in http_get
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 425, in _request_wrapper
    response = get_session().request(method=method, url=url, **params)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 63, in send
    return super().send(request, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 532, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: (ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: a0627225-7f69-4af2-bb41-94305f4e041d)')

huggingtweets/realdonaldtrump failed
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 449, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 444, in _make_request
    httplib_response = conn.getresponse()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 1374, in getresponse
    response.begin()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 318, in begin
    version, status, reason = self._read_status()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 279, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/anaconda/envs/amc/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1274, in recv_into
    return self.read(nbytes, buffer)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1130, in read
    return self._sslobj.read(len, buffer)
TimeoutError: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/util/retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/packages/six.py", line 770, in reraise
    raise value
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 451, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 340, in _raise_timeout
    raise ReadTimeoutError(
urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 1983, in from_pretrained
    resolved_vocab_files[file_id] = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1461, in hf_hub_download
    http_get(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 468, in http_get
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 425, in _request_wrapper
    response = get_session().request(method=method, url=url, **params)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 63, in send
    return super().send(request, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 532, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: (ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: cbb836c4-bbd2-4f52-a49d-41d0e3fa0491)')

mobashgr/NCBI-disease-Original-384-SciBERT failed
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/mobashgr/NCBI-disease-Original-384-SciBERT/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656ff7dc-0c32d0c8526638bd1268c387;716a125c-7141-419a-9a08-f52a05cc8fe4)

Repository Not Found for url: https://huggingface.co/mobashgr/NCBI-disease-Original-384-SciBERT/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: mobashgr/NCBI-disease-Original-384-SciBERT is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

huggingtweets/modus_irrumandi failed
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 449, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 444, in _make_request
    httplib_response = conn.getresponse()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 1374, in getresponse
    response.begin()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 318, in begin
    version, status, reason = self._read_status()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 279, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/anaconda/envs/amc/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1274, in recv_into
    return self.read(nbytes, buffer)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1130, in read
    return self._sslobj.read(len, buffer)
TimeoutError: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/util/retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/packages/six.py", line 770, in reraise
    raise value
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 451, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 340, in _raise_timeout
    raise ReadTimeoutError(
urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 106, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 1947, in from_pretrained
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1461, in hf_hub_download
    http_get(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 468, in http_get
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 425, in _request_wrapper
    response = get_session().request(method=method, url=url, **params)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 63, in send
    return super().send(request, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 532, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: (ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 017ff5c6-0505-4dd1-91a9-1eab50c5aa10)')

CEBaB/lstm.CEBaB.sa.3-class.exclusive.seed_77 failed
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 107, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'CEBaB/lstm.CEBaB.sa.3-class.exclusive.seed_77'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'CEBaB/lstm.CEBaB.sa.3-class.exclusive.seed_77' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

mboth/distil-eng failed
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 449, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 444, in _make_request
    httplib_response = conn.getresponse()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 1374, in getresponse
    response.begin()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 318, in begin
    version, status, reason = self._read_status()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 279, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/anaconda/envs/amc/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1274, in recv_into
    return self.read(nbytes, buffer)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1130, in read
    return self._sslobj.read(len, buffer)
TimeoutError: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/util/retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/packages/six.py", line 770, in reraise
    raise value
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 451, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 340, in _raise_timeout
    raise ReadTimeoutError(
urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 113, in <module>
    config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1461, in hf_hub_download
    http_get(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 468, in http_get
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 425, in _request_wrapper
    response = get_session().request(method=method, url=url, **params)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 63, in send
    return super().send(request, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 532, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: (ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 8b8becfd-f7bf-474c-82f9-cecb5c056309)')

mobashgr/BioRED-Dis-WLT-128-BlueBERT-isns failed
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 449, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 444, in _make_request
    httplib_response = conn.getresponse()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 1374, in getresponse
    response.begin()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 318, in begin
    version, status, reason = self._read_status()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 279, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/anaconda/envs/amc/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1274, in recv_into
    return self.read(nbytes, buffer)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1130, in read
    return self._sslobj.read(len, buffer)
TimeoutError: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/util/retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/packages/six.py", line 770, in reraise
    raise value
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 451, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 340, in _raise_timeout
    raise ReadTimeoutError(
urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 107, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 1983, in from_pretrained
    resolved_vocab_files[file_id] = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1461, in hf_hub_download
    http_get(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 468, in http_get
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 425, in _request_wrapper
    response = get_session().request(method=method, url=url, **params)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 63, in send
    return super().send(request, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 532, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: (ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 68848978-fa9a-46bd-bdd4-1d8ca752c657)')

Helsinki-NLP/opus-mt-alv-en failed
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 107, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 789, in from_pretrained
    return tokenizer_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py", line 150, in __init__
    self.encoder = load_json(vocab)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py", line 412, in load_json
    with open(path, "r") as f:
TypeError: expected str, bytes or os.PathLike object, not NoneType

j0hngou/t5-base-finetuned-en-to-fr failed
Architectures: ['T5ForConditionalGeneration']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 118, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1521, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1011, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

DTAI-KULeuven/robbert-2022-dutch-base failed
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 449, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 444, in _make_request
    httplib_response = conn.getresponse()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 1374, in getresponse
    response.begin()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 318, in begin
    version, status, reason = self._read_status()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 279, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/anaconda/envs/amc/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1274, in recv_into
    return self.read(nbytes, buffer)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1130, in read
    return self._sslobj.read(len, buffer)
TimeoutError: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/util/retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/packages/six.py", line 770, in reraise
    raise value
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 451, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 340, in _raise_timeout
    raise ReadTimeoutError(
urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 107, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 1983, in from_pretrained
    resolved_vocab_files[file_id] = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1461, in hf_hub_download
    http_get(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 468, in http_get
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 425, in _request_wrapper
    response = get_session().request(method=method, url=url, **params)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 63, in send
    return super().send(request, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 532, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: (ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 8d9a1ecd-7a0a-4689-9830-6873333bd8bc)')

masakhane/m2m100_418M_en_tsn_news failed
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 107, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/tokenization_m2m_100.py", line 160, in __init__
    self.encoder = load_json(vocab_file)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/m2m_100/tokenization_m2m_100.py", line 392, in load_json
    with open(path, "r") as f:
TypeError: expected str, bytes or os.PathLike object, not NoneType

pritoms/gpt-neo-125M-Byethon failed
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 107, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'pritoms/gpt-neo-125M-Byethon'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'pritoms/gpt-neo-125M-Byethon' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.

diegozs97/finetuned-chemprot-seed-0-60k failed
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 107, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/finetuned-chemprot-seed-0-60k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/finetuned-chemprot-seed-0-60k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

anas-awadalla/roberta-large-data-seed-2 failed
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 449, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 444, in _make_request
    httplib_response = conn.getresponse()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 1374, in getresponse
    response.begin()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 318, in begin
    version, status, reason = self._read_status()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 279, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/anaconda/envs/amc/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1274, in recv_into
    return self.read(nbytes, buffer)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1130, in read
    return self._sslobj.read(len, buffer)
TimeoutError: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/util/retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/packages/six.py", line 770, in reraise
    raise value
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 451, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 340, in _raise_timeout
    raise ReadTimeoutError(
urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 107, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 1983, in from_pretrained
    resolved_vocab_files[file_id] = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1461, in hf_hub_download
    http_get(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 468, in http_get
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 425, in _request_wrapper
    response = get_session().request(method=method, url=url, **params)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 63, in send
    return super().send(request, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 532, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: (ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 104b3aaf-58c9-49cd-a987-76883b5845c2)')

TehranNLP-org/bert-base-uncased-mrpc-2e-5-42 failed
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 449, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 444, in _make_request
    httplib_response = conn.getresponse()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 1374, in getresponse
    response.begin()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 318, in begin
    version, status, reason = self._read_status()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 279, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/anaconda/envs/amc/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1274, in recv_into
    return self.read(nbytes, buffer)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1130, in read
    return self._sslobj.read(len, buffer)
TimeoutError: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/util/retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/packages/six.py", line 770, in reraise
    raise value
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 451, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 340, in _raise_timeout
    raise ReadTimeoutError(
urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 107, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 1947, in from_pretrained
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1461, in hf_hub_download
    http_get(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 468, in http_get
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 425, in _request_wrapper
    response = get_session().request(method=method, url=url, **params)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 63, in send
    return super().send(request, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 532, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: (ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: d7e5afc2-a3a2-4c6d-907c-8cc432fc4164)')

m3/m3-experiment-roberta-base-citation-intent-word-swapping-random-3 failed
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/m3/m3-experiment-roberta-base-citation-intent-word-swapping-random-3/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656ff975-68f65ecf6d9975a16cacf239;8d5c4e4e-259d-464f-9cc5-57221a46dc3e)

Repository Not Found for url: https://huggingface.co/m3/m3-experiment-roberta-base-citation-intent-word-swapping-random-3/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 107, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: m3/m3-experiment-roberta-base-citation-intent-word-swapping-random-3 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

diegozs97/finetuned-sciie-seed-1-2000k failed
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 107, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'diegozs97/finetuned-sciie-seed-1-2000k'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'diegozs97/finetuned-sciie-seed-1-2000k' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.

robb17/XLNet-finetuned-sentiment-analysis failed
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/robb17/XLNet-finetuned-sentiment-analysis/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656ff9f9-73c0f462102fbfe047eda4e4;1fe47fc8-9209-4c32-98ba-46aba84868bc)

Repository Not Found for url: https://huggingface.co/robb17/XLNet-finetuned-sentiment-analysis/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 107, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: robb17/XLNet-finetuned-sentiment-analysis is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

AdapterHub/bert-base-uncased-pf-sick failed
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 449, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 444, in _make_request
    httplib_response = conn.getresponse()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 1374, in getresponse
    response.begin()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 318, in begin
    version, status, reason = self._read_status()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 279, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/anaconda/envs/amc/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1274, in recv_into
    return self.read(nbytes, buffer)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1130, in read
    return self._sslobj.read(len, buffer)
TimeoutError: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/util/retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/packages/six.py", line 770, in reraise
    raise value
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 451, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 340, in _raise_timeout
    raise ReadTimeoutError(
urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 425, in _request_wrapper
    response = get_session().request(method=method, url=url, **params)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 63, in send
    return super().send(request, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 532, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: (ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: f5e9c813-fea6-4504-bb3f-eca426d8cef6)')

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1377, in hf_hub_download
    raise LocalEntryNotFoundError(
huggingface_hub.utils._errors.LocalEntryNotFoundError: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 107, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 470, in cached_file
    raise EnvironmentError(
OSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like AdapterHub/bert-base-uncased-pf-sick is not the path to a directory containing a file named config.json.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.

philschmid/my-onnx-repo failed
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 113, in <module>
    config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1073, in from_pretrained
    raise ValueError(
ValueError: Unrecognized model in philschmid/my-onnx-repo. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chinese_clip, clap, clip, clipseg, code_llama, codegen, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, data2vec-audio, data2vec-text, data2vec-vision, deberta, deberta-v2, decision_transformer, deformable_detr, deit, deta, detr, dinat, dinov2, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, git, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, graphormer, groupvit, hubert, ibert, idefics, imagegpt, informer, instructblip, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, longformer, longt5, luke, lxmert, m2m_100, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mistral, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, mpnet, mpt, mra, mt5, musicgen, mvp, nat, nezha, nllb-moe, nougat, nystromformer, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, pegasus, pegasus_x, perceiver, persimmon, pix2struct, plbart, poolformer, pop2piano, prophetnet, pvt, qdqbert, rag, realm, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rwkv, sam, seamless_m4t, segformer, sew, sew-d, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, umt5, unispeech, unispeech-sat, upernet, van, videomae, vilt, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vits, vivit, wav2vec2, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso

Hausax/Poetry_test failed
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 449, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 444, in _make_request
    httplib_response = conn.getresponse()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 1374, in getresponse
    response.begin()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 318, in begin
    version, status, reason = self._read_status()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 279, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/anaconda/envs/amc/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1274, in recv_into
    return self.read(nbytes, buffer)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1130, in read
    return self._sslobj.read(len, buffer)
TimeoutError: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/util/retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/packages/six.py", line 770, in reraise
    raise value
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 451, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 340, in _raise_timeout
    raise ReadTimeoutError(
urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 425, in _request_wrapper
    response = get_session().request(method=method, url=url, **params)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 63, in send
    return super().send(request, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 532, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: (ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 6b6987aa-d9fb-4705-9fac-9f0f95863709)')

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1377, in hf_hub_download
    raise LocalEntryNotFoundError(
huggingface_hub.utils._errors.LocalEntryNotFoundError: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 107, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 470, in cached_file
    raise EnvironmentError(
OSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like Hausax/Poetry_test is not the path to a directory containing a file named config.json.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.

tals/albert-xlarge-vitaminc-fever failed
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 107, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2024, in from_pretrained
    return cls._from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2256, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/albert/tokenization_albert_fast.py", line 148, in __init__
    super().__init__(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1344, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 459, in __init__
    requires_backends(self, "protobuf")
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1247, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
AlbertConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Nausheen/bert-finetuned-squad-accelerate failed
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 449, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 444, in _make_request
    httplib_response = conn.getresponse()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 1374, in getresponse
    response.begin()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 318, in begin
    version, status, reason = self._read_status()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 279, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/anaconda/envs/amc/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1274, in recv_into
    return self.read(nbytes, buffer)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1130, in read
    return self._sslobj.read(len, buffer)
TimeoutError: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/util/retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/packages/six.py", line 770, in reraise
    raise value
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 451, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 340, in _raise_timeout
    raise ReadTimeoutError(
urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 107, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 1947, in from_pretrained
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1461, in hf_hub_download
    http_get(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 468, in http_get
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 425, in _request_wrapper
    response = get_session().request(method=method, url=url, **params)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 63, in send
    return super().send(request, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 532, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: (ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: b17ff14f-9ee0-44d3-bde3-cd7d8a6efe9f)')

elopezlopez/Bio_ClinicalBERT_fold_6_ternary_v1 failed
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 449, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 444, in _make_request
    httplib_response = conn.getresponse()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 1374, in getresponse
    response.begin()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 318, in begin
    version, status, reason = self._read_status()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 279, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/anaconda/envs/amc/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1274, in recv_into
    return self.read(nbytes, buffer)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1130, in read
    return self._sslobj.read(len, buffer)
TimeoutError: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/util/retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/packages/six.py", line 770, in reraise
    raise value
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 451, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 340, in _raise_timeout
    raise ReadTimeoutError(
urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 113, in <module>
    config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1461, in hf_hub_download
    http_get(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 468, in http_get
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 425, in _request_wrapper
    response = get_session().request(method=method, url=url, **params)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 63, in send
    return super().send(request, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 532, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: (ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 72838f03-04d5-48cb-ade5-5747b3486b9c)')

akreal/mbart-large-50-finetuned-portmedia-dom failed
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 449, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 444, in _make_request
    httplib_response = conn.getresponse()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 1374, in getresponse
    response.begin()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 318, in begin
    version, status, reason = self._read_status()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 279, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/anaconda/envs/amc/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1274, in recv_into
    return self.read(nbytes, buffer)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1130, in read
    return self._sslobj.read(len, buffer)
TimeoutError: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/util/retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/packages/six.py", line 770, in reraise
    raise value
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 451, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 340, in _raise_timeout
    raise ReadTimeoutError(
urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 425, in _request_wrapper
    response = get_session().request(method=method, url=url, **params)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 63, in send
    return super().send(request, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 532, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: (ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 9837739e-d4bd-4b14-a0cf-7885e2122c7a)')

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1377, in hf_hub_download
    raise LocalEntryNotFoundError(
huggingface_hub.utils._errors.LocalEntryNotFoundError: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 107, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 733, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 470, in cached_file
    raise EnvironmentError(
OSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like akreal/mbart-large-50-finetuned-portmedia-dom is not the path to a directory containing a file named config.json.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.

yanaiela/roberta-base-epoch_23 failed
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 449, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 444, in _make_request
    httplib_response = conn.getresponse()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 1374, in getresponse
    response.begin()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 318, in begin
    version, status, reason = self._read_status()
  File "/anaconda/envs/amc/lib/python3.10/http/client.py", line 279, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/anaconda/envs/amc/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1274, in recv_into
    return self.read(nbytes, buffer)
  File "/anaconda/envs/amc/lib/python3.10/ssl.py", line 1130, in read
    return self._sslobj.read(len, buffer)
TimeoutError: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/util/retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/packages/six.py", line 770, in reraise
    raise value
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 451, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/urllib3/connectionpool.py", line 340, in _raise_timeout
    raise ReadTimeoutError(
urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 107, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 768, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 1983, in from_pretrained
    resolved_vocab_files[file_id] = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1461, in hf_hub_download
    http_get(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 468, in http_get
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 425, in _request_wrapper
    response = get_session().request(method=method, url=url, **params)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 63, in send
    return super().send(request, *args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/adapters.py", line 532, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: (ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: cb951b49-7006-4078-801d-a68121097188)')

Helsinki-NLP/opus-mt-en-alv failed
Architectures: ['MarianMTModel']
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 118, in <module>
    before_trace = model(**concrete_args)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 1203, in forward
    decoder_outputs = self.decoder(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py", line 932, in forward
    raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

SimoC/distilbert-base-uncased-finetuned-emotion failed
Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 270, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda/envs/amc/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/SimoC/distilbert-base-uncased-finetuned-emotion/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 430, in cached_file
    resolved_file = hf_hub_download(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in hf_hub_download
    raise head_call_error
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1247, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1624, in get_hf_file_metadata
    r = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 402, in _request_wrapper
    response = _request_wrapper(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 426, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 320, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-656ffad6-05620886141cfa471ec2356a;124100ef-4e65-4303-91fc-0149f2f97522)

Repository Not Found for url: https://huggingface.co/SimoC/distilbert-base-uncased-finetuned-emotion/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 107, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 718, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 550, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/utils/hub.py", line 451, in cached_file
    raise EnvironmentError(
OSError: SimoC/distilbert-base-uncased-finetuned-emotion is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

ParkSaeroyi/distilroberta-base-finetuned-wikitext2 failed
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 107, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ParkSaeroyi/distilroberta-base-finetuned-wikitext2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ParkSaeroyi/distilroberta-base-finetuned-wikitext2' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.

ghadeermobasher/BC5CDR-disease-WLT-512-SciBERT-t failed
Traceback (most recent call last):
  File "/home/yileiyang/workspace/auto-model-compiler/graph/concrete_trace_nlp.py", line 107, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 786, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/anaconda/envs/amc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2008, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'ghadeermobasher/BC5CDR-disease-WLT-512-SciBERT-t'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ghadeermobasher/BC5CDR-disease-WLT-512-SciBERT-t' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.
